{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"./tianchi_datasets/track3_round1_train.tsv\", sep = \"\\t\", header = None,\n",
    "                        names = [\"sentence1\", \"sentence2\", \"labels\"])\n",
    "test_data = pd.read_csv(\"./tianchi_datasets/track3_round1_testA.tsv\", sep = \"\\t\", header = None,\n",
    "                        names = [\"sentence1\", \"sentence2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"sentence1\"] = train_data[\"sentence1\"].map(lambda x: [int(x) for x in x.split(\" \")])\n",
    "train_data[\"sentence2\"] = train_data[\"sentence2\"].map(lambda x: [int(x) for x in x.split(\" \")])\n",
    "test_data[\"sentence1\"] = test_data[\"sentence1\"].map(lambda x: [int(x) for x in x.split(\" \")])\n",
    "test_data[\"sentence2\"] = test_data[\"sentence2\"].map(lambda x: [int(x) for x in x.split(\" \")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab_v2i_i2v(train_data, test_data):\n",
    "    vocab = set()\n",
    "    for data in [train_data, test_data]:\n",
    "        for n in [\"sentence1\", \"sentence2\"]:\n",
    "            for i in range(len(data[n])):\n",
    "                vocab.update(data[n][i])\n",
    "    \n",
    "    v2i = {v:i for i, v in enumerate(sorted(vocab), start = 1)}\n",
    "    v2i['[PAD]'] = 0\n",
    "    v2i['[CLS]'] = len(v2i)\n",
    "    v2i['[SEP]'] = len(v2i)\n",
    "    i2v = {i:v for v, i in v2i.items()}\n",
    "    \n",
    "    return vocab, v2i, i2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, v2i, i2v = get_vocab_v2i_i2v(train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = max([len(s1) + len(s2) + 3  for s1, s2 in zip(\n",
    "    train_data[\"sentence1\"].tolist() + test_data[\"sentence1\"].tolist(), \n",
    "    train_data[\"sentence2\"].tolist() + test_data[\"sentence2\"].tolist())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"document\"] = [[v2i['[CLS]']] + train_data[\"sentence1\"][i] + [v2i['[SEP]']] + \n",
    "        train_data[\"sentence2\"][i] + [v2i['[SEP]']] for i in range(len(train_data))]\n",
    "test_data[\"document\"] = [[v2i['[CLS]']] + test_data[\"sentence1\"][i] + [v2i['[SEP]']] + \n",
    "        test_data[\"sentence2\"][i] + [v2i['[SEP]']] for i in range(len(test_data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.1\n",
    "seed_val = 2021\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "            train_data[\"document\"], train_data[\"labels\"], test_size=test_size,\n",
    "            stratify= train_data[\"labels\"], random_state=seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_bert_datasets(data, labels, max_length):\n",
    "    input_ids = []\n",
    "    \n",
    "    for x in data:\n",
    "        padded = np.full(max_length, fill_value = 0, dtype = np.int32)\n",
    "        padded[:len(x)] = x\n",
    "        input_ids.append(torch.LongTensor([padded]))\n",
    "    \n",
    "    input_ids = torch.cat(input_ids, dim = 0)\n",
    "    \n",
    "    if labels is None:\n",
    "        return Data.TensorDataset(input_ids)\n",
    "    else:\n",
    "        labels = torch.tensor(labels.values)\n",
    "        return Data.TensorDataset(input_ids, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = convert_to_bert_datasets(X_train, y_train, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation = convert_to_bert_datasets(X_val, y_val, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = convert_to_bert_datasets(test_data[\"document\"], None, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "train_loader = Data.DataLoader(train, batch_size = batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = Data.DataLoader(test, batch_size = batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "slices = list(range(1, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_max(nums):\n",
    "            n = len(nums)\n",
    "            if n <= 2:\n",
    "                return max(nums)\n",
    "            choose = (n+1) // 3\n",
    "            dp = [[0 for _ in range(choose+1)] for _ in range(n+1)] \n",
    "            for i in range(1, n+1):\n",
    "                for j in range(1, choose+1):\n",
    "                    dp[i][j] = max((dp[i-2][j-1] if i >= 2 else 0) + nums[i-1], dp[i-1][j])\n",
    "                    print(dp[i][j])\n",
    "                    \n",
    "            return dp[n][choose]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxSizeSlices(slices):\n",
    "        \n",
    "        def cal_max(nums):\n",
    "            n = len(nums)\n",
    "            if n <= 2:\n",
    "                return max(nums)\n",
    "            choose = (n+1) // 3\n",
    "            dp = [[0 for _ in range(choose+1)] for _ in range(n+1)] \n",
    "            for i in range(1, n+1):\n",
    "                for j in range(1, choose+1):\n",
    "                    dp[i][j] = max((dp[i-2][j-1] if i >= 2 else 0) + nums[i-1], dp[i-1][j])\n",
    "            \n",
    "            return dp[n][choose]\n",
    "\n",
    "        n = len(slices)\n",
    "        if n <= 3:\n",
    "            return max(slices)\n",
    "        \n",
    "        return max(cal_max(slices[1:]), cal_max(slices[:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxSizeSlices(slices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n",
      "3\n",
      "3\n",
      "4\n",
      "6\n",
      "5\n",
      "8\n",
      "6\n",
      "10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cal_max(slices[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1])[np.newaxis, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1]])"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-244-46ef93bedd18>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0ma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "a[np.newaxis, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "            train_data[\"document\"], train_data[\"class\"], test_size=test_size,\n",
    "            stratify= train_data[\"class\"], random_state=seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sum = max([len(x) for x in X_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [[v2i['[CLS]']] + data + []]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertModel.from_pretrained(\"./pretrain_model/chinese_roberta_wwm_ext\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                   [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 4, 11]\n",
       "1                         [12, 13, 14, 15, 12, 15, 11, 16]\n",
       "2        [17, 18, 12, 19, 20, 21, 22, 23, 24, 12, 23, 2...\n",
       "3                 [28, 29, 30, 31, 11, 32, 33, 34, 30, 31]\n",
       "4                         [29, 35, 36, 29, 29, 37, 36, 29]\n",
       "                               ...                        \n",
       "99995    [12, 19, 1162, 126, 53, 66, 12, 19, 79, 389, 1...\n",
       "99996    [275, 552, 553, 433, 881, 338, 1104, 101, 202,...\n",
       "99997    [421, 330, 62, 12, 80, 81, 82, 76, 202, 62, 12...\n",
       "99998    [177, 455, 456, 3474, 964, 1364, 55, 1364, 133...\n",
       "99999    [29, 168, 12, 19, 1003, 719, 23, 29, 263, 276,...\n",
       "Name: document, Length: 100000, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[\"document\"].map(lambda x: [int(x) for x in x.split(\" \")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"./pretrain_model/chinese_roberta_wwm_ext\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21128"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 122, 123, 124, 102], 'token_type_ids': [0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"1 2 3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', '1', '2', '3', '[SEP]']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens([101, 122, 123, 124, 102])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1], dtype=int64), array([63564, 36436], dtype=int64))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(train_data[\"class\"], return_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 1],\n",
       "         [1, 1]],\n",
       "\n",
       "        [[1, 1],\n",
       "         [1, 1]]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(2, 2, 2).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 2769, 2769, 1521,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"我我哦\", return_tensors= \"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1]])"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.LongTensor([[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Data_generator import Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "seed_val = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[0.3, 0.7],[0.2, 0.8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.7, 0.8])"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************Data_precessing*********************************\n",
      "## dataset size is 100000\n",
      "*******************************Load  dataset ....*******************************\n",
      "## Load Datasets Consume 0:00:10 s ###\n",
      "************************All Train and Test Data loaded !************************\n",
      "[tensor([[20601,    45,    46,  ...,     0,     0,     0],\n",
      "        [20601,  1038,   591,  ...,     0,     0,     0],\n",
      "        [20601,  3299,   249,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [20601,    87,   956,  ...,     0,     0,     0],\n",
      "        [20601,  5553,   578,  ...,     0,     0,     0],\n",
      "        [20601,   670,   217,  ...,     0,     0,     0]])]\n",
      "tensor([0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "from Data_generator import Corpus\n",
    "batch_size = 16\n",
    "seed_val = 1\n",
    "corpus = Corpus(batch_size, seed_val)\n",
    "train_loader, valid_loader, test_loader = corpus.get_loaders()\n",
    "for batch in train_loader:\n",
    "    print(batch[:-1])\n",
    "    print(batch[-1])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[20601,    45,    46,  ...,     0,     0,     0],\n",
       "        [20601,  1038,   591,  ...,     0,     0,     0],\n",
       "        [20601,  3299,   249,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [20601,    87,   956,  ...,     0,     0,     0],\n",
       "        [20601,  5553,   578,  ...,     0,     0,     0],\n",
       "        [20601,   670,   217,  ...,     0,     0,     0]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[:-1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[20601,    45,    46,  ...,     0,     0,     0],\n",
       "        [20601,  1038,   591,  ...,     0,     0,     0],\n",
       "        [20601,  3299,   249,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [20601,    87,   956,  ...,     0,     0,     0],\n",
       "        [20601,  5553,   578,  ...,     0,     0,     0],\n",
       "        [20601,   670,   217,  ...,     0,     0,     0]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(*batch[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "y_test = np.array([0,0,1,1])\n",
    "y_pred1 = np.array([0.3,0.2,0.25,0.7])\n",
    "y_pred2 = np.array([0,0,1,0]) \n",
    "from sklearn.metrics import roc_auc_score \n",
    "# 预测值是概率\n",
    "auc_score1 = roc_auc_score(y_test,y_pred1)\n",
    "print(auc_score1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = np.random.rand(16, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 2)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.47471689, 0.76443855, 0.73893367, 0.46974965])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.array([])\n",
    "y_pred = np.append(y_pred, logits[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.82289618e-01, 6.56078992e-01, 7.94656512e-04, 3.93339930e-01,\n",
       "       2.82572855e-01, 9.58476154e-01, 2.31650849e-01, 6.05707484e-01,\n",
       "       8.09247471e-01, 7.02316260e-01, 1.88429106e-02, 7.00293130e-01,\n",
       "       9.72872622e-01, 7.89888877e-01, 7.48251878e-01, 8.73607196e-02])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = a.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, y_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_pred2, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90016"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2813 * 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imp import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Data_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'Data_generator' from 'E:\\\\lv_python\\\\NLP\\\\阿里天池小布助手\\\\Data_generator.py'>"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(Data_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Data_generator import Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************Data_precessing*********************************\n",
      "## dataset size is 100000\n",
      "*******************************Load  dataset ....*******************************\n",
      "## Load Datasets Consume 0:00:10 s ###\n",
      "************************All Train and Test Data loaded !************************\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "seed_val = 2021\n",
    "corpus = Corpus(batch_size, seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader, valid_dataloader, test_dataloader = corpus.get_loaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0][0].size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in test_dataloader:\n",
    "    if x[0][0].size(0) != 93: \n",
    "        print(\"bug\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|██████▏                                                                    | 2063/25000 [12:23<2:17:42,  2.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[20165,    12,   126,  1758,    66,   444, 20166,    12,   444,    28,\n",
      "             9,  1758,    76, 20166,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0]])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for y in tqdm(test_dataloader):\n",
    "    try:\n",
    "        outputs = model(*y)\n",
    "    except:\n",
    "        print(y)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[20601,   200,  1789,    52,    52,  6191,   433,  1652,    52,    11,\n",
       "            415,     8,   344,     9,  5192,   267, 20602,  2320,  2570,  1104,\n",
       "             52,  1405, 21600,  1382,   173, 20602,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0]])]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(*y)[0][:, 0, :].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(21128, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"./tianchi_datasets/track3_round1_train.tsv\", sep=\"\\t\", header=None,\n",
    "                                 names=[\"sentence1\", \"sentence2\", \"labels\"])\n",
    "test_data = pd.read_csv(\"./tianchi_datasets/track3_round1_testA.tsv\", sep=\"\\t\", header=None,\n",
    "                                names=[\"sentence1\", \"sentence2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convrt_str_to_list(train_data, test_data):\n",
    "    for n in  [\"sentence1\", \"sentence2\"]:\n",
    "        for m in [train_data, test_data]:\n",
    "                    m[n] = m[n].map(lambda x: [int(x) for x in x.split(\" \")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "convrt_str_to_list(train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set()\n",
    "for data in [train_data, test_data]:\n",
    "    for n in [\"sentence1\", \"sentence2\"]:\n",
    "        for i in range(len(data[n])):\n",
    "            regular(data[n][i])\n",
    "            vocab.update(data[n][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regular(data):\n",
    "    ## 因为bert的vocab size最大为21128,因此将大于21128的全部替换为0，['PAD']\n",
    "    for i in range(len(data)):\n",
    "        num = data[i]\n",
    "        data[i] = num if num < 21125 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab_v2i_i2v(train_data, test_data):\n",
    "    ## 获得语料库vocab, idx2label, label2idx\n",
    "    vocab = set()\n",
    "    for data in [train_data, test_data]:\n",
    "        for n in [\"sentence1\", \"sentence2\"]:\n",
    "            for i in range(len(data[n])):\n",
    "                regular(data[n][i])\n",
    "                vocab.update(data[n][i])\n",
    "\n",
    "\n",
    "    v2i = {v: i for i, v in enumerate(sorted(vocab))}\n",
    "    v2i['[PAD]'] = v2i.pop(0)\n",
    "    v2i['[CLS]'] = len(v2i)\n",
    "    v2i['[SEP]'] = len(v2i)\n",
    "    i2v = {i: v for v, i in v2i.items()}\n",
    "\n",
    "    return vocab, v2i, i2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab1, v2i1, i2v1 = get_vocab_v2i_i2v(train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v2i['[PAD]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 63,\n",
       " 64,\n",
       " 65,\n",
       " 66,\n",
       " 67,\n",
       " 68,\n",
       " 69,\n",
       " 70,\n",
       " 71,\n",
       " 72,\n",
       " 73,\n",
       " 74,\n",
       " 75,\n",
       " 76,\n",
       " 77,\n",
       " 78,\n",
       " 79,\n",
       " 80,\n",
       " 81,\n",
       " 82,\n",
       " 83,\n",
       " 84,\n",
       " 85,\n",
       " 86,\n",
       " 87,\n",
       " 88,\n",
       " 89,\n",
       " 90,\n",
       " 91,\n",
       " 92,\n",
       " 93,\n",
       " 94,\n",
       " 95,\n",
       " 96,\n",
       " 97,\n",
       " 98,\n",
       " 99,\n",
       " 100,\n",
       " 101,\n",
       " 102,\n",
       " 103,\n",
       " 104,\n",
       " 105,\n",
       " 106,\n",
       " 107,\n",
       " 108,\n",
       " 109,\n",
       " 110,\n",
       " 111,\n",
       " 112,\n",
       " 113,\n",
       " 114,\n",
       " 115,\n",
       " 116,\n",
       " 117,\n",
       " 118,\n",
       " 119,\n",
       " 120,\n",
       " 121,\n",
       " 122,\n",
       " 123,\n",
       " 124,\n",
       " 125,\n",
       " 126,\n",
       " 127,\n",
       " 128,\n",
       " 129,\n",
       " 130,\n",
       " 131,\n",
       " 132,\n",
       " 133,\n",
       " 134,\n",
       " 135,\n",
       " 136,\n",
       " 137,\n",
       " 138,\n",
       " 139,\n",
       " 140,\n",
       " 141,\n",
       " 142,\n",
       " 143,\n",
       " 144,\n",
       " 145,\n",
       " 146,\n",
       " 147,\n",
       " 148,\n",
       " 149,\n",
       " 150,\n",
       " 151,\n",
       " 152,\n",
       " 153,\n",
       " 154,\n",
       " 155,\n",
       " 156,\n",
       " 157,\n",
       " 158,\n",
       " 159,\n",
       " 160,\n",
       " 161,\n",
       " 162,\n",
       " 163,\n",
       " 164,\n",
       " 165,\n",
       " 166,\n",
       " 167,\n",
       " 168,\n",
       " 169,\n",
       " 170,\n",
       " 171,\n",
       " 172,\n",
       " 173,\n",
       " 174,\n",
       " 175,\n",
       " 176,\n",
       " 177,\n",
       " 178,\n",
       " 179,\n",
       " 180,\n",
       " 181,\n",
       " 182,\n",
       " 183,\n",
       " 184,\n",
       " 185,\n",
       " 186,\n",
       " 187,\n",
       " 188,\n",
       " 189,\n",
       " 190,\n",
       " 191,\n",
       " 192,\n",
       " 193,\n",
       " 194,\n",
       " 195,\n",
       " 196,\n",
       " 197,\n",
       " 198,\n",
       " 199,\n",
       " 200,\n",
       " 201,\n",
       " 202,\n",
       " 203,\n",
       " 204,\n",
       " 205,\n",
       " 206,\n",
       " 207,\n",
       " 208,\n",
       " 209,\n",
       " 210,\n",
       " 211,\n",
       " 212,\n",
       " 213,\n",
       " 214,\n",
       " 215,\n",
       " 216,\n",
       " 217,\n",
       " 218,\n",
       " 219,\n",
       " 220,\n",
       " 221,\n",
       " 222,\n",
       " 223,\n",
       " 224,\n",
       " 225,\n",
       " 226,\n",
       " 227,\n",
       " 228,\n",
       " 229,\n",
       " 230,\n",
       " 231,\n",
       " 232,\n",
       " 233,\n",
       " 234,\n",
       " 235,\n",
       " 236,\n",
       " 237,\n",
       " 238,\n",
       " 239,\n",
       " 240,\n",
       " 241,\n",
       " 242,\n",
       " 243,\n",
       " 244,\n",
       " 245,\n",
       " 246,\n",
       " 247,\n",
       " 248,\n",
       " 249,\n",
       " 250,\n",
       " 251,\n",
       " 252,\n",
       " 253,\n",
       " 254,\n",
       " 255,\n",
       " 256,\n",
       " 257,\n",
       " 258,\n",
       " 259,\n",
       " 260,\n",
       " 261,\n",
       " 262,\n",
       " 263,\n",
       " 264,\n",
       " 265,\n",
       " 266,\n",
       " 267,\n",
       " 268,\n",
       " 269,\n",
       " 270,\n",
       " 271,\n",
       " 272,\n",
       " 273,\n",
       " 274,\n",
       " 275,\n",
       " 276,\n",
       " 277,\n",
       " 278,\n",
       " 279,\n",
       " 280,\n",
       " 281,\n",
       " 282,\n",
       " 283,\n",
       " 284,\n",
       " 285,\n",
       " 286,\n",
       " 287,\n",
       " 288,\n",
       " 289,\n",
       " 290,\n",
       " 291,\n",
       " 292,\n",
       " 293,\n",
       " 294,\n",
       " 295,\n",
       " 296,\n",
       " 297,\n",
       " 298,\n",
       " 299,\n",
       " 300,\n",
       " 301,\n",
       " 302,\n",
       " 303,\n",
       " 304,\n",
       " 305,\n",
       " 306,\n",
       " 307,\n",
       " 308,\n",
       " 309,\n",
       " 310,\n",
       " 311,\n",
       " 312,\n",
       " 313,\n",
       " 314,\n",
       " 315,\n",
       " 316,\n",
       " 317,\n",
       " 318,\n",
       " 319,\n",
       " 320,\n",
       " 321,\n",
       " 322,\n",
       " 323,\n",
       " 324,\n",
       " 325,\n",
       " 326,\n",
       " 327,\n",
       " 328,\n",
       " 329,\n",
       " 330,\n",
       " 331,\n",
       " 332,\n",
       " 333,\n",
       " 334,\n",
       " 335,\n",
       " 336,\n",
       " 337,\n",
       " 338,\n",
       " 339,\n",
       " 340,\n",
       " 341,\n",
       " 342,\n",
       " 343,\n",
       " 344,\n",
       " 345,\n",
       " 346,\n",
       " 347,\n",
       " 348,\n",
       " 349,\n",
       " 350,\n",
       " 351,\n",
       " 352,\n",
       " 353,\n",
       " 354,\n",
       " 355,\n",
       " 356,\n",
       " 357,\n",
       " 358,\n",
       " 359,\n",
       " 360,\n",
       " 361,\n",
       " 362,\n",
       " 363,\n",
       " 364,\n",
       " 365,\n",
       " 366,\n",
       " 367,\n",
       " 368,\n",
       " 369,\n",
       " 370,\n",
       " 371,\n",
       " 372,\n",
       " 373,\n",
       " 374,\n",
       " 375,\n",
       " 376,\n",
       " 377,\n",
       " 378,\n",
       " 379,\n",
       " 380,\n",
       " 381,\n",
       " 382,\n",
       " 383,\n",
       " 384,\n",
       " 385,\n",
       " 386,\n",
       " 387,\n",
       " 388,\n",
       " 389,\n",
       " 390,\n",
       " 391,\n",
       " 392,\n",
       " 393,\n",
       " 394,\n",
       " 395,\n",
       " 396,\n",
       " 397,\n",
       " 398,\n",
       " 399,\n",
       " 400,\n",
       " 401,\n",
       " 402,\n",
       " 403,\n",
       " 404,\n",
       " 405,\n",
       " 406,\n",
       " 407,\n",
       " 408,\n",
       " 409,\n",
       " 410,\n",
       " 411,\n",
       " 412,\n",
       " 413,\n",
       " 414,\n",
       " 415,\n",
       " 416,\n",
       " 417,\n",
       " 418,\n",
       " 419,\n",
       " 420,\n",
       " 421,\n",
       " 422,\n",
       " 423,\n",
       " 424,\n",
       " 425,\n",
       " 426,\n",
       " 427,\n",
       " 428,\n",
       " 429,\n",
       " 430,\n",
       " 431,\n",
       " 432,\n",
       " 433,\n",
       " 434,\n",
       " 435,\n",
       " 436,\n",
       " 437,\n",
       " 438,\n",
       " 439,\n",
       " 440,\n",
       " 441,\n",
       " 442,\n",
       " 443,\n",
       " 444,\n",
       " 445,\n",
       " 446,\n",
       " 447,\n",
       " 448,\n",
       " 449,\n",
       " 450,\n",
       " 451,\n",
       " 452,\n",
       " 453,\n",
       " 454,\n",
       " 455,\n",
       " 456,\n",
       " 457,\n",
       " 458,\n",
       " 459,\n",
       " 460,\n",
       " 461,\n",
       " 462,\n",
       " 463,\n",
       " 464,\n",
       " 465,\n",
       " 466,\n",
       " 467,\n",
       " 468,\n",
       " 469,\n",
       " 470,\n",
       " 471,\n",
       " 472,\n",
       " 473,\n",
       " 474,\n",
       " 475,\n",
       " 476,\n",
       " 477,\n",
       " 478,\n",
       " 479,\n",
       " 480,\n",
       " 481,\n",
       " 482,\n",
       " 483,\n",
       " 484,\n",
       " 485,\n",
       " 486,\n",
       " 487,\n",
       " 488,\n",
       " 489,\n",
       " 490,\n",
       " 491,\n",
       " 492,\n",
       " 493,\n",
       " 494,\n",
       " 495,\n",
       " 496,\n",
       " 497,\n",
       " 498,\n",
       " 499,\n",
       " 500,\n",
       " 501,\n",
       " 502,\n",
       " 503,\n",
       " 504,\n",
       " 505,\n",
       " 506,\n",
       " 507,\n",
       " 508,\n",
       " 509,\n",
       " 510,\n",
       " 511,\n",
       " 512,\n",
       " 513,\n",
       " 514,\n",
       " 515,\n",
       " 516,\n",
       " 517,\n",
       " 518,\n",
       " 519,\n",
       " 520,\n",
       " 521,\n",
       " 522,\n",
       " 523,\n",
       " 524,\n",
       " 525,\n",
       " 526,\n",
       " 527,\n",
       " 528,\n",
       " 529,\n",
       " 530,\n",
       " 531,\n",
       " 532,\n",
       " 533,\n",
       " 534,\n",
       " 535,\n",
       " 536,\n",
       " 537,\n",
       " 538,\n",
       " 539,\n",
       " 540,\n",
       " 541,\n",
       " 542,\n",
       " 543,\n",
       " 544,\n",
       " 545,\n",
       " 546,\n",
       " 547,\n",
       " 548,\n",
       " 549,\n",
       " 550,\n",
       " 551,\n",
       " 552,\n",
       " 553,\n",
       " 554,\n",
       " 555,\n",
       " 556,\n",
       " 557,\n",
       " 558,\n",
       " 559,\n",
       " 560,\n",
       " 561,\n",
       " 562,\n",
       " 563,\n",
       " 564,\n",
       " 565,\n",
       " 566,\n",
       " 567,\n",
       " 568,\n",
       " 569,\n",
       " 570,\n",
       " 571,\n",
       " 572,\n",
       " 573,\n",
       " 574,\n",
       " 575,\n",
       " 576,\n",
       " 577,\n",
       " 578,\n",
       " 579,\n",
       " 580,\n",
       " 581,\n",
       " 582,\n",
       " 583,\n",
       " 584,\n",
       " 585,\n",
       " 586,\n",
       " 587,\n",
       " 588,\n",
       " 589,\n",
       " 590,\n",
       " 591,\n",
       " 592,\n",
       " 593,\n",
       " 594,\n",
       " 595,\n",
       " 596,\n",
       " 597,\n",
       " 598,\n",
       " 599,\n",
       " 600,\n",
       " 601,\n",
       " 602,\n",
       " 603,\n",
       " 604,\n",
       " 605,\n",
       " 606,\n",
       " 607,\n",
       " 608,\n",
       " 609,\n",
       " 610,\n",
       " 611,\n",
       " 612,\n",
       " 613,\n",
       " 614,\n",
       " 615,\n",
       " 616,\n",
       " 617,\n",
       " 618,\n",
       " 619,\n",
       " 620,\n",
       " 621,\n",
       " 622,\n",
       " 623,\n",
       " 624,\n",
       " 625,\n",
       " 626,\n",
       " 627,\n",
       " 628,\n",
       " 629,\n",
       " 630,\n",
       " 631,\n",
       " 632,\n",
       " 633,\n",
       " 634,\n",
       " 635,\n",
       " 636,\n",
       " 637,\n",
       " 638,\n",
       " 639,\n",
       " 640,\n",
       " 641,\n",
       " 642,\n",
       " 643,\n",
       " 644,\n",
       " 645,\n",
       " 646,\n",
       " 647,\n",
       " 648,\n",
       " 649,\n",
       " 650,\n",
       " 651,\n",
       " 652,\n",
       " 653,\n",
       " 654,\n",
       " 655,\n",
       " 656,\n",
       " 657,\n",
       " 658,\n",
       " 659,\n",
       " 660,\n",
       " 661,\n",
       " 662,\n",
       " 663,\n",
       " 664,\n",
       " 665,\n",
       " 666,\n",
       " 667,\n",
       " 668,\n",
       " 669,\n",
       " 670,\n",
       " 671,\n",
       " 672,\n",
       " 673,\n",
       " 674,\n",
       " 675,\n",
       " 676,\n",
       " 677,\n",
       " 678,\n",
       " 679,\n",
       " 680,\n",
       " 681,\n",
       " 682,\n",
       " 683,\n",
       " 684,\n",
       " 685,\n",
       " 686,\n",
       " 687,\n",
       " 688,\n",
       " 689,\n",
       " 690,\n",
       " 691,\n",
       " 692,\n",
       " 693,\n",
       " 694,\n",
       " 695,\n",
       " 696,\n",
       " 697,\n",
       " 698,\n",
       " 699,\n",
       " 700,\n",
       " 701,\n",
       " 702,\n",
       " 703,\n",
       " 704,\n",
       " 705,\n",
       " 706,\n",
       " 707,\n",
       " 708,\n",
       " 709,\n",
       " 710,\n",
       " 711,\n",
       " 712,\n",
       " 713,\n",
       " 714,\n",
       " 715,\n",
       " 716,\n",
       " 717,\n",
       " 718,\n",
       " 719,\n",
       " 720,\n",
       " 721,\n",
       " 722,\n",
       " 723,\n",
       " 724,\n",
       " 725,\n",
       " 726,\n",
       " 727,\n",
       " 728,\n",
       " 729,\n",
       " 730,\n",
       " 731,\n",
       " 732,\n",
       " 733,\n",
       " 734,\n",
       " 735,\n",
       " 736,\n",
       " 737,\n",
       " 738,\n",
       " 739,\n",
       " 740,\n",
       " 741,\n",
       " 742,\n",
       " 743,\n",
       " 744,\n",
       " 745,\n",
       " 746,\n",
       " 747,\n",
       " 748,\n",
       " 749,\n",
       " 750,\n",
       " 751,\n",
       " 752,\n",
       " 753,\n",
       " 754,\n",
       " 755,\n",
       " 756,\n",
       " 757,\n",
       " 758,\n",
       " 759,\n",
       " 760,\n",
       " 761,\n",
       " 762,\n",
       " 763,\n",
       " 764,\n",
       " 765,\n",
       " 766,\n",
       " 767,\n",
       " 768,\n",
       " 769,\n",
       " 770,\n",
       " 771,\n",
       " 772,\n",
       " 773,\n",
       " 774,\n",
       " 775,\n",
       " 776,\n",
       " 777,\n",
       " 778,\n",
       " 779,\n",
       " 780,\n",
       " 781,\n",
       " 782,\n",
       " 783,\n",
       " 784,\n",
       " 785,\n",
       " 786,\n",
       " 787,\n",
       " 788,\n",
       " 789,\n",
       " 790,\n",
       " 791,\n",
       " 792,\n",
       " 793,\n",
       " 794,\n",
       " 795,\n",
       " 796,\n",
       " 797,\n",
       " 798,\n",
       " 799,\n",
       " 800,\n",
       " 801,\n",
       " 802,\n",
       " 803,\n",
       " 804,\n",
       " 805,\n",
       " 806,\n",
       " 807,\n",
       " 808,\n",
       " 809,\n",
       " 810,\n",
       " 811,\n",
       " 812,\n",
       " 813,\n",
       " 814,\n",
       " 815,\n",
       " 816,\n",
       " 817,\n",
       " 818,\n",
       " 819,\n",
       " 820,\n",
       " 821,\n",
       " 822,\n",
       " 823,\n",
       " 824,\n",
       " 825,\n",
       " 826,\n",
       " 827,\n",
       " 828,\n",
       " 829,\n",
       " 830,\n",
       " 831,\n",
       " 832,\n",
       " 833,\n",
       " 834,\n",
       " 835,\n",
       " 836,\n",
       " 837,\n",
       " 838,\n",
       " 839,\n",
       " 840,\n",
       " 841,\n",
       " 842,\n",
       " 843,\n",
       " 844,\n",
       " 845,\n",
       " 846,\n",
       " 847,\n",
       " 848,\n",
       " 849,\n",
       " 850,\n",
       " 851,\n",
       " 852,\n",
       " 853,\n",
       " 854,\n",
       " 855,\n",
       " 856,\n",
       " 857,\n",
       " 858,\n",
       " 859,\n",
       " 860,\n",
       " 861,\n",
       " 862,\n",
       " 863,\n",
       " 864,\n",
       " 865,\n",
       " 866,\n",
       " 867,\n",
       " 868,\n",
       " 869,\n",
       " 870,\n",
       " 871,\n",
       " 872,\n",
       " 873,\n",
       " 874,\n",
       " 875,\n",
       " 876,\n",
       " 877,\n",
       " 878,\n",
       " 879,\n",
       " 880,\n",
       " 881,\n",
       " 882,\n",
       " 883,\n",
       " 884,\n",
       " 885,\n",
       " 886,\n",
       " 887,\n",
       " 888,\n",
       " 889,\n",
       " 890,\n",
       " 891,\n",
       " 892,\n",
       " 893,\n",
       " 894,\n",
       " 895,\n",
       " 896,\n",
       " 897,\n",
       " 898,\n",
       " 899,\n",
       " 900,\n",
       " 901,\n",
       " 902,\n",
       " 903,\n",
       " 904,\n",
       " 905,\n",
       " 906,\n",
       " 907,\n",
       " 908,\n",
       " 909,\n",
       " 910,\n",
       " 911,\n",
       " 912,\n",
       " 913,\n",
       " 914,\n",
       " 915,\n",
       " 916,\n",
       " 917,\n",
       " 918,\n",
       " 919,\n",
       " 920,\n",
       " 921,\n",
       " 922,\n",
       " 923,\n",
       " 924,\n",
       " 925,\n",
       " 926,\n",
       " 927,\n",
       " 928,\n",
       " 929,\n",
       " 930,\n",
       " 931,\n",
       " 932,\n",
       " 933,\n",
       " 934,\n",
       " 935,\n",
       " 936,\n",
       " 937,\n",
       " 938,\n",
       " 939,\n",
       " 940,\n",
       " 941,\n",
       " 942,\n",
       " 943,\n",
       " 944,\n",
       " 945,\n",
       " 946,\n",
       " 947,\n",
       " 948,\n",
       " 949,\n",
       " 950,\n",
       " 951,\n",
       " 952,\n",
       " 953,\n",
       " 954,\n",
       " 955,\n",
       " 956,\n",
       " 957,\n",
       " 958,\n",
       " 959,\n",
       " 960,\n",
       " 961,\n",
       " 962,\n",
       " 963,\n",
       " 964,\n",
       " 965,\n",
       " 966,\n",
       " 967,\n",
       " 968,\n",
       " 969,\n",
       " 970,\n",
       " 971,\n",
       " 972,\n",
       " 973,\n",
       " 974,\n",
       " 975,\n",
       " 976,\n",
       " 977,\n",
       " 978,\n",
       " 979,\n",
       " 980,\n",
       " 981,\n",
       " 982,\n",
       " 983,\n",
       " 984,\n",
       " 985,\n",
       " 986,\n",
       " 987,\n",
       " 988,\n",
       " 989,\n",
       " 990,\n",
       " 991,\n",
       " 992,\n",
       " 993,\n",
       " 994,\n",
       " 995,\n",
       " 996,\n",
       " 997,\n",
       " 998,\n",
       " 999,\n",
       " ...}"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set()\n",
    "for data in [train_data, test_data]:\n",
    "    for n in [\"sentence1\", \"sentence2\"]:\n",
    "        for i in range(len(data[n])):\n",
    "            regular(data[n][i])\n",
    "            vocab.update(data[n][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20164"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v2i[21124]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21124"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i2v[20164]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "v2i = {v: i for i, v in enumerate(sorted(vocab))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "v2i['[PAD]'] = v2i.pop(0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v2i['[PAD]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "v2i['[CLS]'] = len(v2i)\n",
    "v2i['[SEP]'] = len(v2i)\n",
    "i2v = {i: v for v, i in v2i.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[21124,\n",
       " 21122,\n",
       " 21120,\n",
       " 21118,\n",
       " 21115,\n",
       " 21113,\n",
       " 21109,\n",
       " 21108,\n",
       " 21106,\n",
       " 21105,\n",
       " 21104,\n",
       " 21103,\n",
       " 21101,\n",
       " 21099,\n",
       " 21098,\n",
       " 21094,\n",
       " 21093,\n",
       " 21092,\n",
       " 21091,\n",
       " 21090,\n",
       " 21085,\n",
       " 21084,\n",
       " 21083,\n",
       " 21081,\n",
       " 21080,\n",
       " 21078,\n",
       " 21077,\n",
       " 21076,\n",
       " 21075,\n",
       " 21073,\n",
       " 21072,\n",
       " 21070,\n",
       " 21069,\n",
       " 21068,\n",
       " 21067,\n",
       " 21065,\n",
       " 21062,\n",
       " 21059,\n",
       " 21055,\n",
       " 21051,\n",
       " 21050,\n",
       " 21048,\n",
       " 21039,\n",
       " 21034,\n",
       " 21032,\n",
       " 21023,\n",
       " 21020,\n",
       " 21019,\n",
       " 21017,\n",
       " 21014,\n",
       " 21012,\n",
       " 21009,\n",
       " 21006,\n",
       " 21005,\n",
       " 21003,\n",
       " 21002,\n",
       " 21001,\n",
       " 20997,\n",
       " 20996,\n",
       " 20995,\n",
       " 20992,\n",
       " 20991,\n",
       " 20984,\n",
       " 20981,\n",
       " 20979,\n",
       " 20978,\n",
       " 20977,\n",
       " 20976,\n",
       " 20975,\n",
       " 20974,\n",
       " 20973,\n",
       " 20972,\n",
       " 20971,\n",
       " 20961,\n",
       " 20960,\n",
       " 20959,\n",
       " 20958,\n",
       " 20957,\n",
       " 20956,\n",
       " 20955,\n",
       " 20954,\n",
       " 20953,\n",
       " 20949,\n",
       " 20948,\n",
       " 20947,\n",
       " 20946,\n",
       " 20943,\n",
       " 20942,\n",
       " 20939,\n",
       " 20936,\n",
       " 20935,\n",
       " 20933,\n",
       " 20931,\n",
       " 20930,\n",
       " 20929,\n",
       " 20928,\n",
       " 20927,\n",
       " 20924,\n",
       " 20923,\n",
       " 20921,\n",
       " 20919,\n",
       " 20918,\n",
       " 20916,\n",
       " 20915,\n",
       " 20913,\n",
       " 20911,\n",
       " 20909,\n",
       " 20908,\n",
       " 20907,\n",
       " 20905,\n",
       " 20904,\n",
       " 20903,\n",
       " 20902,\n",
       " 20900,\n",
       " 20895,\n",
       " 20893,\n",
       " 20890,\n",
       " 20887,\n",
       " 20885,\n",
       " 20880,\n",
       " 20879,\n",
       " 20877,\n",
       " 20875,\n",
       " 20872,\n",
       " 20871,\n",
       " 20870,\n",
       " 20866,\n",
       " 20865,\n",
       " 20863,\n",
       " 20862,\n",
       " 20860,\n",
       " 20859,\n",
       " 20858,\n",
       " 20853,\n",
       " 20851,\n",
       " 20848,\n",
       " 20847,\n",
       " 20844,\n",
       " 20843,\n",
       " 20842,\n",
       " 20841,\n",
       " 20836,\n",
       " 20835,\n",
       " 20834,\n",
       " 20832,\n",
       " 20830,\n",
       " 20828,\n",
       " 20826,\n",
       " 20823,\n",
       " 20822,\n",
       " 20821,\n",
       " 20820,\n",
       " 20819,\n",
       " 20816,\n",
       " 20814,\n",
       " 20813,\n",
       " 20812,\n",
       " 20811,\n",
       " 20809,\n",
       " 20807,\n",
       " 20806,\n",
       " 20805,\n",
       " 20802,\n",
       " 20801,\n",
       " 20799,\n",
       " 20795,\n",
       " 20787,\n",
       " 20785,\n",
       " 20784,\n",
       " 20783,\n",
       " 20782,\n",
       " 20781,\n",
       " 20779,\n",
       " 20778,\n",
       " 20777,\n",
       " 20776,\n",
       " 20774,\n",
       " 20773,\n",
       " 20771,\n",
       " 20770,\n",
       " 20768,\n",
       " 20767,\n",
       " 20763,\n",
       " 20762,\n",
       " 20761,\n",
       " 20760,\n",
       " 20758,\n",
       " 20756,\n",
       " 20755,\n",
       " 20754,\n",
       " 20751,\n",
       " 20750,\n",
       " 20748,\n",
       " 20747,\n",
       " 20745,\n",
       " 20743,\n",
       " 20737,\n",
       " 20736,\n",
       " 20735,\n",
       " 20732,\n",
       " 20730,\n",
       " 20729,\n",
       " 20727,\n",
       " 20724,\n",
       " 20723,\n",
       " 20722,\n",
       " 20720,\n",
       " 20719,\n",
       " 20717,\n",
       " 20716,\n",
       " 20715,\n",
       " 20712,\n",
       " 20711,\n",
       " 20707,\n",
       " 20706,\n",
       " 20699,\n",
       " 20696,\n",
       " 20695,\n",
       " 20694,\n",
       " 20693,\n",
       " 20692,\n",
       " 20691,\n",
       " 20690,\n",
       " 20689,\n",
       " 20688,\n",
       " 20685,\n",
       " 20683,\n",
       " 20682,\n",
       " 20681,\n",
       " 20680,\n",
       " 20679,\n",
       " 20677,\n",
       " 20676,\n",
       " 20674,\n",
       " 20673,\n",
       " 20667,\n",
       " 20665,\n",
       " 20664,\n",
       " 20663,\n",
       " 20661,\n",
       " 20657,\n",
       " 20656,\n",
       " 20654,\n",
       " 20653,\n",
       " 20650,\n",
       " 20649,\n",
       " 20648,\n",
       " 20647,\n",
       " 20646,\n",
       " 20644,\n",
       " 20643,\n",
       " 20642,\n",
       " 20641,\n",
       " 20640,\n",
       " 20639,\n",
       " 20638,\n",
       " 20636,\n",
       " 20633,\n",
       " 20631,\n",
       " 20629,\n",
       " 20627,\n",
       " 20626,\n",
       " 20625,\n",
       " 20624,\n",
       " 20623,\n",
       " 20622,\n",
       " 20621,\n",
       " 20620,\n",
       " 20618,\n",
       " 20616,\n",
       " 20615,\n",
       " 20612,\n",
       " 20611,\n",
       " 20610,\n",
       " 20609,\n",
       " 20607,\n",
       " 20606,\n",
       " 20604,\n",
       " 20603,\n",
       " 20601,\n",
       " 20599,\n",
       " 20597,\n",
       " 20596,\n",
       " 20595,\n",
       " 20593,\n",
       " 20592,\n",
       " 20591,\n",
       " 20589,\n",
       " 20588,\n",
       " 20587,\n",
       " 20584,\n",
       " 20581,\n",
       " 20578,\n",
       " 20576,\n",
       " 20573,\n",
       " 20572,\n",
       " 20571,\n",
       " 20568,\n",
       " 20567,\n",
       " 20564,\n",
       " 20563,\n",
       " 20559,\n",
       " 20552,\n",
       " 20551,\n",
       " 20549,\n",
       " 20547,\n",
       " 20546,\n",
       " 20545,\n",
       " 20542,\n",
       " 20541,\n",
       " 20537,\n",
       " 20536,\n",
       " 20535,\n",
       " 20534,\n",
       " 20533,\n",
       " 20532,\n",
       " 20530,\n",
       " 20527,\n",
       " 20526,\n",
       " 20525,\n",
       " 20523,\n",
       " 20519,\n",
       " 20516,\n",
       " 20513,\n",
       " 20512,\n",
       " 20510,\n",
       " 20508,\n",
       " 20504,\n",
       " 20501,\n",
       " 20500,\n",
       " 20499,\n",
       " 20498,\n",
       " 20496,\n",
       " 20495,\n",
       " 20494,\n",
       " 20493,\n",
       " 20487,\n",
       " 20484,\n",
       " 20482,\n",
       " 20481,\n",
       " 20480,\n",
       " 20478,\n",
       " 20477,\n",
       " 20476,\n",
       " 20475,\n",
       " 20474,\n",
       " 20473,\n",
       " 20472,\n",
       " 20471,\n",
       " 20469,\n",
       " 20468,\n",
       " 20467,\n",
       " 20461,\n",
       " 20458,\n",
       " 20455,\n",
       " 20454,\n",
       " 20451,\n",
       " 20449,\n",
       " 20447,\n",
       " 20446,\n",
       " 20444,\n",
       " 20442,\n",
       " 20440,\n",
       " 20431,\n",
       " 20429,\n",
       " 20428,\n",
       " 20427,\n",
       " 20426,\n",
       " 20424,\n",
       " 20421,\n",
       " 20420,\n",
       " 20419,\n",
       " 20417,\n",
       " 20416,\n",
       " 20415,\n",
       " 20414,\n",
       " 20413,\n",
       " 20410,\n",
       " 20409,\n",
       " 20408,\n",
       " 20407,\n",
       " 20405,\n",
       " 20397,\n",
       " 20395,\n",
       " 20393,\n",
       " 20392,\n",
       " 20391,\n",
       " 20389,\n",
       " 20388,\n",
       " 20387,\n",
       " 20386,\n",
       " 20385,\n",
       " 20384,\n",
       " 20382,\n",
       " 20379,\n",
       " 20378,\n",
       " 20377,\n",
       " 20375,\n",
       " 20373,\n",
       " 20372,\n",
       " 20370,\n",
       " 20369,\n",
       " 20368,\n",
       " 20367,\n",
       " 20364,\n",
       " 20362,\n",
       " 20359,\n",
       " 20358,\n",
       " 20354,\n",
       " 20353,\n",
       " 20351,\n",
       " 20350,\n",
       " 20348,\n",
       " 20347,\n",
       " 20346,\n",
       " 20345,\n",
       " 20344,\n",
       " 20343,\n",
       " 20341,\n",
       " 20340,\n",
       " 20338,\n",
       " 20337,\n",
       " 20331,\n",
       " 20328,\n",
       " 20327,\n",
       " 20324,\n",
       " 20323,\n",
       " 20322,\n",
       " 20319,\n",
       " 20317,\n",
       " 20315,\n",
       " 20314,\n",
       " 20312,\n",
       " 20311,\n",
       " 20309,\n",
       " 20306,\n",
       " 20305,\n",
       " 20304,\n",
       " 20303,\n",
       " 20300,\n",
       " 20299,\n",
       " 20298,\n",
       " 20296,\n",
       " 20295,\n",
       " 20292,\n",
       " 20291,\n",
       " 20289,\n",
       " 20288,\n",
       " 20287,\n",
       " 20286,\n",
       " 20285,\n",
       " 20284,\n",
       " 20281,\n",
       " 20280,\n",
       " 20279,\n",
       " 20276,\n",
       " 20274,\n",
       " 20273,\n",
       " 20271,\n",
       " 20270,\n",
       " 20267,\n",
       " 20266,\n",
       " 20265,\n",
       " 20263,\n",
       " 20261,\n",
       " 20258,\n",
       " 20257,\n",
       " 20256,\n",
       " 20255,\n",
       " 20254,\n",
       " 20253,\n",
       " 20251,\n",
       " 20250,\n",
       " 20249,\n",
       " 20248,\n",
       " 20247,\n",
       " 20246,\n",
       " 20244,\n",
       " 20242,\n",
       " 20241,\n",
       " 20240,\n",
       " 20239,\n",
       " 20234,\n",
       " 20233,\n",
       " 20231,\n",
       " 20230,\n",
       " 20228,\n",
       " 20224,\n",
       " 20223,\n",
       " 20221,\n",
       " 20220,\n",
       " 20219,\n",
       " 20216,\n",
       " 20215,\n",
       " 20214,\n",
       " 20210,\n",
       " 20208,\n",
       " 20207,\n",
       " 20206,\n",
       " 20201,\n",
       " 20200,\n",
       " 20199,\n",
       " 20197,\n",
       " 20194,\n",
       " 20191,\n",
       " 20189,\n",
       " 20187,\n",
       " 20186,\n",
       " 20185,\n",
       " 20183,\n",
       " 20180,\n",
       " 20179,\n",
       " 20178,\n",
       " 20174,\n",
       " 20172,\n",
       " 20169,\n",
       " 20168,\n",
       " 20164,\n",
       " 20163,\n",
       " 20162,\n",
       " 20159,\n",
       " 20157,\n",
       " 20155,\n",
       " 20154,\n",
       " 20152,\n",
       " 20151,\n",
       " 20149,\n",
       " 20144,\n",
       " 20140,\n",
       " 20139,\n",
       " 20134,\n",
       " 20132,\n",
       " 20131,\n",
       " 20129,\n",
       " 20128,\n",
       " 20125,\n",
       " 20124,\n",
       " 20122,\n",
       " 20121,\n",
       " 20118,\n",
       " 20117,\n",
       " 20115,\n",
       " 20114,\n",
       " 20110,\n",
       " 20108,\n",
       " 20107,\n",
       " 20106,\n",
       " 20105,\n",
       " 20104,\n",
       " 20103,\n",
       " 20102,\n",
       " 20100,\n",
       " 20099,\n",
       " 20098,\n",
       " 20097,\n",
       " 20094,\n",
       " 20093,\n",
       " 20092,\n",
       " 20090,\n",
       " 20084,\n",
       " 20081,\n",
       " 20080,\n",
       " 20078,\n",
       " 20077,\n",
       " 20076,\n",
       " 20073,\n",
       " 20069,\n",
       " 20066,\n",
       " 20064,\n",
       " 20062,\n",
       " 20061,\n",
       " 20060,\n",
       " 20059,\n",
       " 20058,\n",
       " 20057,\n",
       " 20053,\n",
       " 20048,\n",
       " 20046,\n",
       " 20043,\n",
       " 20042,\n",
       " 20041,\n",
       " 20038,\n",
       " 20037,\n",
       " 20034,\n",
       " 20033,\n",
       " 20032,\n",
       " 20030,\n",
       " 20027,\n",
       " 20025,\n",
       " 20020,\n",
       " 20019,\n",
       " 20017,\n",
       " 20016,\n",
       " 20015,\n",
       " 20014,\n",
       " 20012,\n",
       " 20010,\n",
       " 20006,\n",
       " 20001,\n",
       " 20000,\n",
       " 19997,\n",
       " 19994,\n",
       " 19993,\n",
       " 19991,\n",
       " 19989,\n",
       " 19985,\n",
       " 19984,\n",
       " 19980,\n",
       " 19974,\n",
       " 19973,\n",
       " 19972,\n",
       " 19971,\n",
       " 19969,\n",
       " 19968,\n",
       " 19966,\n",
       " 19964,\n",
       " 19962,\n",
       " 19961,\n",
       " 19960,\n",
       " 19959,\n",
       " 19955,\n",
       " 19954,\n",
       " 19953,\n",
       " 19952,\n",
       " 19951,\n",
       " 19950,\n",
       " 19949,\n",
       " 19948,\n",
       " 19947,\n",
       " 19945,\n",
       " 19944,\n",
       " 19939,\n",
       " 19937,\n",
       " 19935,\n",
       " 19929,\n",
       " 19924,\n",
       " 19923,\n",
       " 19922,\n",
       " 19921,\n",
       " 19920,\n",
       " 19919,\n",
       " 19918,\n",
       " 19917,\n",
       " 19916,\n",
       " 19914,\n",
       " 19913,\n",
       " 19912,\n",
       " 19911,\n",
       " 19910,\n",
       " 19908,\n",
       " 19906,\n",
       " 19903,\n",
       " 19902,\n",
       " 19897,\n",
       " 19894,\n",
       " 19893,\n",
       " 19892,\n",
       " 19891,\n",
       " 19889,\n",
       " 19885,\n",
       " 19884,\n",
       " 19883,\n",
       " 19881,\n",
       " 19880,\n",
       " 19879,\n",
       " 19877,\n",
       " 19875,\n",
       " 19874,\n",
       " 19869,\n",
       " 19868,\n",
       " 19867,\n",
       " 19866,\n",
       " 19865,\n",
       " 19862,\n",
       " 19860,\n",
       " 19858,\n",
       " 19857,\n",
       " 19853,\n",
       " 19852,\n",
       " 19850,\n",
       " 19849,\n",
       " 19848,\n",
       " 19847,\n",
       " 19846,\n",
       " 19845,\n",
       " 19844,\n",
       " 19843,\n",
       " 19842,\n",
       " 19841,\n",
       " 19838,\n",
       " 19835,\n",
       " 19834,\n",
       " 19831,\n",
       " 19823,\n",
       " 19819,\n",
       " 19818,\n",
       " 19817,\n",
       " 19816,\n",
       " 19815,\n",
       " 19814,\n",
       " 19813,\n",
       " 19812,\n",
       " 19810,\n",
       " 19809,\n",
       " 19808,\n",
       " 19807,\n",
       " 19806,\n",
       " 19803,\n",
       " 19802,\n",
       " 19800,\n",
       " 19798,\n",
       " 19795,\n",
       " 19793,\n",
       " 19789,\n",
       " 19785,\n",
       " 19784,\n",
       " 19783,\n",
       " 19782,\n",
       " 19779,\n",
       " 19774,\n",
       " 19773,\n",
       " 19772,\n",
       " 19771,\n",
       " 19770,\n",
       " 19768,\n",
       " 19767,\n",
       " 19766,\n",
       " 19765,\n",
       " 19764,\n",
       " 19763,\n",
       " 19762,\n",
       " 19759,\n",
       " 19758,\n",
       " 19757,\n",
       " 19754,\n",
       " 19750,\n",
       " 19749,\n",
       " 19748,\n",
       " 19746,\n",
       " 19745,\n",
       " 19742,\n",
       " 19741,\n",
       " 19740,\n",
       " 19739,\n",
       " 19738,\n",
       " 19736,\n",
       " 19734,\n",
       " 19732,\n",
       " 19730,\n",
       " 19729,\n",
       " 19728,\n",
       " 19727,\n",
       " 19725,\n",
       " 19724,\n",
       " 19723,\n",
       " 19719,\n",
       " 19716,\n",
       " 19715,\n",
       " 19712,\n",
       " 19711,\n",
       " 19710,\n",
       " 19709,\n",
       " 19708,\n",
       " 19707,\n",
       " 19706,\n",
       " 19701,\n",
       " 19699,\n",
       " 19694,\n",
       " 19693,\n",
       " 19690,\n",
       " 19685,\n",
       " 19684,\n",
       " 19683,\n",
       " 19682,\n",
       " 19679,\n",
       " 19676,\n",
       " 19675,\n",
       " 19674,\n",
       " 19673,\n",
       " 19670,\n",
       " 19668,\n",
       " 19665,\n",
       " 19664,\n",
       " 19662,\n",
       " 19659,\n",
       " 19658,\n",
       " 19656,\n",
       " 19655,\n",
       " 19654,\n",
       " 19653,\n",
       " 19652,\n",
       " 19650,\n",
       " 19648,\n",
       " 19647,\n",
       " 19645,\n",
       " 19643,\n",
       " 19640,\n",
       " 19639,\n",
       " 19638,\n",
       " 19636,\n",
       " 19635,\n",
       " 19634,\n",
       " 19633,\n",
       " 19631,\n",
       " 19630,\n",
       " 19629,\n",
       " 19628,\n",
       " 19626,\n",
       " 19625,\n",
       " 19624,\n",
       " 19623,\n",
       " 19619,\n",
       " 19618,\n",
       " 19617,\n",
       " 19615,\n",
       " 19613,\n",
       " 19611,\n",
       " 19603,\n",
       " 19602,\n",
       " 19600,\n",
       " 19599,\n",
       " 19595,\n",
       " 19594,\n",
       " 19592,\n",
       " 19591,\n",
       " 19589,\n",
       " 19587,\n",
       " 19586,\n",
       " 19585,\n",
       " 19584,\n",
       " 19580,\n",
       " 19579,\n",
       " 19578,\n",
       " 19577,\n",
       " 19575,\n",
       " 19573,\n",
       " 19571,\n",
       " 19569,\n",
       " 19568,\n",
       " 19567,\n",
       " 19566,\n",
       " 19565,\n",
       " 19562,\n",
       " 19560,\n",
       " 19551,\n",
       " 19549,\n",
       " 19547,\n",
       " 19544,\n",
       " 19543,\n",
       " 19542,\n",
       " 19540,\n",
       " 19538,\n",
       " 19537,\n",
       " 19535,\n",
       " 19533,\n",
       " 19532,\n",
       " 19530,\n",
       " 19529,\n",
       " 19528,\n",
       " 19527,\n",
       " 19523,\n",
       " 19522,\n",
       " 19521,\n",
       " 19520,\n",
       " 19518,\n",
       " 19517,\n",
       " 19514,\n",
       " 19513,\n",
       " 19512,\n",
       " 19511,\n",
       " 19510,\n",
       " 19509,\n",
       " 19508,\n",
       " 19507,\n",
       " 19506,\n",
       " 19505,\n",
       " 19504,\n",
       " 19502,\n",
       " 19501,\n",
       " 19500,\n",
       " 19499,\n",
       " 19498,\n",
       " 19494,\n",
       " 19493,\n",
       " 19487,\n",
       " 19485,\n",
       " 19484,\n",
       " 19483,\n",
       " 19482,\n",
       " 19481,\n",
       " 19480,\n",
       " 19479,\n",
       " 19478,\n",
       " 19475,\n",
       " 19472,\n",
       " 19471,\n",
       " 19470,\n",
       " 19468,\n",
       " 19467,\n",
       " 19464,\n",
       " 19463,\n",
       " 19461,\n",
       " 19460,\n",
       " 19459,\n",
       " 19456,\n",
       " 19455,\n",
       " 19450,\n",
       " 19449,\n",
       " 19447,\n",
       " 19445,\n",
       " 19442,\n",
       " 19441,\n",
       " 19439,\n",
       " 19435,\n",
       " 19434,\n",
       " 19433,\n",
       " 19431,\n",
       " 19430,\n",
       " 19429,\n",
       " 19425,\n",
       " 19423,\n",
       " 19416,\n",
       " 19415,\n",
       " 19414,\n",
       " 19411,\n",
       " 19410,\n",
       " 19402,\n",
       " 19401,\n",
       " 19399,\n",
       " 19398,\n",
       " 19397,\n",
       " 19394,\n",
       " 19393,\n",
       " 19392,\n",
       " 19390,\n",
       " 19384,\n",
       " 19383,\n",
       " 19382,\n",
       " 19381,\n",
       " 19380,\n",
       " 19379,\n",
       " 19376,\n",
       " 19375,\n",
       " 19374,\n",
       " 19370,\n",
       " 19366,\n",
       " 19364,\n",
       " 19362,\n",
       " 19361,\n",
       " 19358,\n",
       " 19356,\n",
       " 19355,\n",
       " 19353,\n",
       " 19349,\n",
       " 19348,\n",
       " 19346,\n",
       " 19344,\n",
       " 19341,\n",
       " 19340,\n",
       " 19339,\n",
       " 19338,\n",
       " 19336,\n",
       " 19335,\n",
       " 19334,\n",
       " 19333,\n",
       " 19330,\n",
       " 19327,\n",
       " 19324,\n",
       " 19323,\n",
       " 19322,\n",
       " 19321,\n",
       " 19320,\n",
       " 19319,\n",
       " 19318,\n",
       " 19317,\n",
       " 19316,\n",
       " 19315,\n",
       " 19314,\n",
       " 19313,\n",
       " 19312,\n",
       " 19309,\n",
       " 19307,\n",
       " 19306,\n",
       " 19304,\n",
       " 19303,\n",
       " 19302,\n",
       " 19301,\n",
       " 19297,\n",
       " 19293,\n",
       " 19290,\n",
       " 19289,\n",
       " 19288,\n",
       " 19284,\n",
       " 19283,\n",
       " 19282,\n",
       " 19281,\n",
       " 19277,\n",
       " 19276,\n",
       " 19273,\n",
       " 19272,\n",
       " ...]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(list(vocab), reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "21124 in v2i.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[20166,\n",
       " 20165,\n",
       " 20164,\n",
       " 20163,\n",
       " 20162,\n",
       " 20161,\n",
       " 20160,\n",
       " 20159,\n",
       " 20158,\n",
       " 20157,\n",
       " 20156,\n",
       " 20155,\n",
       " 20154,\n",
       " 20153,\n",
       " 20152,\n",
       " 20151,\n",
       " 20150,\n",
       " 20149,\n",
       " 20148,\n",
       " 20147,\n",
       " 20146,\n",
       " 20145,\n",
       " 20144,\n",
       " 20143,\n",
       " 20142,\n",
       " 20141,\n",
       " 20140,\n",
       " 20139,\n",
       " 20138,\n",
       " 20137,\n",
       " 20136,\n",
       " 20135,\n",
       " 20134,\n",
       " 20133,\n",
       " 20132,\n",
       " 20131,\n",
       " 20130,\n",
       " 20129,\n",
       " 20128,\n",
       " 20127,\n",
       " 20126,\n",
       " 20125,\n",
       " 20124,\n",
       " 20123,\n",
       " 20122,\n",
       " 20121,\n",
       " 20120,\n",
       " 20119,\n",
       " 20118,\n",
       " 20117,\n",
       " 20116,\n",
       " 20115,\n",
       " 20114,\n",
       " 20113,\n",
       " 20112,\n",
       " 20111,\n",
       " 20110,\n",
       " 20109,\n",
       " 20108,\n",
       " 20107,\n",
       " 20106,\n",
       " 20105,\n",
       " 20104,\n",
       " 20103,\n",
       " 20102,\n",
       " 20101,\n",
       " 20100,\n",
       " 20099,\n",
       " 20098,\n",
       " 20097,\n",
       " 20096,\n",
       " 20095,\n",
       " 20094,\n",
       " 20093,\n",
       " 20092,\n",
       " 20091,\n",
       " 20090,\n",
       " 20089,\n",
       " 20088,\n",
       " 20087,\n",
       " 20086,\n",
       " 20085,\n",
       " 20084,\n",
       " 20083,\n",
       " 20082,\n",
       " 20081,\n",
       " 20080,\n",
       " 20079,\n",
       " 20078,\n",
       " 20077,\n",
       " 20076,\n",
       " 20075,\n",
       " 20074,\n",
       " 20073,\n",
       " 20072,\n",
       " 20071,\n",
       " 20070,\n",
       " 20069,\n",
       " 20068,\n",
       " 20067,\n",
       " 20066,\n",
       " 20065,\n",
       " 20064,\n",
       " 20063,\n",
       " 20062,\n",
       " 20061,\n",
       " 20060,\n",
       " 20059,\n",
       " 20058,\n",
       " 20057,\n",
       " 20056,\n",
       " 20055,\n",
       " 20054,\n",
       " 20053,\n",
       " 20052,\n",
       " 20051,\n",
       " 20050,\n",
       " 20049,\n",
       " 20048,\n",
       " 20047,\n",
       " 20046,\n",
       " 20045,\n",
       " 20044,\n",
       " 20043,\n",
       " 20042,\n",
       " 20041,\n",
       " 20040,\n",
       " 20039,\n",
       " 20038,\n",
       " 20037,\n",
       " 20036,\n",
       " 20035,\n",
       " 20034,\n",
       " 20033,\n",
       " 20032,\n",
       " 20031,\n",
       " 20030,\n",
       " 20029,\n",
       " 20028,\n",
       " 20027,\n",
       " 20026,\n",
       " 20025,\n",
       " 20024,\n",
       " 20023,\n",
       " 20022,\n",
       " 20021,\n",
       " 20020,\n",
       " 20019,\n",
       " 20018,\n",
       " 20017,\n",
       " 20016,\n",
       " 20015,\n",
       " 20014,\n",
       " 20013,\n",
       " 20012,\n",
       " 20011,\n",
       " 20010,\n",
       " 20009,\n",
       " 20008,\n",
       " 20007,\n",
       " 20006,\n",
       " 20005,\n",
       " 20004,\n",
       " 20003,\n",
       " 20002,\n",
       " 20001,\n",
       " 20000,\n",
       " 19999,\n",
       " 19998,\n",
       " 19997,\n",
       " 19996,\n",
       " 19995,\n",
       " 19994,\n",
       " 19993,\n",
       " 19992,\n",
       " 19991,\n",
       " 19990,\n",
       " 19989,\n",
       " 19988,\n",
       " 19987,\n",
       " 19986,\n",
       " 19985,\n",
       " 19984,\n",
       " 19983,\n",
       " 19982,\n",
       " 19981,\n",
       " 19980,\n",
       " 19979,\n",
       " 19978,\n",
       " 19977,\n",
       " 19976,\n",
       " 19975,\n",
       " 19974,\n",
       " 19973,\n",
       " 19972,\n",
       " 19971,\n",
       " 19970,\n",
       " 19969,\n",
       " 19968,\n",
       " 19967,\n",
       " 19966,\n",
       " 19965,\n",
       " 19964,\n",
       " 19963,\n",
       " 19962,\n",
       " 19961,\n",
       " 19960,\n",
       " 19959,\n",
       " 19958,\n",
       " 19957,\n",
       " 19956,\n",
       " 19955,\n",
       " 19954,\n",
       " 19953,\n",
       " 19952,\n",
       " 19951,\n",
       " 19950,\n",
       " 19949,\n",
       " 19948,\n",
       " 19947,\n",
       " 19946,\n",
       " 19945,\n",
       " 19944,\n",
       " 19943,\n",
       " 19942,\n",
       " 19941,\n",
       " 19940,\n",
       " 19939,\n",
       " 19938,\n",
       " 19937,\n",
       " 19936,\n",
       " 19935,\n",
       " 19934,\n",
       " 19933,\n",
       " 19932,\n",
       " 19931,\n",
       " 19930,\n",
       " 19929,\n",
       " 19928,\n",
       " 19927,\n",
       " 19926,\n",
       " 19925,\n",
       " 19924,\n",
       " 19923,\n",
       " 19922,\n",
       " 19921,\n",
       " 19920,\n",
       " 19919,\n",
       " 19918,\n",
       " 19917,\n",
       " 19916,\n",
       " 19915,\n",
       " 19914,\n",
       " 19913,\n",
       " 19912,\n",
       " 19911,\n",
       " 19910,\n",
       " 19909,\n",
       " 19908,\n",
       " 19907,\n",
       " 19906,\n",
       " 19905,\n",
       " 19904,\n",
       " 19903,\n",
       " 19902,\n",
       " 19901,\n",
       " 19900,\n",
       " 19899,\n",
       " 19898,\n",
       " 19897,\n",
       " 19896,\n",
       " 19895,\n",
       " 19894,\n",
       " 19893,\n",
       " 19892,\n",
       " 19891,\n",
       " 19890,\n",
       " 19889,\n",
       " 19888,\n",
       " 19887,\n",
       " 19886,\n",
       " 19885,\n",
       " 19884,\n",
       " 19883,\n",
       " 19882,\n",
       " 19881,\n",
       " 19880,\n",
       " 19879,\n",
       " 19878,\n",
       " 19877,\n",
       " 19876,\n",
       " 19875,\n",
       " 19874,\n",
       " 19873,\n",
       " 19872,\n",
       " 19871,\n",
       " 19870,\n",
       " 19869,\n",
       " 19868,\n",
       " 19867,\n",
       " 19866,\n",
       " 19865,\n",
       " 19864,\n",
       " 19863,\n",
       " 19862,\n",
       " 19861,\n",
       " 19860,\n",
       " 19859,\n",
       " 19858,\n",
       " 19857,\n",
       " 19856,\n",
       " 19855,\n",
       " 19854,\n",
       " 19853,\n",
       " 19852,\n",
       " 19851,\n",
       " 19850,\n",
       " 19849,\n",
       " 19848,\n",
       " 19847,\n",
       " 19846,\n",
       " 19845,\n",
       " 19844,\n",
       " 19843,\n",
       " 19842,\n",
       " 19841,\n",
       " 19840,\n",
       " 19839,\n",
       " 19838,\n",
       " 19837,\n",
       " 19836,\n",
       " 19835,\n",
       " 19834,\n",
       " 19833,\n",
       " 19832,\n",
       " 19831,\n",
       " 19830,\n",
       " 19829,\n",
       " 19828,\n",
       " 19827,\n",
       " 19826,\n",
       " 19825,\n",
       " 19824,\n",
       " 19823,\n",
       " 19822,\n",
       " 19821,\n",
       " 19820,\n",
       " 19819,\n",
       " 19818,\n",
       " 19817,\n",
       " 19816,\n",
       " 19815,\n",
       " 19814,\n",
       " 19813,\n",
       " 19812,\n",
       " 19811,\n",
       " 19810,\n",
       " 19809,\n",
       " 19808,\n",
       " 19807,\n",
       " 19806,\n",
       " 19805,\n",
       " 19804,\n",
       " 19803,\n",
       " 19802,\n",
       " 19801,\n",
       " 19800,\n",
       " 19799,\n",
       " 19798,\n",
       " 19797,\n",
       " 19796,\n",
       " 19795,\n",
       " 19794,\n",
       " 19793,\n",
       " 19792,\n",
       " 19791,\n",
       " 19790,\n",
       " 19789,\n",
       " 19788,\n",
       " 19787,\n",
       " 19786,\n",
       " 19785,\n",
       " 19784,\n",
       " 19783,\n",
       " 19782,\n",
       " 19781,\n",
       " 19780,\n",
       " 19779,\n",
       " 19778,\n",
       " 19777,\n",
       " 19776,\n",
       " 19775,\n",
       " 19774,\n",
       " 19773,\n",
       " 19772,\n",
       " 19771,\n",
       " 19770,\n",
       " 19769,\n",
       " 19768,\n",
       " 19767,\n",
       " 19766,\n",
       " 19765,\n",
       " 19764,\n",
       " 19763,\n",
       " 19762,\n",
       " 19761,\n",
       " 19760,\n",
       " 19759,\n",
       " 19758,\n",
       " 19757,\n",
       " 19756,\n",
       " 19755,\n",
       " 19754,\n",
       " 19753,\n",
       " 19752,\n",
       " 19751,\n",
       " 19750,\n",
       " 19749,\n",
       " 19748,\n",
       " 19747,\n",
       " 19746,\n",
       " 19745,\n",
       " 19744,\n",
       " 19743,\n",
       " 19742,\n",
       " 19741,\n",
       " 19740,\n",
       " 19739,\n",
       " 19738,\n",
       " 19737,\n",
       " 19736,\n",
       " 19735,\n",
       " 19734,\n",
       " 19733,\n",
       " 19732,\n",
       " 19731,\n",
       " 19730,\n",
       " 19729,\n",
       " 19728,\n",
       " 19727,\n",
       " 19726,\n",
       " 19725,\n",
       " 19724,\n",
       " 19723,\n",
       " 19722,\n",
       " 19721,\n",
       " 19720,\n",
       " 19719,\n",
       " 19718,\n",
       " 19717,\n",
       " 19716,\n",
       " 19715,\n",
       " 19714,\n",
       " 19713,\n",
       " 19712,\n",
       " 19711,\n",
       " 19710,\n",
       " 19709,\n",
       " 19708,\n",
       " 19707,\n",
       " 19706,\n",
       " 19705,\n",
       " 19704,\n",
       " 19703,\n",
       " 19702,\n",
       " 19701,\n",
       " 19700,\n",
       " 19699,\n",
       " 19698,\n",
       " 19697,\n",
       " 19696,\n",
       " 19695,\n",
       " 19694,\n",
       " 19693,\n",
       " 19692,\n",
       " 19691,\n",
       " 19690,\n",
       " 19689,\n",
       " 19688,\n",
       " 19687,\n",
       " 19686,\n",
       " 19685,\n",
       " 19684,\n",
       " 19683,\n",
       " 19682,\n",
       " 19681,\n",
       " 19680,\n",
       " 19679,\n",
       " 19678,\n",
       " 19677,\n",
       " 19676,\n",
       " 19675,\n",
       " 19674,\n",
       " 19673,\n",
       " 19672,\n",
       " 19671,\n",
       " 19670,\n",
       " 19669,\n",
       " 19668,\n",
       " 19667,\n",
       " 19666,\n",
       " 19665,\n",
       " 19664,\n",
       " 19663,\n",
       " 19662,\n",
       " 19661,\n",
       " 19660,\n",
       " 19659,\n",
       " 19658,\n",
       " 19657,\n",
       " 19656,\n",
       " 19655,\n",
       " 19654,\n",
       " 19653,\n",
       " 19652,\n",
       " 19651,\n",
       " 19650,\n",
       " 19649,\n",
       " 19648,\n",
       " 19647,\n",
       " 19646,\n",
       " 19645,\n",
       " 19644,\n",
       " 19643,\n",
       " 19642,\n",
       " 19641,\n",
       " 19640,\n",
       " 19639,\n",
       " 19638,\n",
       " 19637,\n",
       " 19636,\n",
       " 19635,\n",
       " 19634,\n",
       " 19633,\n",
       " 19632,\n",
       " 19631,\n",
       " 19630,\n",
       " 19629,\n",
       " 19628,\n",
       " 19627,\n",
       " 19626,\n",
       " 19625,\n",
       " 19624,\n",
       " 19623,\n",
       " 19622,\n",
       " 19621,\n",
       " 19620,\n",
       " 19619,\n",
       " 19618,\n",
       " 19617,\n",
       " 19616,\n",
       " 19615,\n",
       " 19614,\n",
       " 19613,\n",
       " 19612,\n",
       " 19611,\n",
       " 19610,\n",
       " 19609,\n",
       " 19608,\n",
       " 19607,\n",
       " 19606,\n",
       " 19605,\n",
       " 19604,\n",
       " 19603,\n",
       " 19602,\n",
       " 19601,\n",
       " 19600,\n",
       " 19599,\n",
       " 19598,\n",
       " 19597,\n",
       " 19596,\n",
       " 19595,\n",
       " 19594,\n",
       " 19593,\n",
       " 19592,\n",
       " 19591,\n",
       " 19590,\n",
       " 19589,\n",
       " 19588,\n",
       " 19587,\n",
       " 19586,\n",
       " 19585,\n",
       " 19584,\n",
       " 19583,\n",
       " 19582,\n",
       " 19581,\n",
       " 19580,\n",
       " 19579,\n",
       " 19578,\n",
       " 19577,\n",
       " 19576,\n",
       " 19575,\n",
       " 19574,\n",
       " 19573,\n",
       " 19572,\n",
       " 19571,\n",
       " 19570,\n",
       " 19569,\n",
       " 19568,\n",
       " 19567,\n",
       " 19566,\n",
       " 19565,\n",
       " 19564,\n",
       " 19563,\n",
       " 19562,\n",
       " 19561,\n",
       " 19560,\n",
       " 19559,\n",
       " 19558,\n",
       " 19557,\n",
       " 19556,\n",
       " 19555,\n",
       " 19554,\n",
       " 19553,\n",
       " 19552,\n",
       " 19551,\n",
       " 19550,\n",
       " 19549,\n",
       " 19548,\n",
       " 19547,\n",
       " 19546,\n",
       " 19545,\n",
       " 19544,\n",
       " 19543,\n",
       " 19542,\n",
       " 19541,\n",
       " 19540,\n",
       " 19539,\n",
       " 19538,\n",
       " 19537,\n",
       " 19536,\n",
       " 19535,\n",
       " 19534,\n",
       " 19533,\n",
       " 19532,\n",
       " 19531,\n",
       " 19530,\n",
       " 19529,\n",
       " 19528,\n",
       " 19527,\n",
       " 19526,\n",
       " 19525,\n",
       " 19524,\n",
       " 19523,\n",
       " 19522,\n",
       " 19521,\n",
       " 19520,\n",
       " 19519,\n",
       " 19518,\n",
       " 19517,\n",
       " 19516,\n",
       " 19515,\n",
       " 19514,\n",
       " 19513,\n",
       " 19512,\n",
       " 19511,\n",
       " 19510,\n",
       " 19509,\n",
       " 19508,\n",
       " 19507,\n",
       " 19506,\n",
       " 19505,\n",
       " 19504,\n",
       " 19503,\n",
       " 19502,\n",
       " 19501,\n",
       " 19500,\n",
       " 19499,\n",
       " 19498,\n",
       " 19497,\n",
       " 19496,\n",
       " 19495,\n",
       " 19494,\n",
       " 19493,\n",
       " 19492,\n",
       " 19491,\n",
       " 19490,\n",
       " 19489,\n",
       " 19488,\n",
       " 19487,\n",
       " 19486,\n",
       " 19485,\n",
       " 19484,\n",
       " 19483,\n",
       " 19482,\n",
       " 19481,\n",
       " 19480,\n",
       " 19479,\n",
       " 19478,\n",
       " 19477,\n",
       " 19476,\n",
       " 19475,\n",
       " 19474,\n",
       " 19473,\n",
       " 19472,\n",
       " 19471,\n",
       " 19470,\n",
       " 19469,\n",
       " 19468,\n",
       " 19467,\n",
       " 19466,\n",
       " 19465,\n",
       " 19464,\n",
       " 19463,\n",
       " 19462,\n",
       " 19461,\n",
       " 19460,\n",
       " 19459,\n",
       " 19458,\n",
       " 19457,\n",
       " 19456,\n",
       " 19455,\n",
       " 19454,\n",
       " 19453,\n",
       " 19452,\n",
       " 19451,\n",
       " 19450,\n",
       " 19449,\n",
       " 19448,\n",
       " 19447,\n",
       " 19446,\n",
       " 19445,\n",
       " 19444,\n",
       " 19443,\n",
       " 19442,\n",
       " 19441,\n",
       " 19440,\n",
       " 19439,\n",
       " 19438,\n",
       " 19437,\n",
       " 19436,\n",
       " 19435,\n",
       " 19434,\n",
       " 19433,\n",
       " 19432,\n",
       " 19431,\n",
       " 19430,\n",
       " 19429,\n",
       " 19428,\n",
       " 19427,\n",
       " 19426,\n",
       " 19425,\n",
       " 19424,\n",
       " 19423,\n",
       " 19422,\n",
       " 19421,\n",
       " 19420,\n",
       " 19419,\n",
       " 19418,\n",
       " 19417,\n",
       " 19416,\n",
       " 19415,\n",
       " 19414,\n",
       " 19413,\n",
       " 19412,\n",
       " 19411,\n",
       " 19410,\n",
       " 19409,\n",
       " 19408,\n",
       " 19407,\n",
       " 19406,\n",
       " 19405,\n",
       " 19404,\n",
       " 19403,\n",
       " 19402,\n",
       " 19401,\n",
       " 19400,\n",
       " 19399,\n",
       " 19398,\n",
       " 19397,\n",
       " 19396,\n",
       " 19395,\n",
       " 19394,\n",
       " 19393,\n",
       " 19392,\n",
       " 19391,\n",
       " 19390,\n",
       " 19389,\n",
       " 19388,\n",
       " 19387,\n",
       " 19386,\n",
       " 19385,\n",
       " 19384,\n",
       " 19383,\n",
       " 19382,\n",
       " 19381,\n",
       " 19380,\n",
       " 19379,\n",
       " 19378,\n",
       " 19377,\n",
       " 19376,\n",
       " 19375,\n",
       " 19374,\n",
       " 19373,\n",
       " 19372,\n",
       " 19371,\n",
       " 19370,\n",
       " 19369,\n",
       " 19368,\n",
       " 19367,\n",
       " 19366,\n",
       " 19365,\n",
       " 19364,\n",
       " 19363,\n",
       " 19362,\n",
       " 19361,\n",
       " 19360,\n",
       " 19359,\n",
       " 19358,\n",
       " 19357,\n",
       " 19356,\n",
       " 19355,\n",
       " 19354,\n",
       " 19353,\n",
       " 19352,\n",
       " 19351,\n",
       " 19350,\n",
       " 19349,\n",
       " 19348,\n",
       " 19347,\n",
       " 19346,\n",
       " 19345,\n",
       " 19344,\n",
       " 19343,\n",
       " 19342,\n",
       " 19341,\n",
       " 19340,\n",
       " 19339,\n",
       " 19338,\n",
       " 19337,\n",
       " 19336,\n",
       " 19335,\n",
       " 19334,\n",
       " 19333,\n",
       " 19332,\n",
       " 19331,\n",
       " 19330,\n",
       " 19329,\n",
       " 19328,\n",
       " 19327,\n",
       " 19326,\n",
       " 19325,\n",
       " 19324,\n",
       " 19323,\n",
       " 19322,\n",
       " 19321,\n",
       " 19320,\n",
       " 19319,\n",
       " 19318,\n",
       " 19317,\n",
       " 19316,\n",
       " 19315,\n",
       " 19314,\n",
       " 19313,\n",
       " 19312,\n",
       " 19311,\n",
       " 19310,\n",
       " 19309,\n",
       " 19308,\n",
       " 19307,\n",
       " 19306,\n",
       " 19305,\n",
       " 19304,\n",
       " 19303,\n",
       " 19302,\n",
       " 19301,\n",
       " 19300,\n",
       " 19299,\n",
       " 19298,\n",
       " 19297,\n",
       " 19296,\n",
       " 19295,\n",
       " 19294,\n",
       " 19293,\n",
       " 19292,\n",
       " 19291,\n",
       " 19290,\n",
       " 19289,\n",
       " 19288,\n",
       " 19287,\n",
       " 19286,\n",
       " 19285,\n",
       " 19284,\n",
       " 19283,\n",
       " 19282,\n",
       " 19281,\n",
       " 19280,\n",
       " 19279,\n",
       " 19278,\n",
       " 19277,\n",
       " 19276,\n",
       " 19275,\n",
       " 19274,\n",
       " 19273,\n",
       " 19272,\n",
       " 19271,\n",
       " 19270,\n",
       " 19269,\n",
       " 19268,\n",
       " 19267,\n",
       " 19266,\n",
       " 19265,\n",
       " 19264,\n",
       " 19263,\n",
       " 19262,\n",
       " 19261,\n",
       " 19260,\n",
       " 19259,\n",
       " 19258,\n",
       " 19257,\n",
       " 19256,\n",
       " 19255,\n",
       " 19254,\n",
       " 19253,\n",
       " 19252,\n",
       " 19251,\n",
       " 19250,\n",
       " 19249,\n",
       " 19248,\n",
       " 19247,\n",
       " 19246,\n",
       " 19245,\n",
       " 19244,\n",
       " 19243,\n",
       " 19242,\n",
       " 19241,\n",
       " 19240,\n",
       " 19239,\n",
       " 19238,\n",
       " 19237,\n",
       " 19236,\n",
       " 19235,\n",
       " 19234,\n",
       " 19233,\n",
       " 19232,\n",
       " 19231,\n",
       " 19230,\n",
       " 19229,\n",
       " 19228,\n",
       " 19227,\n",
       " 19226,\n",
       " 19225,\n",
       " 19224,\n",
       " 19223,\n",
       " 19222,\n",
       " 19221,\n",
       " 19220,\n",
       " 19219,\n",
       " 19218,\n",
       " 19217,\n",
       " 19216,\n",
       " 19215,\n",
       " 19214,\n",
       " 19213,\n",
       " 19212,\n",
       " 19211,\n",
       " 19210,\n",
       " 19209,\n",
       " 19208,\n",
       " 19207,\n",
       " 19206,\n",
       " 19205,\n",
       " 19204,\n",
       " 19203,\n",
       " 19202,\n",
       " 19201,\n",
       " 19200,\n",
       " 19199,\n",
       " 19198,\n",
       " 19197,\n",
       " 19196,\n",
       " 19195,\n",
       " 19194,\n",
       " 19193,\n",
       " 19192,\n",
       " 19191,\n",
       " 19190,\n",
       " 19189,\n",
       " 19188,\n",
       " 19187,\n",
       " 19186,\n",
       " 19185,\n",
       " 19184,\n",
       " 19183,\n",
       " 19182,\n",
       " 19181,\n",
       " 19180,\n",
       " 19179,\n",
       " 19178,\n",
       " 19177,\n",
       " 19176,\n",
       " 19175,\n",
       " 19174,\n",
       " 19173,\n",
       " 19172,\n",
       " 19171,\n",
       " 19170,\n",
       " 19169,\n",
       " 19168,\n",
       " 19167,\n",
       " ...]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(i2v.keys(),reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[PAD]'"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v2i[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[PAD]'"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v2i.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v2i['[PAD]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "v2i = {v: i for i, v in enumerate(sorted(vocab))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0,\n",
       " 1: 1,\n",
       " 2: 2,\n",
       " 3: 3,\n",
       " 4: 4,\n",
       " 5: 5,\n",
       " 6: 6,\n",
       " 7: 7,\n",
       " 8: 8,\n",
       " 9: 9,\n",
       " 10: 10,\n",
       " 11: 11,\n",
       " 12: 12,\n",
       " 13: 13,\n",
       " 14: 14,\n",
       " 15: 15,\n",
       " 16: 16,\n",
       " 17: 17,\n",
       " 18: 18,\n",
       " 19: 19,\n",
       " 20: 20,\n",
       " 21: 21,\n",
       " 22: 22,\n",
       " 23: 23,\n",
       " 24: 24,\n",
       " 25: 25,\n",
       " 26: 26,\n",
       " 27: 27,\n",
       " 28: 28,\n",
       " 29: 29,\n",
       " 30: 30,\n",
       " 31: 31,\n",
       " 32: 32,\n",
       " 33: 33,\n",
       " 34: 34,\n",
       " 35: 35,\n",
       " 36: 36,\n",
       " 37: 37,\n",
       " 38: 38,\n",
       " 39: 39,\n",
       " 40: 40,\n",
       " 41: 41,\n",
       " 42: 42,\n",
       " 43: 43,\n",
       " 44: 44,\n",
       " 45: 45,\n",
       " 46: 46,\n",
       " 47: 47,\n",
       " 48: 48,\n",
       " 49: 49,\n",
       " 50: 50,\n",
       " 51: 51,\n",
       " 52: 52,\n",
       " 53: 53,\n",
       " 54: 54,\n",
       " 55: 55,\n",
       " 56: 56,\n",
       " 57: 57,\n",
       " 58: 58,\n",
       " 59: 59,\n",
       " 60: 60,\n",
       " 61: 61,\n",
       " 62: 62,\n",
       " 63: 63,\n",
       " 64: 64,\n",
       " 65: 65,\n",
       " 66: 66,\n",
       " 67: 67,\n",
       " 68: 68,\n",
       " 69: 69,\n",
       " 70: 70,\n",
       " 71: 71,\n",
       " 72: 72,\n",
       " 73: 73,\n",
       " 74: 74,\n",
       " 75: 75,\n",
       " 76: 76,\n",
       " 77: 77,\n",
       " 78: 78,\n",
       " 79: 79,\n",
       " 80: 80,\n",
       " 81: 81,\n",
       " 82: 82,\n",
       " 83: 83,\n",
       " 84: 84,\n",
       " 85: 85,\n",
       " 86: 86,\n",
       " 87: 87,\n",
       " 88: 88,\n",
       " 89: 89,\n",
       " 90: 90,\n",
       " 91: 91,\n",
       " 92: 92,\n",
       " 93: 93,\n",
       " 94: 94,\n",
       " 95: 95,\n",
       " 96: 96,\n",
       " 97: 97,\n",
       " 98: 98,\n",
       " 99: 99,\n",
       " 100: 100,\n",
       " 101: 101,\n",
       " 102: 102,\n",
       " 103: 103,\n",
       " 104: 104,\n",
       " 105: 105,\n",
       " 106: 106,\n",
       " 107: 107,\n",
       " 108: 108,\n",
       " 109: 109,\n",
       " 110: 110,\n",
       " 111: 111,\n",
       " 112: 112,\n",
       " 113: 113,\n",
       " 114: 114,\n",
       " 115: 115,\n",
       " 116: 116,\n",
       " 117: 117,\n",
       " 118: 118,\n",
       " 119: 119,\n",
       " 120: 120,\n",
       " 121: 121,\n",
       " 122: 122,\n",
       " 123: 123,\n",
       " 124: 124,\n",
       " 125: 125,\n",
       " 126: 126,\n",
       " 127: 127,\n",
       " 128: 128,\n",
       " 129: 129,\n",
       " 130: 130,\n",
       " 131: 131,\n",
       " 132: 132,\n",
       " 133: 133,\n",
       " 134: 134,\n",
       " 135: 135,\n",
       " 136: 136,\n",
       " 137: 137,\n",
       " 138: 138,\n",
       " 139: 139,\n",
       " 140: 140,\n",
       " 141: 141,\n",
       " 142: 142,\n",
       " 143: 143,\n",
       " 144: 144,\n",
       " 145: 145,\n",
       " 146: 146,\n",
       " 147: 147,\n",
       " 148: 148,\n",
       " 149: 149,\n",
       " 150: 150,\n",
       " 151: 151,\n",
       " 152: 152,\n",
       " 153: 153,\n",
       " 154: 154,\n",
       " 155: 155,\n",
       " 156: 156,\n",
       " 157: 157,\n",
       " 158: 158,\n",
       " 159: 159,\n",
       " 160: 160,\n",
       " 161: 161,\n",
       " 162: 162,\n",
       " 163: 163,\n",
       " 164: 164,\n",
       " 165: 165,\n",
       " 166: 166,\n",
       " 167: 167,\n",
       " 168: 168,\n",
       " 169: 169,\n",
       " 170: 170,\n",
       " 171: 171,\n",
       " 172: 172,\n",
       " 173: 173,\n",
       " 174: 174,\n",
       " 175: 175,\n",
       " 176: 176,\n",
       " 177: 177,\n",
       " 178: 178,\n",
       " 179: 179,\n",
       " 180: 180,\n",
       " 181: 181,\n",
       " 182: 182,\n",
       " 183: 183,\n",
       " 184: 184,\n",
       " 185: 185,\n",
       " 186: 186,\n",
       " 187: 187,\n",
       " 188: 188,\n",
       " 189: 189,\n",
       " 190: 190,\n",
       " 191: 191,\n",
       " 192: 192,\n",
       " 193: 193,\n",
       " 194: 194,\n",
       " 195: 195,\n",
       " 196: 196,\n",
       " 197: 197,\n",
       " 198: 198,\n",
       " 199: 199,\n",
       " 200: 200,\n",
       " 201: 201,\n",
       " 202: 202,\n",
       " 203: 203,\n",
       " 204: 204,\n",
       " 205: 205,\n",
       " 206: 206,\n",
       " 207: 207,\n",
       " 208: 208,\n",
       " 209: 209,\n",
       " 210: 210,\n",
       " 211: 211,\n",
       " 212: 212,\n",
       " 213: 213,\n",
       " 214: 214,\n",
       " 215: 215,\n",
       " 216: 216,\n",
       " 217: 217,\n",
       " 218: 218,\n",
       " 219: 219,\n",
       " 220: 220,\n",
       " 221: 221,\n",
       " 222: 222,\n",
       " 223: 223,\n",
       " 224: 224,\n",
       " 225: 225,\n",
       " 226: 226,\n",
       " 227: 227,\n",
       " 228: 228,\n",
       " 229: 229,\n",
       " 230: 230,\n",
       " 231: 231,\n",
       " 232: 232,\n",
       " 233: 233,\n",
       " 234: 234,\n",
       " 235: 235,\n",
       " 236: 236,\n",
       " 237: 237,\n",
       " 238: 238,\n",
       " 239: 239,\n",
       " 240: 240,\n",
       " 241: 241,\n",
       " 242: 242,\n",
       " 243: 243,\n",
       " 244: 244,\n",
       " 245: 245,\n",
       " 246: 246,\n",
       " 247: 247,\n",
       " 248: 248,\n",
       " 249: 249,\n",
       " 250: 250,\n",
       " 251: 251,\n",
       " 252: 252,\n",
       " 253: 253,\n",
       " 254: 254,\n",
       " 255: 255,\n",
       " 256: 256,\n",
       " 257: 257,\n",
       " 258: 258,\n",
       " 259: 259,\n",
       " 260: 260,\n",
       " 261: 261,\n",
       " 262: 262,\n",
       " 263: 263,\n",
       " 264: 264,\n",
       " 265: 265,\n",
       " 266: 266,\n",
       " 267: 267,\n",
       " 268: 268,\n",
       " 269: 269,\n",
       " 270: 270,\n",
       " 271: 271,\n",
       " 272: 272,\n",
       " 273: 273,\n",
       " 274: 274,\n",
       " 275: 275,\n",
       " 276: 276,\n",
       " 277: 277,\n",
       " 278: 278,\n",
       " 279: 279,\n",
       " 280: 280,\n",
       " 281: 281,\n",
       " 282: 282,\n",
       " 283: 283,\n",
       " 284: 284,\n",
       " 285: 285,\n",
       " 286: 286,\n",
       " 287: 287,\n",
       " 288: 288,\n",
       " 289: 289,\n",
       " 290: 290,\n",
       " 291: 291,\n",
       " 292: 292,\n",
       " 293: 293,\n",
       " 294: 294,\n",
       " 295: 295,\n",
       " 296: 296,\n",
       " 297: 297,\n",
       " 298: 298,\n",
       " 299: 299,\n",
       " 300: 300,\n",
       " 301: 301,\n",
       " 302: 302,\n",
       " 303: 303,\n",
       " 304: 304,\n",
       " 305: 305,\n",
       " 306: 306,\n",
       " 307: 307,\n",
       " 308: 308,\n",
       " 309: 309,\n",
       " 310: 310,\n",
       " 311: 311,\n",
       " 312: 312,\n",
       " 313: 313,\n",
       " 314: 314,\n",
       " 315: 315,\n",
       " 316: 316,\n",
       " 317: 317,\n",
       " 318: 318,\n",
       " 319: 319,\n",
       " 320: 320,\n",
       " 321: 321,\n",
       " 322: 322,\n",
       " 323: 323,\n",
       " 324: 324,\n",
       " 325: 325,\n",
       " 326: 326,\n",
       " 327: 327,\n",
       " 328: 328,\n",
       " 329: 329,\n",
       " 330: 330,\n",
       " 331: 331,\n",
       " 332: 332,\n",
       " 333: 333,\n",
       " 334: 334,\n",
       " 335: 335,\n",
       " 336: 336,\n",
       " 337: 337,\n",
       " 338: 338,\n",
       " 339: 339,\n",
       " 340: 340,\n",
       " 341: 341,\n",
       " 342: 342,\n",
       " 343: 343,\n",
       " 344: 344,\n",
       " 345: 345,\n",
       " 346: 346,\n",
       " 347: 347,\n",
       " 348: 348,\n",
       " 349: 349,\n",
       " 350: 350,\n",
       " 351: 351,\n",
       " 352: 352,\n",
       " 353: 353,\n",
       " 354: 354,\n",
       " 355: 355,\n",
       " 356: 356,\n",
       " 357: 357,\n",
       " 358: 358,\n",
       " 359: 359,\n",
       " 360: 360,\n",
       " 361: 361,\n",
       " 362: 362,\n",
       " 363: 363,\n",
       " 364: 364,\n",
       " 365: 365,\n",
       " 366: 366,\n",
       " 367: 367,\n",
       " 368: 368,\n",
       " 369: 369,\n",
       " 370: 370,\n",
       " 371: 371,\n",
       " 372: 372,\n",
       " 373: 373,\n",
       " 374: 374,\n",
       " 375: 375,\n",
       " 376: 376,\n",
       " 377: 377,\n",
       " 378: 378,\n",
       " 379: 379,\n",
       " 380: 380,\n",
       " 381: 381,\n",
       " 382: 382,\n",
       " 383: 383,\n",
       " 384: 384,\n",
       " 385: 385,\n",
       " 386: 386,\n",
       " 387: 387,\n",
       " 388: 388,\n",
       " 389: 389,\n",
       " 390: 390,\n",
       " 391: 391,\n",
       " 392: 392,\n",
       " 393: 393,\n",
       " 394: 394,\n",
       " 395: 395,\n",
       " 396: 396,\n",
       " 397: 397,\n",
       " 398: 398,\n",
       " 399: 399,\n",
       " 400: 400,\n",
       " 401: 401,\n",
       " 402: 402,\n",
       " 403: 403,\n",
       " 404: 404,\n",
       " 405: 405,\n",
       " 406: 406,\n",
       " 407: 407,\n",
       " 408: 408,\n",
       " 409: 409,\n",
       " 410: 410,\n",
       " 411: 411,\n",
       " 412: 412,\n",
       " 413: 413,\n",
       " 414: 414,\n",
       " 415: 415,\n",
       " 416: 416,\n",
       " 417: 417,\n",
       " 418: 418,\n",
       " 419: 419,\n",
       " 420: 420,\n",
       " 421: 421,\n",
       " 422: 422,\n",
       " 423: 423,\n",
       " 424: 424,\n",
       " 425: 425,\n",
       " 426: 426,\n",
       " 427: 427,\n",
       " 428: 428,\n",
       " 429: 429,\n",
       " 430: 430,\n",
       " 431: 431,\n",
       " 432: 432,\n",
       " 433: 433,\n",
       " 434: 434,\n",
       " 435: 435,\n",
       " 436: 436,\n",
       " 437: 437,\n",
       " 438: 438,\n",
       " 439: 439,\n",
       " 440: 440,\n",
       " 441: 441,\n",
       " 442: 442,\n",
       " 443: 443,\n",
       " 444: 444,\n",
       " 445: 445,\n",
       " 446: 446,\n",
       " 447: 447,\n",
       " 448: 448,\n",
       " 449: 449,\n",
       " 450: 450,\n",
       " 451: 451,\n",
       " 452: 452,\n",
       " 453: 453,\n",
       " 454: 454,\n",
       " 455: 455,\n",
       " 456: 456,\n",
       " 457: 457,\n",
       " 458: 458,\n",
       " 459: 459,\n",
       " 460: 460,\n",
       " 461: 461,\n",
       " 462: 462,\n",
       " 463: 463,\n",
       " 464: 464,\n",
       " 465: 465,\n",
       " 466: 466,\n",
       " 467: 467,\n",
       " 468: 468,\n",
       " 469: 469,\n",
       " 470: 470,\n",
       " 471: 471,\n",
       " 472: 472,\n",
       " 473: 473,\n",
       " 474: 474,\n",
       " 475: 475,\n",
       " 476: 476,\n",
       " 477: 477,\n",
       " 478: 478,\n",
       " 479: 479,\n",
       " 480: 480,\n",
       " 481: 481,\n",
       " 482: 482,\n",
       " 483: 483,\n",
       " 484: 484,\n",
       " 485: 485,\n",
       " 486: 486,\n",
       " 487: 487,\n",
       " 488: 488,\n",
       " 489: 489,\n",
       " 490: 490,\n",
       " 491: 491,\n",
       " 492: 492,\n",
       " 493: 493,\n",
       " 494: 494,\n",
       " 495: 495,\n",
       " 496: 496,\n",
       " 497: 497,\n",
       " 498: 498,\n",
       " 499: 499,\n",
       " 500: 500,\n",
       " 501: 501,\n",
       " 502: 502,\n",
       " 503: 503,\n",
       " 504: 504,\n",
       " 505: 505,\n",
       " 506: 506,\n",
       " 507: 507,\n",
       " 508: 508,\n",
       " 509: 509,\n",
       " 510: 510,\n",
       " 511: 511,\n",
       " 512: 512,\n",
       " 513: 513,\n",
       " 514: 514,\n",
       " 515: 515,\n",
       " 516: 516,\n",
       " 517: 517,\n",
       " 518: 518,\n",
       " 519: 519,\n",
       " 520: 520,\n",
       " 521: 521,\n",
       " 522: 522,\n",
       " 523: 523,\n",
       " 524: 524,\n",
       " 525: 525,\n",
       " 526: 526,\n",
       " 527: 527,\n",
       " 528: 528,\n",
       " 529: 529,\n",
       " 530: 530,\n",
       " 531: 531,\n",
       " 532: 532,\n",
       " 533: 533,\n",
       " 534: 534,\n",
       " 535: 535,\n",
       " 536: 536,\n",
       " 537: 537,\n",
       " 538: 538,\n",
       " 539: 539,\n",
       " 540: 540,\n",
       " 541: 541,\n",
       " 542: 542,\n",
       " 543: 543,\n",
       " 544: 544,\n",
       " 545: 545,\n",
       " 546: 546,\n",
       " 547: 547,\n",
       " 548: 548,\n",
       " 549: 549,\n",
       " 550: 550,\n",
       " 551: 551,\n",
       " 552: 552,\n",
       " 553: 553,\n",
       " 554: 554,\n",
       " 555: 555,\n",
       " 556: 556,\n",
       " 557: 557,\n",
       " 558: 558,\n",
       " 559: 559,\n",
       " 560: 560,\n",
       " 561: 561,\n",
       " 562: 562,\n",
       " 563: 563,\n",
       " 564: 564,\n",
       " 565: 565,\n",
       " 566: 566,\n",
       " 567: 567,\n",
       " 568: 568,\n",
       " 569: 569,\n",
       " 570: 570,\n",
       " 571: 571,\n",
       " 572: 572,\n",
       " 573: 573,\n",
       " 574: 574,\n",
       " 575: 575,\n",
       " 576: 576,\n",
       " 577: 577,\n",
       " 578: 578,\n",
       " 579: 579,\n",
       " 580: 580,\n",
       " 581: 581,\n",
       " 582: 582,\n",
       " 583: 583,\n",
       " 584: 584,\n",
       " 585: 585,\n",
       " 586: 586,\n",
       " 587: 587,\n",
       " 588: 588,\n",
       " 589: 589,\n",
       " 590: 590,\n",
       " 591: 591,\n",
       " 592: 592,\n",
       " 593: 593,\n",
       " 594: 594,\n",
       " 595: 595,\n",
       " 596: 596,\n",
       " 597: 597,\n",
       " 598: 598,\n",
       " 599: 599,\n",
       " 600: 600,\n",
       " 601: 601,\n",
       " 602: 602,\n",
       " 603: 603,\n",
       " 604: 604,\n",
       " 605: 605,\n",
       " 606: 606,\n",
       " 607: 607,\n",
       " 608: 608,\n",
       " 609: 609,\n",
       " 610: 610,\n",
       " 611: 611,\n",
       " 612: 612,\n",
       " 613: 613,\n",
       " 614: 614,\n",
       " 615: 615,\n",
       " 616: 616,\n",
       " 617: 617,\n",
       " 618: 618,\n",
       " 619: 619,\n",
       " 620: 620,\n",
       " 621: 621,\n",
       " 622: 622,\n",
       " 623: 623,\n",
       " 624: 624,\n",
       " 625: 625,\n",
       " 626: 626,\n",
       " 627: 627,\n",
       " 628: 628,\n",
       " 629: 629,\n",
       " 630: 630,\n",
       " 631: 631,\n",
       " 632: 632,\n",
       " 633: 633,\n",
       " 634: 634,\n",
       " 635: 635,\n",
       " 636: 636,\n",
       " 637: 637,\n",
       " 638: 638,\n",
       " 639: 639,\n",
       " 640: 640,\n",
       " 641: 641,\n",
       " 642: 642,\n",
       " 643: 643,\n",
       " 644: 644,\n",
       " 645: 645,\n",
       " 646: 646,\n",
       " 647: 647,\n",
       " 648: 648,\n",
       " 649: 649,\n",
       " 650: 650,\n",
       " 651: 651,\n",
       " 652: 652,\n",
       " 653: 653,\n",
       " 654: 654,\n",
       " 655: 655,\n",
       " 656: 656,\n",
       " 657: 657,\n",
       " 658: 658,\n",
       " 659: 659,\n",
       " 660: 660,\n",
       " 661: 661,\n",
       " 662: 662,\n",
       " 663: 663,\n",
       " 664: 664,\n",
       " 665: 665,\n",
       " 666: 666,\n",
       " 667: 667,\n",
       " 668: 668,\n",
       " 669: 669,\n",
       " 670: 670,\n",
       " 671: 671,\n",
       " 672: 672,\n",
       " 673: 673,\n",
       " 674: 674,\n",
       " 675: 675,\n",
       " 676: 676,\n",
       " 677: 677,\n",
       " 678: 678,\n",
       " 679: 679,\n",
       " 680: 680,\n",
       " 681: 681,\n",
       " 682: 682,\n",
       " 683: 683,\n",
       " 684: 684,\n",
       " 685: 685,\n",
       " 686: 686,\n",
       " 687: 687,\n",
       " 688: 688,\n",
       " 689: 689,\n",
       " 690: 690,\n",
       " 691: 691,\n",
       " 692: 692,\n",
       " 693: 693,\n",
       " 694: 694,\n",
       " 695: 695,\n",
       " 696: 696,\n",
       " 697: 697,\n",
       " 698: 698,\n",
       " 699: 699,\n",
       " 700: 700,\n",
       " 701: 701,\n",
       " 702: 702,\n",
       " 703: 703,\n",
       " 704: 704,\n",
       " 705: 705,\n",
       " 706: 706,\n",
       " 707: 707,\n",
       " 708: 708,\n",
       " 709: 709,\n",
       " 710: 710,\n",
       " 711: 711,\n",
       " 712: 712,\n",
       " 713: 713,\n",
       " 714: 714,\n",
       " 715: 715,\n",
       " 716: 716,\n",
       " 717: 717,\n",
       " 718: 718,\n",
       " 719: 719,\n",
       " 720: 720,\n",
       " 721: 721,\n",
       " 722: 722,\n",
       " 723: 723,\n",
       " 724: 724,\n",
       " 725: 725,\n",
       " 726: 726,\n",
       " 727: 727,\n",
       " 728: 728,\n",
       " 729: 729,\n",
       " 730: 730,\n",
       " 731: 731,\n",
       " 732: 732,\n",
       " 733: 733,\n",
       " 734: 734,\n",
       " 735: 735,\n",
       " 736: 736,\n",
       " 737: 737,\n",
       " 738: 738,\n",
       " 739: 739,\n",
       " 740: 740,\n",
       " 741: 741,\n",
       " 742: 742,\n",
       " 743: 743,\n",
       " 744: 744,\n",
       " 745: 745,\n",
       " 746: 746,\n",
       " 747: 747,\n",
       " 748: 748,\n",
       " 749: 749,\n",
       " 750: 750,\n",
       " 751: 751,\n",
       " 752: 752,\n",
       " 753: 753,\n",
       " 754: 754,\n",
       " 755: 755,\n",
       " 756: 756,\n",
       " 757: 757,\n",
       " 758: 758,\n",
       " 759: 759,\n",
       " 760: 760,\n",
       " 761: 761,\n",
       " 762: 762,\n",
       " 763: 763,\n",
       " 764: 764,\n",
       " 765: 765,\n",
       " 766: 766,\n",
       " 767: 767,\n",
       " 768: 768,\n",
       " 769: 769,\n",
       " 770: 770,\n",
       " 771: 771,\n",
       " 772: 772,\n",
       " 773: 773,\n",
       " 774: 774,\n",
       " 775: 775,\n",
       " 776: 776,\n",
       " 777: 777,\n",
       " 778: 778,\n",
       " 779: 779,\n",
       " 780: 780,\n",
       " 781: 781,\n",
       " 782: 782,\n",
       " 783: 783,\n",
       " 784: 784,\n",
       " 785: 785,\n",
       " 786: 786,\n",
       " 787: 787,\n",
       " 788: 788,\n",
       " 789: 789,\n",
       " 790: 790,\n",
       " 791: 791,\n",
       " 792: 792,\n",
       " 793: 793,\n",
       " 794: 794,\n",
       " 795: 795,\n",
       " 796: 796,\n",
       " 797: 797,\n",
       " 798: 798,\n",
       " 799: 799,\n",
       " 800: 800,\n",
       " 801: 801,\n",
       " 802: 802,\n",
       " 803: 803,\n",
       " 804: 804,\n",
       " 805: 805,\n",
       " 806: 806,\n",
       " 807: 807,\n",
       " 808: 808,\n",
       " 809: 809,\n",
       " 810: 810,\n",
       " 811: 811,\n",
       " 812: 812,\n",
       " 813: 813,\n",
       " 814: 814,\n",
       " 815: 815,\n",
       " 816: 816,\n",
       " 817: 817,\n",
       " 818: 818,\n",
       " 819: 819,\n",
       " 820: 820,\n",
       " 821: 821,\n",
       " 822: 822,\n",
       " 823: 823,\n",
       " 824: 824,\n",
       " 825: 825,\n",
       " 826: 826,\n",
       " 827: 827,\n",
       " 828: 828,\n",
       " 829: 829,\n",
       " 830: 830,\n",
       " 831: 831,\n",
       " 832: 832,\n",
       " 833: 833,\n",
       " 834: 834,\n",
       " 835: 835,\n",
       " 836: 836,\n",
       " 837: 837,\n",
       " 838: 838,\n",
       " 839: 839,\n",
       " 840: 840,\n",
       " 841: 841,\n",
       " 842: 842,\n",
       " 843: 843,\n",
       " 844: 844,\n",
       " 845: 845,\n",
       " 846: 846,\n",
       " 847: 847,\n",
       " 848: 848,\n",
       " 849: 849,\n",
       " 850: 850,\n",
       " 851: 851,\n",
       " 852: 852,\n",
       " 853: 853,\n",
       " 854: 854,\n",
       " 855: 855,\n",
       " 856: 856,\n",
       " 857: 857,\n",
       " 858: 858,\n",
       " 859: 859,\n",
       " 860: 860,\n",
       " 861: 861,\n",
       " 862: 862,\n",
       " 863: 863,\n",
       " 864: 864,\n",
       " 865: 865,\n",
       " 866: 866,\n",
       " 867: 867,\n",
       " 868: 868,\n",
       " 869: 869,\n",
       " 870: 870,\n",
       " 871: 871,\n",
       " 872: 872,\n",
       " 873: 873,\n",
       " 874: 874,\n",
       " 875: 875,\n",
       " 876: 876,\n",
       " 877: 877,\n",
       " 878: 878,\n",
       " 879: 879,\n",
       " 880: 880,\n",
       " 881: 881,\n",
       " 882: 882,\n",
       " 883: 883,\n",
       " 884: 884,\n",
       " 885: 885,\n",
       " 886: 886,\n",
       " 887: 887,\n",
       " 888: 888,\n",
       " 889: 889,\n",
       " 890: 890,\n",
       " 891: 891,\n",
       " 892: 892,\n",
       " 893: 893,\n",
       " 894: 894,\n",
       " 895: 895,\n",
       " 896: 896,\n",
       " 897: 897,\n",
       " 898: 898,\n",
       " 899: 899,\n",
       " 900: 900,\n",
       " 901: 901,\n",
       " 902: 902,\n",
       " 903: 903,\n",
       " 904: 904,\n",
       " 905: 905,\n",
       " 906: 906,\n",
       " 907: 907,\n",
       " 908: 908,\n",
       " 909: 909,\n",
       " 910: 910,\n",
       " 911: 911,\n",
       " 912: 912,\n",
       " 913: 913,\n",
       " 914: 914,\n",
       " 915: 915,\n",
       " 916: 916,\n",
       " 917: 917,\n",
       " 918: 918,\n",
       " 919: 919,\n",
       " 920: 920,\n",
       " 921: 921,\n",
       " 922: 922,\n",
       " 923: 923,\n",
       " 924: 924,\n",
       " 925: 925,\n",
       " 926: 926,\n",
       " 927: 927,\n",
       " 928: 928,\n",
       " 929: 929,\n",
       " 930: 930,\n",
       " 931: 931,\n",
       " 932: 932,\n",
       " 933: 933,\n",
       " 934: 934,\n",
       " 935: 935,\n",
       " 936: 936,\n",
       " 937: 937,\n",
       " 938: 938,\n",
       " 939: 939,\n",
       " 940: 940,\n",
       " 941: 941,\n",
       " 942: 942,\n",
       " 943: 943,\n",
       " 944: 944,\n",
       " 945: 945,\n",
       " 946: 946,\n",
       " 947: 947,\n",
       " 948: 948,\n",
       " 949: 949,\n",
       " 950: 950,\n",
       " 951: 951,\n",
       " 952: 952,\n",
       " 953: 953,\n",
       " 954: 954,\n",
       " 955: 955,\n",
       " 956: 956,\n",
       " 957: 957,\n",
       " 958: 958,\n",
       " 959: 959,\n",
       " 960: 960,\n",
       " 961: 961,\n",
       " 962: 962,\n",
       " 963: 963,\n",
       " 964: 964,\n",
       " 965: 965,\n",
       " 966: 966,\n",
       " 967: 967,\n",
       " 968: 968,\n",
       " 969: 969,\n",
       " 970: 970,\n",
       " 971: 971,\n",
       " 972: 972,\n",
       " 973: 973,\n",
       " 974: 974,\n",
       " 975: 975,\n",
       " 976: 976,\n",
       " 977: 977,\n",
       " 978: 978,\n",
       " 979: 979,\n",
       " 980: 980,\n",
       " 981: 981,\n",
       " 982: 982,\n",
       " 983: 983,\n",
       " 984: 984,\n",
       " 985: 985,\n",
       " 986: 986,\n",
       " 987: 987,\n",
       " 988: 988,\n",
       " 989: 989,\n",
       " 990: 990,\n",
       " 991: 991,\n",
       " 992: 992,\n",
       " 993: 993,\n",
       " 994: 994,\n",
       " 995: 995,\n",
       " 996: 996,\n",
       " 997: 997,\n",
       " 998: 998,\n",
       " 999: 999,\n",
       " ...}"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v2i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1607317"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_list = sorted(list(vocab), reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[21124,\n",
       " 21122,\n",
       " 21120,\n",
       " 21118,\n",
       " 21115,\n",
       " 21113,\n",
       " 21109,\n",
       " 21108,\n",
       " 21106,\n",
       " 21105,\n",
       " 21104,\n",
       " 21103,\n",
       " 21101,\n",
       " 21099,\n",
       " 21098,\n",
       " 21094,\n",
       " 21093,\n",
       " 21092,\n",
       " 21091,\n",
       " 21090,\n",
       " 21085,\n",
       " 21084,\n",
       " 21083,\n",
       " 21081,\n",
       " 21080,\n",
       " 21078,\n",
       " 21077,\n",
       " 21076,\n",
       " 21075,\n",
       " 21073,\n",
       " 21072,\n",
       " 21070,\n",
       " 21069,\n",
       " 21068,\n",
       " 21067,\n",
       " 21065,\n",
       " 21062,\n",
       " 21059,\n",
       " 21055,\n",
       " 21051,\n",
       " 21050,\n",
       " 21048,\n",
       " 21039,\n",
       " 21034,\n",
       " 21032,\n",
       " 21023,\n",
       " 21020,\n",
       " 21019,\n",
       " 21017,\n",
       " 21014,\n",
       " 21012,\n",
       " 21009,\n",
       " 21006,\n",
       " 21005,\n",
       " 21003,\n",
       " 21002,\n",
       " 21001,\n",
       " 20997,\n",
       " 20996,\n",
       " 20995,\n",
       " 20992,\n",
       " 20991,\n",
       " 20984,\n",
       " 20981,\n",
       " 20979,\n",
       " 20978,\n",
       " 20977,\n",
       " 20976,\n",
       " 20975,\n",
       " 20974,\n",
       " 20973,\n",
       " 20972,\n",
       " 20971,\n",
       " 20961,\n",
       " 20960,\n",
       " 20959,\n",
       " 20958,\n",
       " 20957,\n",
       " 20956,\n",
       " 20955,\n",
       " 20954,\n",
       " 20953,\n",
       " 20949,\n",
       " 20948,\n",
       " 20947,\n",
       " 20946,\n",
       " 20943,\n",
       " 20942,\n",
       " 20939,\n",
       " 20936,\n",
       " 20935,\n",
       " 20933,\n",
       " 20931,\n",
       " 20930,\n",
       " 20929,\n",
       " 20928,\n",
       " 20927,\n",
       " 20924,\n",
       " 20923,\n",
       " 20921,\n",
       " 20919,\n",
       " 20918,\n",
       " 20916,\n",
       " 20915,\n",
       " 20913,\n",
       " 20911,\n",
       " 20909,\n",
       " 20908,\n",
       " 20907,\n",
       " 20905,\n",
       " 20904,\n",
       " 20903,\n",
       " 20902,\n",
       " 20900,\n",
       " 20895,\n",
       " 20893,\n",
       " 20890,\n",
       " 20887,\n",
       " 20885,\n",
       " 20880,\n",
       " 20879,\n",
       " 20877,\n",
       " 20875,\n",
       " 20872,\n",
       " 20871,\n",
       " 20870,\n",
       " 20866,\n",
       " 20865,\n",
       " 20863,\n",
       " 20862,\n",
       " 20860,\n",
       " 20859,\n",
       " 20858,\n",
       " 20853,\n",
       " 20851,\n",
       " 20848,\n",
       " 20847,\n",
       " 20844,\n",
       " 20843,\n",
       " 20842,\n",
       " 20841,\n",
       " 20836,\n",
       " 20835,\n",
       " 20834,\n",
       " 20832,\n",
       " 20830,\n",
       " 20828,\n",
       " 20826,\n",
       " 20823,\n",
       " 20822,\n",
       " 20821,\n",
       " 20820,\n",
       " 20819,\n",
       " 20816,\n",
       " 20814,\n",
       " 20813,\n",
       " 20812,\n",
       " 20811,\n",
       " 20809,\n",
       " 20807,\n",
       " 20806,\n",
       " 20805,\n",
       " 20802,\n",
       " 20801,\n",
       " 20799,\n",
       " 20795,\n",
       " 20787,\n",
       " 20785,\n",
       " 20784,\n",
       " 20783,\n",
       " 20782,\n",
       " 20781,\n",
       " 20779,\n",
       " 20778,\n",
       " 20777,\n",
       " 20776,\n",
       " 20774,\n",
       " 20773,\n",
       " 20771,\n",
       " 20770,\n",
       " 20768,\n",
       " 20767,\n",
       " 20763,\n",
       " 20762,\n",
       " 20761,\n",
       " 20760,\n",
       " 20758,\n",
       " 20756,\n",
       " 20755,\n",
       " 20754,\n",
       " 20751,\n",
       " 20750,\n",
       " 20748,\n",
       " 20747,\n",
       " 20745,\n",
       " 20743,\n",
       " 20737,\n",
       " 20736,\n",
       " 20735,\n",
       " 20732,\n",
       " 20730,\n",
       " 20729,\n",
       " 20727,\n",
       " 20724,\n",
       " 20723,\n",
       " 20722,\n",
       " 20720,\n",
       " 20719,\n",
       " 20717,\n",
       " 20716,\n",
       " 20715,\n",
       " 20712,\n",
       " 20711,\n",
       " 20707,\n",
       " 20706,\n",
       " 20699,\n",
       " 20696,\n",
       " 20695,\n",
       " 20694,\n",
       " 20693,\n",
       " 20692,\n",
       " 20691,\n",
       " 20690,\n",
       " 20689,\n",
       " 20688,\n",
       " 20685,\n",
       " 20683,\n",
       " 20682,\n",
       " 20681,\n",
       " 20680,\n",
       " 20679,\n",
       " 20677,\n",
       " 20676,\n",
       " 20674,\n",
       " 20673,\n",
       " 20667,\n",
       " 20665,\n",
       " 20664,\n",
       " 20663,\n",
       " 20661,\n",
       " 20657,\n",
       " 20656,\n",
       " 20654,\n",
       " 20653,\n",
       " 20650,\n",
       " 20649,\n",
       " 20648,\n",
       " 20647,\n",
       " 20646,\n",
       " 20644,\n",
       " 20643,\n",
       " 20642,\n",
       " 20641,\n",
       " 20640,\n",
       " 20639,\n",
       " 20638,\n",
       " 20636,\n",
       " 20633,\n",
       " 20631,\n",
       " 20629,\n",
       " 20627,\n",
       " 20626,\n",
       " 20625,\n",
       " 20624,\n",
       " 20623,\n",
       " 20622,\n",
       " 20621,\n",
       " 20620,\n",
       " 20618,\n",
       " 20616,\n",
       " 20615,\n",
       " 20612,\n",
       " 20611,\n",
       " 20610,\n",
       " 20609,\n",
       " 20607,\n",
       " 20606,\n",
       " 20604,\n",
       " 20603,\n",
       " 20601,\n",
       " 20599,\n",
       " 20597,\n",
       " 20596,\n",
       " 20595,\n",
       " 20593,\n",
       " 20592,\n",
       " 20591,\n",
       " 20589,\n",
       " 20588,\n",
       " 20587,\n",
       " 20584,\n",
       " 20581,\n",
       " 20578,\n",
       " 20576,\n",
       " 20573,\n",
       " 20572,\n",
       " 20571,\n",
       " 20568,\n",
       " 20567,\n",
       " 20564,\n",
       " 20563,\n",
       " 20559,\n",
       " 20552,\n",
       " 20551,\n",
       " 20549,\n",
       " 20547,\n",
       " 20546,\n",
       " 20545,\n",
       " 20542,\n",
       " 20541,\n",
       " 20537,\n",
       " 20536,\n",
       " 20535,\n",
       " 20534,\n",
       " 20533,\n",
       " 20532,\n",
       " 20530,\n",
       " 20527,\n",
       " 20526,\n",
       " 20525,\n",
       " 20523,\n",
       " 20519,\n",
       " 20516,\n",
       " 20513,\n",
       " 20512,\n",
       " 20510,\n",
       " 20508,\n",
       " 20504,\n",
       " 20501,\n",
       " 20500,\n",
       " 20499,\n",
       " 20498,\n",
       " 20496,\n",
       " 20495,\n",
       " 20494,\n",
       " 20493,\n",
       " 20487,\n",
       " 20484,\n",
       " 20482,\n",
       " 20481,\n",
       " 20480,\n",
       " 20478,\n",
       " 20477,\n",
       " 20476,\n",
       " 20475,\n",
       " 20474,\n",
       " 20473,\n",
       " 20472,\n",
       " 20471,\n",
       " 20469,\n",
       " 20468,\n",
       " 20467,\n",
       " 20461,\n",
       " 20458,\n",
       " 20455,\n",
       " 20454,\n",
       " 20451,\n",
       " 20449,\n",
       " 20447,\n",
       " 20446,\n",
       " 20444,\n",
       " 20442,\n",
       " 20440,\n",
       " 20431,\n",
       " 20429,\n",
       " 20428,\n",
       " 20427,\n",
       " 20426,\n",
       " 20424,\n",
       " 20421,\n",
       " 20420,\n",
       " 20419,\n",
       " 20417,\n",
       " 20416,\n",
       " 20415,\n",
       " 20414,\n",
       " 20413,\n",
       " 20410,\n",
       " 20409,\n",
       " 20408,\n",
       " 20407,\n",
       " 20405,\n",
       " 20397,\n",
       " 20395,\n",
       " 20393,\n",
       " 20392,\n",
       " 20391,\n",
       " 20389,\n",
       " 20388,\n",
       " 20387,\n",
       " 20386,\n",
       " 20385,\n",
       " 20384,\n",
       " 20382,\n",
       " 20379,\n",
       " 20378,\n",
       " 20377,\n",
       " 20375,\n",
       " 20373,\n",
       " 20372,\n",
       " 20370,\n",
       " 20369,\n",
       " 20368,\n",
       " 20367,\n",
       " 20364,\n",
       " 20362,\n",
       " 20359,\n",
       " 20358,\n",
       " 20354,\n",
       " 20353,\n",
       " 20351,\n",
       " 20350,\n",
       " 20348,\n",
       " 20347,\n",
       " 20346,\n",
       " 20345,\n",
       " 20344,\n",
       " 20343,\n",
       " 20341,\n",
       " 20340,\n",
       " 20338,\n",
       " 20337,\n",
       " 20331,\n",
       " 20328,\n",
       " 20327,\n",
       " 20324,\n",
       " 20323,\n",
       " 20322,\n",
       " 20319,\n",
       " 20317,\n",
       " 20315,\n",
       " 20314,\n",
       " 20312,\n",
       " 20311,\n",
       " 20309,\n",
       " 20306,\n",
       " 20305,\n",
       " 20304,\n",
       " 20303,\n",
       " 20300,\n",
       " 20299,\n",
       " 20298,\n",
       " 20296,\n",
       " 20295,\n",
       " 20292,\n",
       " 20291,\n",
       " 20289,\n",
       " 20288,\n",
       " 20287,\n",
       " 20286,\n",
       " 20285,\n",
       " 20284,\n",
       " 20281,\n",
       " 20280,\n",
       " 20279,\n",
       " 20276,\n",
       " 20274,\n",
       " 20273,\n",
       " 20271,\n",
       " 20270,\n",
       " 20267,\n",
       " 20266,\n",
       " 20265,\n",
       " 20263,\n",
       " 20261,\n",
       " 20258,\n",
       " 20257,\n",
       " 20256,\n",
       " 20255,\n",
       " 20254,\n",
       " 20253,\n",
       " 20251,\n",
       " 20250,\n",
       " 20249,\n",
       " 20248,\n",
       " 20247,\n",
       " 20246,\n",
       " 20244,\n",
       " 20242,\n",
       " 20241,\n",
       " 20240,\n",
       " 20239,\n",
       " 20234,\n",
       " 20233,\n",
       " 20231,\n",
       " 20230,\n",
       " 20228,\n",
       " 20224,\n",
       " 20223,\n",
       " 20221,\n",
       " 20220,\n",
       " 20219,\n",
       " 20216,\n",
       " 20215,\n",
       " 20214,\n",
       " 20210,\n",
       " 20208,\n",
       " 20207,\n",
       " 20206,\n",
       " 20201,\n",
       " 20200,\n",
       " 20199,\n",
       " 20197,\n",
       " 20194,\n",
       " 20191,\n",
       " 20189,\n",
       " 20187,\n",
       " 20186,\n",
       " 20185,\n",
       " 20183,\n",
       " 20180,\n",
       " 20179,\n",
       " 20178,\n",
       " 20174,\n",
       " 20172,\n",
       " 20169,\n",
       " 20168,\n",
       " 20164,\n",
       " 20163,\n",
       " 20162,\n",
       " 20159,\n",
       " 20157,\n",
       " 20155,\n",
       " 20154,\n",
       " 20152,\n",
       " 20151,\n",
       " 20149,\n",
       " 20144,\n",
       " 20140,\n",
       " 20139,\n",
       " 20134,\n",
       " 20132,\n",
       " 20131,\n",
       " 20129,\n",
       " 20128,\n",
       " 20125,\n",
       " 20124,\n",
       " 20122,\n",
       " 20121,\n",
       " 20118,\n",
       " 20117,\n",
       " 20115,\n",
       " 20114,\n",
       " 20110,\n",
       " 20108,\n",
       " 20107,\n",
       " 20106,\n",
       " 20105,\n",
       " 20104,\n",
       " 20103,\n",
       " 20102,\n",
       " 20100,\n",
       " 20099,\n",
       " 20098,\n",
       " 20097,\n",
       " 20094,\n",
       " 20093,\n",
       " 20092,\n",
       " 20090,\n",
       " 20084,\n",
       " 20081,\n",
       " 20080,\n",
       " 20078,\n",
       " 20077,\n",
       " 20076,\n",
       " 20073,\n",
       " 20069,\n",
       " 20066,\n",
       " 20064,\n",
       " 20062,\n",
       " 20061,\n",
       " 20060,\n",
       " 20059,\n",
       " 20058,\n",
       " 20057,\n",
       " 20053,\n",
       " 20048,\n",
       " 20046,\n",
       " 20043,\n",
       " 20042,\n",
       " 20041,\n",
       " 20038,\n",
       " 20037,\n",
       " 20034,\n",
       " 20033,\n",
       " 20032,\n",
       " 20030,\n",
       " 20027,\n",
       " 20025,\n",
       " 20020,\n",
       " 20019,\n",
       " 20017,\n",
       " 20016,\n",
       " 20015,\n",
       " 20014,\n",
       " 20012,\n",
       " 20010,\n",
       " 20006,\n",
       " 20001,\n",
       " 20000,\n",
       " 19997,\n",
       " 19994,\n",
       " 19993,\n",
       " 19991,\n",
       " 19989,\n",
       " 19985,\n",
       " 19984,\n",
       " 19980,\n",
       " 19974,\n",
       " 19973,\n",
       " 19972,\n",
       " 19971,\n",
       " 19969,\n",
       " 19968,\n",
       " 19966,\n",
       " 19964,\n",
       " 19962,\n",
       " 19961,\n",
       " 19960,\n",
       " 19959,\n",
       " 19955,\n",
       " 19954,\n",
       " 19953,\n",
       " 19952,\n",
       " 19951,\n",
       " 19950,\n",
       " 19949,\n",
       " 19948,\n",
       " 19947,\n",
       " 19945,\n",
       " 19944,\n",
       " 19939,\n",
       " 19937,\n",
       " 19935,\n",
       " 19929,\n",
       " 19924,\n",
       " 19923,\n",
       " 19922,\n",
       " 19921,\n",
       " 19920,\n",
       " 19919,\n",
       " 19918,\n",
       " 19917,\n",
       " 19916,\n",
       " 19914,\n",
       " 19913,\n",
       " 19912,\n",
       " 19911,\n",
       " 19910,\n",
       " 19908,\n",
       " 19906,\n",
       " 19903,\n",
       " 19902,\n",
       " 19897,\n",
       " 19894,\n",
       " 19893,\n",
       " 19892,\n",
       " 19891,\n",
       " 19889,\n",
       " 19885,\n",
       " 19884,\n",
       " 19883,\n",
       " 19881,\n",
       " 19880,\n",
       " 19879,\n",
       " 19877,\n",
       " 19875,\n",
       " 19874,\n",
       " 19869,\n",
       " 19868,\n",
       " 19867,\n",
       " 19866,\n",
       " 19865,\n",
       " 19862,\n",
       " 19860,\n",
       " 19858,\n",
       " 19857,\n",
       " 19853,\n",
       " 19852,\n",
       " 19850,\n",
       " 19849,\n",
       " 19848,\n",
       " 19847,\n",
       " 19846,\n",
       " 19845,\n",
       " 19844,\n",
       " 19843,\n",
       " 19842,\n",
       " 19841,\n",
       " 19838,\n",
       " 19835,\n",
       " 19834,\n",
       " 19831,\n",
       " 19823,\n",
       " 19819,\n",
       " 19818,\n",
       " 19817,\n",
       " 19816,\n",
       " 19815,\n",
       " 19814,\n",
       " 19813,\n",
       " 19812,\n",
       " 19810,\n",
       " 19809,\n",
       " 19808,\n",
       " 19807,\n",
       " 19806,\n",
       " 19803,\n",
       " 19802,\n",
       " 19800,\n",
       " 19798,\n",
       " 19795,\n",
       " 19793,\n",
       " 19789,\n",
       " 19785,\n",
       " 19784,\n",
       " 19783,\n",
       " 19782,\n",
       " 19779,\n",
       " 19774,\n",
       " 19773,\n",
       " 19772,\n",
       " 19771,\n",
       " 19770,\n",
       " 19768,\n",
       " 19767,\n",
       " 19766,\n",
       " 19765,\n",
       " 19764,\n",
       " 19763,\n",
       " 19762,\n",
       " 19759,\n",
       " 19758,\n",
       " 19757,\n",
       " 19754,\n",
       " 19750,\n",
       " 19749,\n",
       " 19748,\n",
       " 19746,\n",
       " 19745,\n",
       " 19742,\n",
       " 19741,\n",
       " 19740,\n",
       " 19739,\n",
       " 19738,\n",
       " 19736,\n",
       " 19734,\n",
       " 19732,\n",
       " 19730,\n",
       " 19729,\n",
       " 19728,\n",
       " 19727,\n",
       " 19725,\n",
       " 19724,\n",
       " 19723,\n",
       " 19719,\n",
       " 19716,\n",
       " 19715,\n",
       " 19712,\n",
       " 19711,\n",
       " 19710,\n",
       " 19709,\n",
       " 19708,\n",
       " 19707,\n",
       " 19706,\n",
       " 19701,\n",
       " 19699,\n",
       " 19694,\n",
       " 19693,\n",
       " 19690,\n",
       " 19685,\n",
       " 19684,\n",
       " 19683,\n",
       " 19682,\n",
       " 19679,\n",
       " 19676,\n",
       " 19675,\n",
       " 19674,\n",
       " 19673,\n",
       " 19670,\n",
       " 19668,\n",
       " 19665,\n",
       " 19664,\n",
       " 19662,\n",
       " 19659,\n",
       " 19658,\n",
       " 19656,\n",
       " 19655,\n",
       " 19654,\n",
       " 19653,\n",
       " 19652,\n",
       " 19650,\n",
       " 19648,\n",
       " 19647,\n",
       " 19645,\n",
       " 19643,\n",
       " 19640,\n",
       " 19639,\n",
       " 19638,\n",
       " 19636,\n",
       " 19635,\n",
       " 19634,\n",
       " 19633,\n",
       " 19631,\n",
       " 19630,\n",
       " 19629,\n",
       " 19628,\n",
       " 19626,\n",
       " 19625,\n",
       " 19624,\n",
       " 19623,\n",
       " 19619,\n",
       " 19618,\n",
       " 19617,\n",
       " 19615,\n",
       " 19613,\n",
       " 19611,\n",
       " 19603,\n",
       " 19602,\n",
       " 19600,\n",
       " 19599,\n",
       " 19595,\n",
       " 19594,\n",
       " 19592,\n",
       " 19591,\n",
       " 19589,\n",
       " 19587,\n",
       " 19586,\n",
       " 19585,\n",
       " 19584,\n",
       " 19580,\n",
       " 19579,\n",
       " 19578,\n",
       " 19577,\n",
       " 19575,\n",
       " 19573,\n",
       " 19571,\n",
       " 19569,\n",
       " 19568,\n",
       " 19567,\n",
       " 19566,\n",
       " 19565,\n",
       " 19562,\n",
       " 19560,\n",
       " 19551,\n",
       " 19549,\n",
       " 19547,\n",
       " 19544,\n",
       " 19543,\n",
       " 19542,\n",
       " 19540,\n",
       " 19538,\n",
       " 19537,\n",
       " 19535,\n",
       " 19533,\n",
       " 19532,\n",
       " 19530,\n",
       " 19529,\n",
       " 19528,\n",
       " 19527,\n",
       " 19523,\n",
       " 19522,\n",
       " 19521,\n",
       " 19520,\n",
       " 19518,\n",
       " 19517,\n",
       " 19514,\n",
       " 19513,\n",
       " 19512,\n",
       " 19511,\n",
       " 19510,\n",
       " 19509,\n",
       " 19508,\n",
       " 19507,\n",
       " 19506,\n",
       " 19505,\n",
       " 19504,\n",
       " 19502,\n",
       " 19501,\n",
       " 19500,\n",
       " 19499,\n",
       " 19498,\n",
       " 19494,\n",
       " 19493,\n",
       " 19487,\n",
       " 19485,\n",
       " 19484,\n",
       " 19483,\n",
       " 19482,\n",
       " 19481,\n",
       " 19480,\n",
       " 19479,\n",
       " 19478,\n",
       " 19475,\n",
       " 19472,\n",
       " 19471,\n",
       " 19470,\n",
       " 19468,\n",
       " 19467,\n",
       " 19464,\n",
       " 19463,\n",
       " 19461,\n",
       " 19460,\n",
       " 19459,\n",
       " 19456,\n",
       " 19455,\n",
       " 19450,\n",
       " 19449,\n",
       " 19447,\n",
       " 19445,\n",
       " 19442,\n",
       " 19441,\n",
       " 19439,\n",
       " 19435,\n",
       " 19434,\n",
       " 19433,\n",
       " 19431,\n",
       " 19430,\n",
       " 19429,\n",
       " 19425,\n",
       " 19423,\n",
       " 19416,\n",
       " 19415,\n",
       " 19414,\n",
       " 19411,\n",
       " 19410,\n",
       " 19402,\n",
       " 19401,\n",
       " 19399,\n",
       " 19398,\n",
       " 19397,\n",
       " 19394,\n",
       " 19393,\n",
       " 19392,\n",
       " 19390,\n",
       " 19384,\n",
       " 19383,\n",
       " 19382,\n",
       " 19381,\n",
       " 19380,\n",
       " 19379,\n",
       " 19376,\n",
       " 19375,\n",
       " 19374,\n",
       " 19370,\n",
       " 19366,\n",
       " 19364,\n",
       " 19362,\n",
       " 19361,\n",
       " 19358,\n",
       " 19356,\n",
       " 19355,\n",
       " 19353,\n",
       " 19349,\n",
       " 19348,\n",
       " 19346,\n",
       " 19344,\n",
       " 19341,\n",
       " 19340,\n",
       " 19339,\n",
       " 19338,\n",
       " 19336,\n",
       " 19335,\n",
       " 19334,\n",
       " 19333,\n",
       " 19330,\n",
       " 19327,\n",
       " 19324,\n",
       " 19323,\n",
       " 19322,\n",
       " 19321,\n",
       " 19320,\n",
       " 19319,\n",
       " 19318,\n",
       " 19317,\n",
       " 19316,\n",
       " 19315,\n",
       " 19314,\n",
       " 19313,\n",
       " 19312,\n",
       " 19309,\n",
       " 19307,\n",
       " 19306,\n",
       " 19304,\n",
       " 19303,\n",
       " 19302,\n",
       " 19301,\n",
       " 19297,\n",
       " 19293,\n",
       " 19290,\n",
       " 19289,\n",
       " 19288,\n",
       " 19284,\n",
       " 19283,\n",
       " 19282,\n",
       " 19281,\n",
       " 19277,\n",
       " 19276,\n",
       " 19273,\n",
       " 19272,\n",
       " ...]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20600"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "v2i = {v: i for i, v in enumerate(sorted(vocab), start=1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab['[PAD]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def regular()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {\"a\":1, \"c\":2}\n",
    "#方法\n",
    "a[\"b\"]=a.pop(\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'c': 2, 'b': 1}"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts = np.array([1.32131351465, 2.5456546464, 3.65465465464])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "2.0\n",
      "3.0\n"
     ]
    }
   ],
   "source": [
    "for predict in predicts:\n",
    "    print(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(str(predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"aa.tsv\", \"w\") as fw:\n",
    "    for predict in predicts:\n",
    "        fw.write(str(round(predict, 3)))\n",
    "        fw.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 8439, 8311, 8989, 9918, 8683, 8144, 102]"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = tokenizer(\"202010 113 222222\")\n",
    "input_ids[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', '2020', '##10', '113', '222', '##22', '##2', '[SEP]']"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(input_ids[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./tianchi_datasets/track3_round1_train.tsv\", sep=\"\\t\", header=None,\n",
    "                            names=[\"sentence1\", \"sentence2\", \"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_folds = 10\n",
    "seed_val = 2021\n",
    "skf = StratifiedKFold(n_splits = n_folds, shuffle = True, random_state = seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1], dtype=int64), array([63564, 36436], dtype=int64))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(train_data[\"labels\"], return_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7445383686463938"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "63564 / 36436"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                 1 2 3 4 5 6 7\n",
       "2    17 18 12 19 20 21 22 23 24\n",
       "Name: sentence1, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[\"sentence1\"].iloc[[0, 2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 1], dtype=int64), array([57207, 32793], dtype=int64))\n",
      "(array([0, 1], dtype=int64), array([6357, 3643], dtype=int64))\n",
      "(array([0, 1], dtype=int64), array([57207, 32793], dtype=int64))\n",
      "(array([0, 1], dtype=int64), array([6357, 3643], dtype=int64))\n",
      "(array([0, 1], dtype=int64), array([57207, 32793], dtype=int64))\n",
      "(array([0, 1], dtype=int64), array([6357, 3643], dtype=int64))\n",
      "(array([0, 1], dtype=int64), array([57207, 32793], dtype=int64))\n",
      "(array([0, 1], dtype=int64), array([6357, 3643], dtype=int64))\n",
      "(array([0, 1], dtype=int64), array([57208, 32792], dtype=int64))\n",
      "(array([0, 1], dtype=int64), array([6356, 3644], dtype=int64))\n",
      "(array([0, 1], dtype=int64), array([57208, 32792], dtype=int64))\n",
      "(array([0, 1], dtype=int64), array([6356, 3644], dtype=int64))\n",
      "(array([0, 1], dtype=int64), array([57208, 32792], dtype=int64))\n",
      "(array([0, 1], dtype=int64), array([6356, 3644], dtype=int64))\n",
      "(array([0, 1], dtype=int64), array([57208, 32792], dtype=int64))\n",
      "(array([0, 1], dtype=int64), array([6356, 3644], dtype=int64))\n",
      "(array([0, 1], dtype=int64), array([57208, 32792], dtype=int64))\n",
      "(array([0, 1], dtype=int64), array([6356, 3644], dtype=int64))\n",
      "(array([0, 1], dtype=int64), array([57208, 32792], dtype=int64))\n",
      "(array([0, 1], dtype=int64), array([6356, 3644], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "for train_index, test_index in skf.split(train_data, train_data[\"labels\"]):\n",
    "    X_train, X_val = train_data.iloc[train_index], train_data.iloc[test_index]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   16,    22,    35, ..., 99971, 99990, 99992])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counter = Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counter[0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counter[1] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 0, 1: 1})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counter[0] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 1})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counter[1] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 1, 1: 1})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = [\"aa\", \"bb\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse = lambda x: dict(zip(x, range(len(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id = reverse(id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aa': 0, 'bb': 1}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_num = 10\n",
    "data_file = r\"E:/lv_python/NLP/天池大赛_新闻文本分类/tianchidatasets/train_set.csv\"\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_data = []\n",
    "f = pd.read_csv(data_file, sep='\\t', encoding='UTF-8', nrows = 10000)\n",
    "texts = f['text'].tolist()\n",
    "labels = f['label'].tolist()\n",
    "\n",
    "total = len(labels)\n",
    "\n",
    "index = list(range(total))\n",
    "np.random.shuffle(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_texts = []\n",
    "all_labels = []\n",
    "for i in index:\n",
    "    all_texts.append(texts[i])\n",
    "    all_labels.append(labels[i])\n",
    "\n",
    "label2id = {}\n",
    "for i in range(total):\n",
    "    label = str(all_labels[i])\n",
    "    if label not in label2id:\n",
    "        label2id[label] = [i]\n",
    "    else:\n",
    "        label2id[label].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.extend([[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_data2fold(fold_num, num=10000):\n",
    "    fold_data = []\n",
    "    f = pd.read_csv(data_file, sep='\\t', encoding='UTF-8', nrows = num)\n",
    "    texts = f['text'].tolist()\n",
    "    labels = f['label'].tolist()\n",
    "\n",
    "    total = len(labels)\n",
    "\n",
    "    index = list(range(total))\n",
    "    np.random.shuffle(index)\n",
    "\n",
    "    all_texts = []\n",
    "    all_labels = []\n",
    "    for i in index:\n",
    "        all_texts.append(texts[i])\n",
    "        all_labels.append(labels[i])\n",
    "\n",
    "    label2id = {}\n",
    "    for i in range(total):\n",
    "        label = str(all_labels[i])\n",
    "        if label not in label2id:\n",
    "            label2id[label] = [i]\n",
    "        else:\n",
    "            label2id[label].append(i)\n",
    "\n",
    "    all_index = [[] for _ in range(fold_num)]\n",
    "    for label, data in label2id.items():\n",
    "        # print(label, len(data))\n",
    "        batch_size = int(len(data) / fold_num)\n",
    "        other = len(data) - batch_size * fold_num\n",
    "        for i in range(fold_num):\n",
    "            cur_batch_size = batch_size + 1 if i < other else batch_size\n",
    "            # print(cur_batch_size)\n",
    "            batch_data = [data[i * batch_size + b] for b in range(cur_batch_size)]\n",
    "            all_index[i].extend(batch_data)\n",
    "\n",
    "    batch_size = int(total / fold_num)\n",
    "    other_texts = []\n",
    "    other_labels = []\n",
    "    other_num = 0\n",
    "    start = 0\n",
    "    for fold in range(fold_num):\n",
    "        num = len(all_index[fold])\n",
    "        texts = [all_texts[i] for i in all_index[fold]]\n",
    "        labels = [all_labels[i] for i in all_index[fold]]\n",
    "\n",
    "        if num > batch_size:\n",
    "            fold_texts = texts[:batch_size]\n",
    "            other_texts.extend(texts[batch_size:])\n",
    "            fold_labels = labels[:batch_size]\n",
    "            other_labels.extend(labels[batch_size:])\n",
    "            other_num += num - batch_size\n",
    "        elif num < batch_size:\n",
    "            end = start + batch_size - num\n",
    "            fold_texts = texts + other_texts[start: end]\n",
    "            fold_labels = labels + other_labels[start: end]\n",
    "            start = end\n",
    "        else:\n",
    "            fold_texts = texts\n",
    "            fold_labels = labels\n",
    "        assert batch_size == len(fold_labels)\n",
    "\n",
    "        # shuffle\n",
    "        index = list(range(batch_size))\n",
    "        np.random.shuffle(index)\n",
    "\n",
    "        shuffle_fold_texts = []\n",
    "        shuffle_fold_labels = []\n",
    "        for i in index:\n",
    "            shuffle_fold_texts.append(fold_texts[i])\n",
    "            shuffle_fold_labels.append(fold_labels[i])\n",
    "\n",
    "        data = {'label': shuffle_fold_labels, 'text': shuffle_fold_texts}\n",
    "        fold_data.append(data)\n",
    "\n",
    "    logging.info(\"Fold lens %s\", str([len(data['label']) for data in fold_data]))\n",
    "\n",
    "    return fold_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_data = all_data2fold(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fold_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 0]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fold_data[0][\"label\"][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['4462 4381 3578 6909 2522 62 5870 2465 257 5519 4525 1241 4853 5288 5589 1667 1767 4811 2456 4490 6065 3370 332 2456 433 4381 5988 7492 192 1699 5381 4381 3750 4167 5410 6501 299 4381 5988 5381 4381 4429 4646 3370 3370 3370 6242 1291 2538 2506 5445 3530 7058 6045 6569 2420 669 5926 4516 6920 648 4411 4181 4381 6637 5698 2522 5948 62 5870 4462 4381 3578 6909 648 299 2786 3750 669 4490 3750 6175 4381 6637 3945 3272 299 5122 648 4893 4653 980 6017 980 3223 3750 3272 1697 1363 4490 2456 433 4381 5988 803 257 5519 1635 3750 4811 465 3800 7399 4381 6637 299 5122 5977 2109 634 6122 936 900 4167 5410 3750 6501 299 4381 5988 5381 4381 4429 4646 3370 3370 3370 6242 1291 900 4381 6637 7399 3659 3370 3370 2799 1519 2465 4149 2073 4744 4516 1271 1291 2465 4464 5602 4149 1141 2106 648 3495 4220 6515 5920 3750 3618 7399 4490 4392 669 4659 2465 2799 1903 2073 4525 5977 3750 5780 7399 5589 1667 1460 4462 4381 648 3578 6909 1767 5915 2109 868 4516 2990 900 4462 4381 3578 6909 2899 1395 4381 6637 4516 6390 3750 5998 1460 1871 7180 5470 5560 734 6066 1277 6751 4381 2076 2376 4411 648 5915 6178 4811 7509 2109 4498 2315 900 1460 7154 5426 3780 25 1457 3750 4381 6637 4516 6390 669 4939 1465 7194 5612 5718 3901 648 900 7154 5426 3780 4167 5410 1767 1271 4853 2984 3374 2304 2465 5602 3370 3370 6242 1291 3750 3618 4411 4181 4381 6637 5698 7399 1271 1291 5602 3370 1141 2106 648 623 6104 1609 377 3750 2984 3374 6221 3051 648 4498 2315 669 2109 900 3220 7377 3750 669 2717 3374 4381 4411 4128 2490 4462 4381 5381 2396 7399 3503 5497 648 4381 5988 299 3750 2984 2717 1460 6357 2614 648 7486 2197 3750 3272 2724 5393 5333 6977 4381 6637 648 4516 6390 900 25 1460 4462 4381 5096 2539 4893 25 1457 3750 5780 7399 648 5096 2539 1660 6602 512 2502 1736 7399 803 257 5519 1635 900 5780 7399 4811 5393 6407 5948 623 6637 4741 1080 648 5096 2539 4893 3750 5282 6017 1394 512 4811 465 3800 7399 4381 6637 299 5122 5977 2109 634 6122 936 900 4167 5410 3750 5589 1667 1767 4811 2456 4490 6065 3370 332 7255 6980 1699 3659 3370 3370 6242 1291 4462 4381 648 2456 433 4381 5988 3750 7492 192 1699 5381 4381 5430 5964 900 7543 6930 2282 6832 2376 3750 5948 4381 5988 192 1699 5381 4381 6206 3051 6093 5330 2541 2212 307 1588 3750 1871 6929 6122 6405 3605 6983 5949 2570 4659 4741 6831 3750 5998 2489 5705 5430 4381 5988 5381 4381 648 6751 4381 151 2859 3750 1866 3461 1375 5393 3099 7371 6637 2376 4780 5381 4381 900 1283 6122 5858 1567 3750 5998 1816 4381 5988 151 2859 3099 4659 669 5693 648 3971 3772 900 7543 6930 2282 6832 2376 3750 5705 5430 6122 332 4381 5988 6017 5381 4381 3750 1271 4853 486 3504 3915 4149 6250 1324 6242 1141 2106 3750 6637 5702 4939 4392 1519 4921 2621 648 6065 6482 4525 3223 900 5011 5051 4293 3300 3750 6038 299 1903 408 6515 2799 3370 1519 290 6734 3750 5589 1667 1460 6751 4381 648 3578 6909 1394 1264 619 1735 1906 2380 5689 2400 3750 3618 1401 1519 6017 5037 619 2400 4411 5915 6178 7256 2029 6093 5689 3750 6751 4381 3578 6909 4128 2109 868 2380 23 3750 7399 4381 6637 3397 307 4525 4516 3750 6751 4381 151 2859 4128 5176 6966 512 3223 4462 4381 3750 5948 3272 7399 5998 600 307 4381 6637 3397 4148 2400 3971 3772 900 3618 4392 1519 5948 6017 5589 1667 5915 6178 2437 4958 3489 499 3750 4462 4381 1232 2396 619 4939 5176 535 299 2786 900 669 4490 3750 3659 3370 3370 2799 1519 5948 6017 3750 1141 4411 5915 6178 6236 4576 5560 507 3223 150 6350 4355 3961 3750 1460 1987 4381 3578 6909 4128 2376 5780 4516 6390 3750 1141 4411 2556 4381 1299 1767 4657 2984 5598 1987 4381 648 3374 5393 900 5051 1141 4411 5393 3764 2463 2539 3310 3750 1141 4411 4167 5410 648 4462 4381 1232 2396 669 5926 2380 23 3750 1767 7159 2688 4464 1324 3370 3370 6242 3126 1987 1334 208 5257 4464 1519 4525 5430 900 25 5057 7044 1519 5948 6017 3750 4462 4381 1232 2396 1394 2380 23 5598 3659 4646 3370 3370 6242 1291 3750 6972 6810 3659 2073 3659 4149 4853 3750 1141 4411 4462 4381 1232 2396 2304 4659 6065 6250 1324 3370 4646 4230 1291 900 2265 619 1141 4411 1518 3780 1726 5889 1362 6596 1232 7387 648 5381 4381 2400 3870 3750 4822 5381 4381 6980 6822 1699 5589 1141 648 4464 3370 7186 900 7399 4490 4392 5011 1903 2073 5948 6017 3750 5381 4381 6980 2364 669 5926 2380 23 900 5051 3374 3961 541 3299 4893 2444 1327 7261 3750 4822 5381 4381 6980 7509 1375 5393 1767 1344 1401 2017 340 900 5589 1141 5041 3686 4190 4811 507 3223 5381 4381 2400 3870 3750 3618 5037 619 1232 7387 4939 3039 6822 4562 3893 2693 6713 5612 2465 5491 847 832 6289 4180 3844 2662 4462 4381 3272 5519 2693 3158 3686 6104 3750 7377 1264 5381 4381 6980 4436 1344 1401 4751 2873 3750 1394 2402 993 2376 4436 623 648 4381 6637 900 3166 5445 3750 5589 1667 1070 1036 2109 5393 3764 2087 730 4411 4080 4080 4080 2400 4411 3750 565 742 1401 3272 4399 4630 5598 3223 1264 4381 122 6017 5381 4417 1080 4117 4462 4381 3750 3618 4939 5906 7123 1232 2396 6980 4811 330 3750 1080 4117 6980 4128 7399 656 5472 900 2400 4411 6751 4381 6248 3800 3641 5537 5689 2771 293 3585 4853 5410 4936 3859 2400 4411 648 6751 4381 5620 3374 670 3961 1401 3272 1699 5889 669 2109 1308 3634 6501 5736 4741 4381 648 383 4040 5445 293 1815 3750 910 2399 4939 2400 4411 4411 5619 5381 4381 5393 6407 669 307 900 3166 5445 3750 5381 4381 5393 6407 648 62 2786 4939 6122 1903 1279 2975 5505 2730 5560 6093 5330 7509 307 648 4490 7160 3750 669 5393 6122 2223 25 1394 3750 5282 6017 2400 4411 4381 670 2490 2109 2986 7543 6501 5736 7449 4741 1871 4381 1743 6122 2993 648 6751 4381 2539 3374 900',\n",
       " '69 3772 659 5957 6043 2400 4411 2827 4163 531 5566 2289 2107 1519 5977 4811 7212 299 3765 2974 2107 5677 6017 4659 5689 7180 3750 1460 5511 4105 6167 1043 1871 5889 6301 1985 7194 648 803 1225 806 1635 4080 4080 2827 4163 531 5566 3373 7370 5445 1323 4128 5393 2367 531 6301 2400 4411 2873 4893 4163 531 2597 7160 5498 151 3529 1388 6831 2466 69 3772 659 2717 2490 5999 4853 7399 5689 7180 1344 6630 7058 6045 6966 4053 5445 6122 6122 6453 2099 900 4017 6666 1946 3000 3654 5511 4105 4893 150 1641 1854 1070 2107 5677 6017 5689 7180 648 69 3772 659 1460 5511 4105 4893 648 4469 6040 6167 1043 4650 6543 900 1465 5028 3750 6535 5598 3000 6630 4659 5598 5511 4105 648 6722 252 3613 1141 3750 4893 6588 3686 6959 5736 3750 512 3223 648 4939 3000 6630 4659 5598 5511 4105 4893 150 648 4469 6040 3750 1460 4163 531 648 4469 6040 3750 1460 3630 5538 648 6892 26 900 2873 4893 4163 531 2597 7160 4939 6122 1903 2988 2109 648 2597 7160 3750 69 3772 659 5028 3750 5998 2313 1567 4811 7123 1279 2975 3223 648 803 5511 4105 2106 1919 1635 900 4269 1871 2400 4411 2873 4893 4163 531 2597 7160 1215 6206 7261 2282 5264 4630 486 1394 4939 5511 4105 5689 7180 7212 4125 4893 3750 2367 4848 648 7509 3223 2986 5450 3605 4939 5511 4105 2461 3585 900 192 1699 4163 531 5566 3750 1985 7194 4525 5612 5948 5393 2688 1891 3772 2786 2974 3750 1891 3772 2597 192 3750 6663 1730 5536 4936 3750 340 5511 4105 4893 150 648 1641 1854 3750 5659 803 5511 4105 2106 1919 1635 5612 192 648 470 6407 340 4505 3943 4939 7543 669 5176 648 900 803 5998 1877 2471 3912 2597 7160 7399 2466 5889 6122 1903 5235 5139 2376 5598 6240 2214 3750 2899 1395 6122 1877 450 6929 1394 4939 4163 531 5566 669 5393 6663 1730 3686 3397 2376 5536 4936 3022 900 1635 5028 83 3750 69 3772 659 6919 6734 6017 4017 5598 6122 1903 5338 1952 648 6666 1946 7539 803 290 1702 4163 531 5566 3000 3654 5511 4105 4893 150 1460 1985 7194 4163 531 5537 3961 648 1641 1854 3750 3000 3654 3864 7194 648 58 740 2727 7010 900 1635 7039 1629 2827 4163 531 5566 2490 5821 5881 2289 2107 1519 7044 1519 1324 2073 4149 4853 3750 1985 4411 1070 1036 1629 4163 531 5566 151 4211 3750 736 1859 2376 1324 2595 5744 4163 531 5566 5560 3659 2595 2827 4163 531 5566 900 5998 4939 2400 4411 1722 3051 2873 4893 4163 531 2597 7160 5948 6017 3750 7039 5677 1859 7475 2827 2112 4163 531 5566 900 803 5998 1629 4163 531 5566 4939 1699 4516 6122 6050 2974 1920 1722 2348 3529 340 2974 1920 6919 4655 1952 4417 900 1635 69 3772 659 3097 4636 3750 1465 7194 3605 4326 4811 5330 3630 6248 3495 3750 1866 2109 648 6065 1324 5814 3750 1866 4063 648 6065 3370 5814 3750 3504 1767 7042 900 69 3772 659 5957 6043 3750 5998 5677 1859 7475 5338 1952 5659 299 5677 4811 7509 2109 62 307 3750 4269 1871 2367 2210 5445 1920 3915 6909 2456 4490 4464 3659 3370 3370 4063 5445 3750 1889 6810 1460 5736 1315 4117 1315 3605 4811 6027 5702 648 3915 6909 900 2827 4163 531 5566 3373 7370 5445 1323 299 3765 2974 3750 5998 4939 2109 913 1866 1699 910 3870 648 1031 761 900 69 3772 659 1702 3300 3750 4516 6122 6050 3750 4967 7194 2490 5096 4958 4659 5839 4350 760 1736 648 6248 4603 5881 6182 2466 5526 2400 3750 7327 7256 1722 5780 2367 2210 5566 3634 4163 531 5566 648 4407 5681 3750 1214 3012 6734 1985 4411 2873 4893 4163 531 2367 2210 648 5492 1389 1816 6176 900 803 2490 5430 3659 1519 6810 6065 1519 5445 1920 1460 4967 7194 1080 2210 5821 5881 3750 742 5977 2109 913 1394 1375 5948 7399 3765 2974 2400 3263 4659 2400 4411 2827 2112 648 264 6895 900 1635 69 3772 659 5957 6043 5028 900']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fold_data[0][\"text\"][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build train, dev, test data\n",
    "fold_id = 9\n",
    "\n",
    "# dev\n",
    "dev_data = fold_data[fold_id]\n",
    "\n",
    "# train\n",
    "train_texts = []\n",
    "train_labels = []\n",
    "for i in range(0, fold_id):\n",
    "    data = fold_data[i]\n",
    "    train_texts.extend(data['text'])\n",
    "    train_labels.extend(data['label'])\n",
    "\n",
    "train_data = {'label': train_labels, 'text': train_texts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4462 4381 3578 6909 2522 62 5870 2465 257 5519 4525 1241 4853 5288 5589 1667 1767 4811 2456 4490 6065 3370 332 2456 433 4381 5988 7492 192 1699 5381 4381 3750 4167 5410 6501 299 4381 5988 5381 4381 4429 4646 3370 3370 3370 6242 1291 2538 2506 5445 3530 7058 6045 6569 2420 669 5926 4516 6920 648 4411 4181 4381 6637 5698 2522 5948 62 5870 4462 4381 3578 6909 648 299 2786 3750 669 4490 3750 6175 4381 6637 3945 3272 299 5122 648 4893 4653 980 6017 980 3223 3750 3272 1697 1363 4490 2456 433 4381 5988 803 257 5519 1635 3750 4811 465 3800 7399 4381 6637 299 5122 5977 2109 634 6122 936 900 4167 5410 3750 6501 299 4381 5988 5381 4381 4429 4646 3370 3370 3370 6242 1291 900 4381 6637 7399 3659 3370 3370 2799 1519 2465 4149 2073 4744 4516 1271 1291 2465 4464 5602 4149 1141 2106 648 3495 4220 6515 5920 3750 3618 7399 4490 4392 669 4659 2465 2799 1903 2073 4525 5977 3750 5780 7399 5589 1667 1460 4462 4381 648 3578 6909 1767 5915 2109 868 4516 2990 900 4462 4381 3578 6909 2899 1395 4381 6637 4516 6390 3750 5998 1460 1871 7180 5470 5560 734 6066 1277 6751 4381 2076 2376 4411 648 5915 6178 4811 7509 2109 4498 2315 900 1460 7154 5426 3780 25 1457 3750 4381 6637 4516 6390 669 4939 1465 7194 5612 5718 3901 648 900 7154 5426 3780 4167 5410 1767 1271 4853 2984 3374 2304 2465 5602 3370 3370 6242 1291 3750 3618 4411 4181 4381 6637 5698 7399 1271 1291 5602 3370 1141 2106 648 623 6104 1609 377 3750 2984 3374 6221 3051 648 4498 2315 669 2109 900 3220 7377 3750 669 2717 3374 4381 4411 4128 2490 4462 4381 5381 2396 7399 3503 5497 648 4381 5988 299 3750 2984 2717 1460 6357 2614 648 7486 2197 3750 3272 2724 5393 5333 6977 4381 6637 648 4516 6390 900 25 1460 4462 4381 5096 2539 4893 25 1457 3750 5780 7399 648 5096 2539 1660 6602 512 2502 1736 7399 803 257 5519 1635 900 5780 7399 4811 5393 6407 5948 623 6637 4741 1080 648 5096 2539 4893 3750 5282 6017 1394 512 4811 465 3800 7399 4381 6637 299 5122 5977 2109 634 6122 936 900 4167 5410 3750 5589 1667 1767 4811 2456 4490 6065 3370 332 7255 6980 1699 3659 3370 3370 6242 1291 4462 4381 648 2456 433 4381 5988 3750 7492 192 1699 5381 4381 5430 5964 900 7543 6930 2282 6832 2376 3750 5948 4381 5988 192 1699 5381 4381 6206 3051 6093 5330 2541 2212 307 1588 3750 1871 6929 6122 6405 3605 6983 5949 2570 4659 4741 6831 3750 5998 2489 5705 5430 4381 5988 5381 4381 648 6751 4381 151 2859 3750 1866 3461 1375 5393 3099 7371 6637 2376 4780 5381 4381 900 1283 6122 5858 1567 3750 5998 1816 4381 5988 151 2859 3099 4659 669 5693 648 3971 3772 900 7543 6930 2282 6832 2376 3750 5705 5430 6122 332 4381 5988 6017 5381 4381 3750 1271 4853 486 3504 3915 4149 6250 1324 6242 1141 2106 3750 6637 5702 4939 4392 1519 4921 2621 648 6065 6482 4525 3223 900 5011 5051 4293 3300 3750 6038 299 1903 408 6515 2799 3370 1519 290 6734 3750 5589 1667 1460 6751 4381 648 3578 6909 1394 1264 619 1735 1906 2380 5689 2400 3750 3618 1401 1519 6017 5037 619 2400 4411 5915 6178 7256 2029 6093 5689 3750 6751 4381 3578 6909 4128 2109 868 2380 23 3750 7399 4381 6637 3397 307 4525 4516 3750 6751 4381 151 2859 4128 5176 6966 512 3223 4462 4381 3750 5948 3272 7399 5998 600 307 4381 6637 3397 4148 2400 3971 3772 900 3618 4392 1519 5948 6017 5589 1667 5915 6178 2437 4958 3489 499 3750 4462 4381 1232 2396 619 4939 5176 535 299 2786 900 669 4490 3750 3659 3370 3370 2799 1519 5948 6017 3750 1141 4411 5915 6178 6236 4576 5560 507 3223 150 6350 4355 3961 3750 1460 1987 4381 3578 6909 4128 2376 5780 4516 6390 3750 1141 4411 2556 4381 1299 1767 4657 2984 5598 1987 4381 648 3374 5393 900 5051 1141 4411 5393 3764 2463 2539 3310 3750 1141 4411 4167 5410 648 4462 4381 1232 2396 669 5926 2380 23 3750 1767 7159 2688 4464 1324 3370 3370 6242 3126 1987 1334 208 5257 4464 1519 4525 5430 900 25 5057 7044 1519 5948 6017 3750 4462 4381 1232 2396 1394 2380 23 5598 3659 4646 3370 3370 6242 1291 3750 6972 6810 3659 2073 3659 4149 4853 3750 1141 4411 4462 4381 1232 2396 2304 4659 6065 6250 1324 3370 4646 4230 1291 900 2265 619 1141 4411 1518 3780 1726 5889 1362 6596 1232 7387 648 5381 4381 2400 3870 3750 4822 5381 4381 6980 6822 1699 5589 1141 648 4464 3370 7186 900 7399 4490 4392 5011 1903 2073 5948 6017 3750 5381 4381 6980 2364 669 5926 2380 23 900 5051 3374 3961 541 3299 4893 2444 1327 7261 3750 4822 5381 4381 6980 7509 1375 5393 1767 1344 1401 2017 340 900 5589 1141 5041 3686 4190 4811 507 3223 5381 4381 2400 3870 3750 3618 5037 619 1232 7387 4939 3039 6822 4562 3893 2693 6713 5612 2465 5491 847 832 6289 4180 3844 2662 4462 4381 3272 5519 2693 3158 3686 6104 3750 7377 1264 5381 4381 6980 4436 1344 1401 4751 2873 3750 1394 2402 993 2376 4436 623 648 4381 6637 900 3166 5445 3750 5589 1667 1070 1036 2109 5393 3764 2087 730 4411 4080 4080 4080 2400 4411 3750 565 742 1401 3272 4399 4630 5598 3223 1264 4381 122 6017 5381 4417 1080 4117 4462 4381 3750 3618 4939 5906 7123 1232 2396 6980 4811 330 3750 1080 4117 6980 4128 7399 656 5472 900 2400 4411 6751 4381 6248 3800 3641 5537 5689 2771 293 3585 4853 5410 4936 3859 2400 4411 648 6751 4381 5620 3374 670 3961 1401 3272 1699 5889 669 2109 1308 3634 6501 5736 4741 4381 648 383 4040 5445 293 1815 3750 910 2399 4939 2400 4411 4411 5619 5381 4381 5393 6407 669 307 900 3166 5445 3750 5381 4381 5393 6407 648 62 2786 4939 6122 1903 1279 2975 5505 2730 5560 6093 5330 7509 307 648 4490 7160 3750 669 5393 6122 2223 25 1394 3750 5282 6017 2400 4411 4381 670 2490 2109 2986 7543 6501 5736 7449 4741 1871 4381 1743 6122 2993 648 6751 4381 2539 3374 900'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[\"text\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab():\n",
    "    def __init__(self, train_data):\n",
    "        self.min_count = 5\n",
    "        self.pad = 0\n",
    "        self.unk = 1\n",
    "        self._id2word = ['[PAD]', '[UNK]']\n",
    "        self._id2extword = ['[PAD]', '[UNK]']\n",
    "\n",
    "        self._id2label = []\n",
    "        self.target_names = []\n",
    "\n",
    "        self.build_vocab(train_data)\n",
    "\n",
    "        reverse = lambda x: dict(zip(x, range(len(x))))\n",
    "        self._word2id = reverse(self._id2word)\n",
    "        self._label2id = reverse(self._id2label)\n",
    "\n",
    "        logging.info(\"Build vocab: words %d, labels %d.\" % (self.word_size, self.label_size))\n",
    "\n",
    "    def build_vocab(self, data):\n",
    "        self.word_counter = Counter()\n",
    "        \n",
    "        for text in data['text']:\n",
    "            words = text.split()\n",
    "            for word in words:\n",
    "                self.word_counter[word] += 1\n",
    "\n",
    "        for word, count in self.word_counter.most_common():\n",
    "            if count >= self.min_count:\n",
    "                self._id2word.append(word)\n",
    "\n",
    "        label2name = {0: '科技', 1: '股票', 2: '体育', 3: '娱乐', 4: '时政', 5: '社会', 6: '教育', 7: '财经',\n",
    "                      8: '家居', 9: '游戏', 10: '房产', 11: '时尚', 12: '彩票', 13: '星座'}\n",
    "\n",
    "        self.label_counter = Counter(data['label'])\n",
    "\n",
    "        for label in range(len(self.label_counter)):\n",
    "            count = self.label_counter[label]\n",
    "            self._id2label.append(label)\n",
    "            self.target_names.append(label2name[label])\n",
    "        \n",
    "    def word2id(self, xs):\n",
    "        if isinstance(xs, list):\n",
    "            return [self._word2id.get(x, self.unk) for x in xs]\n",
    "        return self._word2id.get(xs, self.unk)\n",
    "\n",
    "    def extword2id(self, xs):\n",
    "        if isinstance(xs, list):\n",
    "            return [self._extword2id.get(x, self.unk) for x in xs]\n",
    "        return self._extword2id.get(xs, self.unk)\n",
    "\n",
    "    def label2id(self, xs):\n",
    "        if isinstance(xs, list):\n",
    "            return [self._label2id.get(x, self.unk) for x in xs]\n",
    "        return self._label2id.get(xs, self.unk)\n",
    "    \n",
    "    @property\n",
    "    def word_size(self):\n",
    "        return len(self._id2word)\n",
    "\n",
    "    @property\n",
    "    def extword_size(self):\n",
    "        return len(self._id2extword)\n",
    "\n",
    "    @property\n",
    "    def label_size(self):\n",
    "        return len(self._id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word = vocab._word2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"document\"] = data[\"sentence1\"].str.cat(data[\"sentence2\"], sep = ' [SEP] ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', '2', '3', '4', '5', '6', '7', '[SEP]', '8', '9', '10', '4', '11']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"document\"][0].split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"./tianchi_datasets/track3_round1_train.tsv\", sep=\"\\t\", header=None,\n",
    "                                 names=[\"sentence1\", \"sentence2\", \"labels\"])\n",
    "train_data[\"document\"] = train_data[\"sentence1\"].str.cat(train_data[\"sentence2\"], sep = \" [SEP] \")\n",
    "test_data = pd.read_csv(\"./tianchi_datasets/track3_round1_testA.tsv\", sep=\"\\t\", header=None,\n",
    "                                names=[\"sentence1\", \"sentence2\"])\n",
    "test_data[\"document\"] = test_data[\"sentence1\"].str.cat(test_data[\"sentence2\"], sep=\" [SEP] \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counter = Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.24it/s]\n"
     ]
    }
   ],
   "source": [
    "for data in tqdm([train_data, test_data]):\n",
    "        data[\"document\"] = data[\"document\"].map(lambda x:[x for x in x.split(\" \")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"./tianchi_datasets/track3_round1_train.tsv\", sep=\"\\t\", header=None,\n",
    "                                 names=[\"sentence1\", \"sentence2\", \"labels\"])\n",
    "train_data[\"document\"] = train_data[\"sentence1\"].str.cat(train_data[\"sentence2\"], sep = \" [SEP] \")\n",
    "train_data[\"document\"] = train_data[\"document\"].map(lambda x:[word for word in x.split(\" \")])\n",
    "test_data = pd.read_csv(\"./tianchi_datasets/track3_round1_testA.tsv\", sep=\"\\t\", header=None,\n",
    "                                names=[\"sentence1\", \"sentence2\"])\n",
    "test_data[\"document\"] = test_data[\"sentence1\"].str.cat(test_data[\"sentence2\"], sep=\" [SEP] \")\n",
    "test_data[\"document\"] = test_data[\"document\"].map(lambda x: [word for word in x.split(\" \")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_max_length(train_data, test_data):\n",
    "    # 计算最大输入长度\n",
    "    max_len = 0\n",
    "    for data in [train_data, test_data]:\n",
    "        for x in data[\"document\"]:\n",
    "            max_len = max(max_len, len(x) + 2) ##这里加2是因为得开头和结尾加上[CLS]和[SEP]\n",
    "\n",
    "    return max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cal_max_length(train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counter = Counter()\n",
    "\n",
    "for data in [train_data, test_data]:\n",
    "    for text in data[\"document\"]:\n",
    "         for word in text:\n",
    "            word_counter[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = word_counter.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20601"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_counter.most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 5\n",
    "total_cnt = 0\n",
    "for x in word_counter.most_common():\n",
    "    total_cnt += x[1]\n",
    "    if x[1] < 5:\n",
    "        cnt += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13680"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1732317"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('10733', 1)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[14250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {\"[SEP]\", \"[CLS]\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_count = 5\n",
    "vocab = ['[PAD]', '[SEP]', '[CLS]', '[UNK]']\n",
    "word_counter = Counter()\n",
    "\n",
    "for data in [train_data, test_data]:\n",
    "     for text in data[\"document\"]:\n",
    "            for word in text:\n",
    "                 word_counter[word] += 1\n",
    "\n",
    "for word, count in word_counter.most_common():\n",
    "    if count >= min_count:\n",
    "        vocab.append(word)\n",
    "\n",
    "v2i = {v:i for i,v in enumerate(vocab)}\n",
    "i2v = {i:v for v, i in v2i.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'[CLS]', '[PAD]', '[SEP]', '[UNK]'}"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{'[PAD]', '[SEP]', '[CLS]', '[UNK]'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6930"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20601"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "for word, count in word_counter.most_common():\n",
    "    if count >= 2:\n",
    "        cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12364"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = ['[PAD]', '[SEP]', '[CLS]', '[UNK]']\n",
    "word_counter = Counter()\n",
    "\n",
    "for data in [train_data, test_data]:\n",
    "    for text in data[\"document\"]:\n",
    "        for word in text:\n",
    "            word_counter[word] += 1\n",
    "        \n",
    "for word, count in word_counter.most_common():\n",
    "        vocab.append(word)\n",
    "\n",
    "v2i = {v:i for i,v in enumerate(vocab)}\n",
    "i2v = {i:v for v, i in v2i.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = ['[PAD]', '[CLS]', '[UNK]']\n",
    "word_counter = Counter()\n",
    "\n",
    "for data in [train_data, test_data]:\n",
    "    for text in data[\"document\"]:\n",
    "        for word in text:\n",
    "            word_counter[word] += 1\n",
    "        \n",
    "for word, count in word_counter.most_common():\n",
    "    vocab.append(word)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "v2i = {v:i for i,v in enumerate(vocab)}\n",
    "i2v = {i:v for v, i in v2i.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20604"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(v2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20019"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v2i[\"21317\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab_v2i_i2v(train_data, test_data):\n",
    "        ## 获得语料库vocab, id2word, word2id\n",
    "        #min_count = 5\n",
    "        vocab = ['[PAD]', '[UNK]', '[CLS]']\n",
    "        word_counter = Counter()\n",
    "\n",
    "        for data in [train_data, test_data]:\n",
    "            for text in data[\"document\"]:\n",
    "                for word in text:\n",
    "                    word_counter[word] += 1\n",
    "\n",
    "        for word, count in word_counter.most_common():\n",
    "            vocab.append(word)\n",
    "\n",
    "        v2i = {v:i for i,v in enumerate(vocab)}\n",
    "        i2v = {i:v for v, i in v2i.items()}\n",
    "\n",
    "        return vocab, v2i, i2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, v2i, i2v = get_vocab_v2i_i2v(train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'[PAD]': 0,\n",
       " '[UNK]': 1,\n",
       " '[CLS]': 2,\n",
       " '[SEP]': 3,\n",
       " '12': 4,\n",
       " '29': 5,\n",
       " '19': 6,\n",
       " '23': 7,\n",
       " '16': 8,\n",
       " '11': 9,\n",
       " '10': 10,\n",
       " '9': 11,\n",
       " '32': 12,\n",
       " '39': 13,\n",
       " '161': 14,\n",
       " '48': 15,\n",
       " '126': 16,\n",
       " '243': 17,\n",
       " '43': 18,\n",
       " '106': 19,\n",
       " '5': 20,\n",
       " '6': 21,\n",
       " '8': 22,\n",
       " '72': 23,\n",
       " '13': 24,\n",
       " '283': 25,\n",
       " '66': 26,\n",
       " '282': 27,\n",
       " '370': 28,\n",
       " '459': 29,\n",
       " '28': 30,\n",
       " '317': 31,\n",
       " '62': 32,\n",
       " '247': 33,\n",
       " '76': 34,\n",
       " '140': 35,\n",
       " '50': 36,\n",
       " '46': 37,\n",
       " '276': 38,\n",
       " '169': 39,\n",
       " '80': 40,\n",
       " '47': 41,\n",
       " '59': 42,\n",
       " '202': 43,\n",
       " '300': 44,\n",
       " '217': 45,\n",
       " '45': 46,\n",
       " '127': 47,\n",
       " '134': 48,\n",
       " '426': 49,\n",
       " '431': 50,\n",
       " '536': 51,\n",
       " '67': 52,\n",
       " '230': 53,\n",
       " '447': 54,\n",
       " '133': 55,\n",
       " '24': 56,\n",
       " '52': 57,\n",
       " '522': 58,\n",
       " '533': 59,\n",
       " '227': 60,\n",
       " '251': 61,\n",
       " '432': 62,\n",
       " '226': 63,\n",
       " '253': 64,\n",
       " '453': 65,\n",
       " '168': 66,\n",
       " '358': 67,\n",
       " '25': 68,\n",
       " '629': 69,\n",
       " '176': 70,\n",
       " '272': 71,\n",
       " '267': 72,\n",
       " '462': 73,\n",
       " '55': 74,\n",
       " '102': 75,\n",
       " '153': 76,\n",
       " '79': 77,\n",
       " '421': 78,\n",
       " '173': 79,\n",
       " '130': 80,\n",
       " '334': 81,\n",
       " '524': 82,\n",
       " '101': 83,\n",
       " '347': 84,\n",
       " '335': 85,\n",
       " '525': 86,\n",
       " '44': 87,\n",
       " '360': 88,\n",
       " '200': 89,\n",
       " '107': 90,\n",
       " '53': 91,\n",
       " '592': 92,\n",
       " '163': 93,\n",
       " '647': 94,\n",
       " '485': 95,\n",
       " '246': 96,\n",
       " '254': 97,\n",
       " '433': 98,\n",
       " '574': 99,\n",
       " '146': 100,\n",
       " '142': 101,\n",
       " '304': 102,\n",
       " '478': 103,\n",
       " '128': 104,\n",
       " '344': 105,\n",
       " '70': 106,\n",
       " '58': 107,\n",
       " '518': 108,\n",
       " '275': 109,\n",
       " '180': 110,\n",
       " '725': 111,\n",
       " '177': 112,\n",
       " '229': 113,\n",
       " '248': 114,\n",
       " '122': 115,\n",
       " '232': 116,\n",
       " '451': 117,\n",
       " '56': 118,\n",
       " '882': 119,\n",
       " '114': 120,\n",
       " '312': 121,\n",
       " '263': 122,\n",
       " '212': 123,\n",
       " '444': 124,\n",
       " '68': 125,\n",
       " '1006': 126,\n",
       " '118': 127,\n",
       " '481': 128,\n",
       " '470': 129,\n",
       " '782': 130,\n",
       " '249': 131,\n",
       " '457': 132,\n",
       " '213': 133,\n",
       " '115': 134,\n",
       " '1017': 135,\n",
       " '355': 136,\n",
       " '837': 137,\n",
       " '4': 138,\n",
       " '90': 139,\n",
       " '35': 140,\n",
       " '49': 141,\n",
       " '192': 142,\n",
       " '415': 143,\n",
       " '379': 144,\n",
       " '307': 145,\n",
       " '33': 146,\n",
       " '108': 147,\n",
       " '162': 148,\n",
       " '193': 149,\n",
       " '456': 150,\n",
       " '158': 151,\n",
       " '800': 152,\n",
       " '422': 153,\n",
       " '342': 154,\n",
       " '416': 155,\n",
       " '159': 156,\n",
       " '116': 157,\n",
       " '348': 158,\n",
       " '455': 159,\n",
       " '2': 160,\n",
       " '87': 161,\n",
       " '541': 162,\n",
       " '83': 163,\n",
       " '120': 164,\n",
       " '172': 165,\n",
       " '74': 166,\n",
       " '903': 167,\n",
       " '65': 168,\n",
       " '718': 169,\n",
       " '472': 170,\n",
       " '296': 171,\n",
       " '208': 172,\n",
       " '252': 173,\n",
       " '454': 174,\n",
       " '223': 175,\n",
       " '215': 176,\n",
       " '941': 177,\n",
       " '565': 178,\n",
       " '698': 179,\n",
       " '222': 180,\n",
       " '135': 181,\n",
       " '228': 182,\n",
       " '34': 183,\n",
       " '546': 184,\n",
       " '3': 185,\n",
       " '285': 186,\n",
       " '1188': 187,\n",
       " '69': 188,\n",
       " '279': 189,\n",
       " '785': 190,\n",
       " '314': 191,\n",
       " '394': 192,\n",
       " '794': 193,\n",
       " '773': 194,\n",
       " '606': 195,\n",
       " '313': 196,\n",
       " '277': 197,\n",
       " '595': 198,\n",
       " '42': 199,\n",
       " '781': 200,\n",
       " '336': 201,\n",
       " '264': 202,\n",
       " '350': 203,\n",
       " '121': 204,\n",
       " '354': 205,\n",
       " '14': 206,\n",
       " '618': 207,\n",
       " '92': 208,\n",
       " '552': 209,\n",
       " '922': 210,\n",
       " '266': 211,\n",
       " '813': 212,\n",
       " '220': 213,\n",
       " '366': 214,\n",
       " '1195': 215,\n",
       " '1475': 216,\n",
       " '634': 217,\n",
       " '553': 218,\n",
       " '448': 219,\n",
       " '75': 220,\n",
       " '776': 221,\n",
       " '345': 222,\n",
       " '124': 223,\n",
       " '221': 224,\n",
       " '1124': 225,\n",
       " '329': 226,\n",
       " '917': 227,\n",
       " '390': 228,\n",
       " '117': 229,\n",
       " '392': 230,\n",
       " '935': 231,\n",
       " '1642': 232,\n",
       " '274': 233,\n",
       " '471': 234,\n",
       " '406': 235,\n",
       " '332': 236,\n",
       " '442': 237,\n",
       " '535': 238,\n",
       " '521': 239,\n",
       " '311': 240,\n",
       " '804': 241,\n",
       " '353': 242,\n",
       " '82': 243,\n",
       " '548': 244,\n",
       " '771': 245,\n",
       " '540': 246,\n",
       " '467': 247,\n",
       " '1596': 248,\n",
       " '330': 249,\n",
       " '511': 250,\n",
       " '1299': 251,\n",
       " '255': 252,\n",
       " '189': 253,\n",
       " '812': 254,\n",
       " '757': 255,\n",
       " '89': 256,\n",
       " '818': 257,\n",
       " '583': 258,\n",
       " '499': 259,\n",
       " '31': 260,\n",
       " '57': 261,\n",
       " '435': 262,\n",
       " '147': 263,\n",
       " '434': 264,\n",
       " '1652': 265,\n",
       " '1203': 266,\n",
       " '1012': 267,\n",
       " '819': 268,\n",
       " '331': 269,\n",
       " '476': 270,\n",
       " '309': 271,\n",
       " '787': 272,\n",
       " '141': 273,\n",
       " '532': 274,\n",
       " '571': 275,\n",
       " '338': 276,\n",
       " '594': 277,\n",
       " '41': 278,\n",
       " '464': 279,\n",
       " '1259': 280,\n",
       " '191': 281,\n",
       " '579': 282,\n",
       " '1053': 283,\n",
       " '762': 284,\n",
       " '136': 285,\n",
       " '1661': 286,\n",
       " '1002': 287,\n",
       " '299': 288,\n",
       " '971': 289,\n",
       " '665': 290,\n",
       " '831': 291,\n",
       " '398': 292,\n",
       " '396': 293,\n",
       " '73': 294,\n",
       " '308': 295,\n",
       " '260': 296,\n",
       " '361': 297,\n",
       " '333': 298,\n",
       " '635': 299,\n",
       " '1003': 300,\n",
       " '2319': 301,\n",
       " '71': 302,\n",
       " '297': 303,\n",
       " '495': 304,\n",
       " '1277': 305,\n",
       " '561': 306,\n",
       " '295': 307,\n",
       " '918': 308,\n",
       " '93': 309,\n",
       " '1499': 310,\n",
       " '439': 311,\n",
       " '250': 312,\n",
       " '1592': 313,\n",
       " '119': 314,\n",
       " '461': 315,\n",
       " '691': 316,\n",
       " '131': 317,\n",
       " '175': 318,\n",
       " '209': 319,\n",
       " '224': 320,\n",
       " '187': 321,\n",
       " '241': 322,\n",
       " '1227': 323,\n",
       " '494': 324,\n",
       " '550': 325,\n",
       " '591': 326,\n",
       " '1644': 327,\n",
       " '615': 328,\n",
       " '328': 329,\n",
       " '1584': 330,\n",
       " '1555': 331,\n",
       " '915': 332,\n",
       " '938': 333,\n",
       " '2162': 334,\n",
       " '473': 335,\n",
       " '657': 336,\n",
       " '240': 337,\n",
       " '678': 338,\n",
       " '1178': 339,\n",
       " '86': 340,\n",
       " '734': 341,\n",
       " '964': 342,\n",
       " '965': 343,\n",
       " '424': 344,\n",
       " '854': 345,\n",
       " '137': 346,\n",
       " '1182': 347,\n",
       " '643': 348,\n",
       " '1359': 349,\n",
       " '365': 350,\n",
       " '477': 351,\n",
       " '575': 352,\n",
       " '138': 353,\n",
       " '440': 354,\n",
       " '1004': 355,\n",
       " '956': 356,\n",
       " '623': 357,\n",
       " '280': 358,\n",
       " '538': 359,\n",
       " '692': 360,\n",
       " '933': 361,\n",
       " '519': 362,\n",
       " '1319': 363,\n",
       " '380': 364,\n",
       " '1051': 365,\n",
       " '1430': 366,\n",
       " '63': 367,\n",
       " '132': 368,\n",
       " '671': 369,\n",
       " '998': 370,\n",
       " '94': 371,\n",
       " '705': 372,\n",
       " '1024': 373,\n",
       " '806': 374,\n",
       " '145': 375,\n",
       " '1192': 376,\n",
       " '1021': 377,\n",
       " '1068': 378,\n",
       " '1338': 379,\n",
       " '505': 380,\n",
       " '981': 381,\n",
       " '236': 382,\n",
       " '373': 383,\n",
       " '988': 384,\n",
       " '1864': 385,\n",
       " '1352': 386,\n",
       " '889': 387,\n",
       " '1290': 388,\n",
       " '337': 389,\n",
       " '641': 390,\n",
       " '1325': 391,\n",
       " '1110': 392,\n",
       " '37': 393,\n",
       " '1645': 394,\n",
       " '403': 395,\n",
       " '578': 396,\n",
       " '322': 397,\n",
       " '711': 398,\n",
       " '878': 399,\n",
       " '341': 400,\n",
       " '739': 401,\n",
       " '690': 402,\n",
       " '1426': 403,\n",
       " '233': 404,\n",
       " '1104': 405,\n",
       " '1593': 406,\n",
       " '652': 407,\n",
       " '853': 408,\n",
       " '1162': 409,\n",
       " '290': 410,\n",
       " '36': 411,\n",
       " '382': 412,\n",
       " '98': 413,\n",
       " '836': 414,\n",
       " '3034': 415,\n",
       " '40': 416,\n",
       " '1032': 417,\n",
       " '1042': 418,\n",
       " '761': 419,\n",
       " '1236': 420,\n",
       " '783': 421,\n",
       " '646': 422,\n",
       " '1100': 423,\n",
       " '1759': 424,\n",
       " '1640': 425,\n",
       " '675': 426,\n",
       " '989': 427,\n",
       " '123': 428,\n",
       " '829': 429,\n",
       " '1101': 430,\n",
       " '1038': 431,\n",
       " '1130': 432,\n",
       " '302': 433,\n",
       " '239': 434,\n",
       " '190': 435,\n",
       " '1263': 436,\n",
       " '626': 437,\n",
       " '707': 438,\n",
       " '706': 439,\n",
       " '875': 440,\n",
       " '1377': 441,\n",
       " '539': 442,\n",
       " '830': 443,\n",
       " '861': 444,\n",
       " '1365': 445,\n",
       " '1133': 446,\n",
       " '884': 447,\n",
       " '2643': 448,\n",
       " '291': 449,\n",
       " '2169': 450,\n",
       " '689': 451,\n",
       " '542': 452,\n",
       " '611': 453,\n",
       " '599': 454,\n",
       " '612': 455,\n",
       " '669': 456,\n",
       " '753': 457,\n",
       " '395': 458,\n",
       " '790': 459,\n",
       " '389': 460,\n",
       " '852': 461,\n",
       " '940': 462,\n",
       " '1698': 463,\n",
       " '862': 464,\n",
       " '1495': 465,\n",
       " '850': 466,\n",
       " '1958': 467,\n",
       " '315': 468,\n",
       " '1347': 469,\n",
       " '526': 470,\n",
       " '1174': 471,\n",
       " '683': 472,\n",
       " '1326': 473,\n",
       " '1619': 474,\n",
       " '270': 475,\n",
       " '466': 476,\n",
       " '662': 477,\n",
       " '810': 478,\n",
       " '507': 479,\n",
       " '1425': 480,\n",
       " '1': 481,\n",
       " '423': 482,\n",
       " '413': 483,\n",
       " '920': 484,\n",
       " '2538': 485,\n",
       " '1570': 486,\n",
       " '157': 487,\n",
       " '405': 488,\n",
       " '1329': 489,\n",
       " '196': 490,\n",
       " '468': 491,\n",
       " '1226': 492,\n",
       " '949': 493,\n",
       " '1424': 494,\n",
       " '1680': 495,\n",
       " '1249': 496,\n",
       " '2641': 497,\n",
       " '292': 498,\n",
       " '990': 499,\n",
       " '1364': 500,\n",
       " '2124': 501,\n",
       " '319': 502,\n",
       " '1512': 503,\n",
       " '113': 504,\n",
       " '352': 505,\n",
       " '1209': 506,\n",
       " '1311': 507,\n",
       " '265': 508,\n",
       " '437': 509,\n",
       " '1097': 510,\n",
       " '359': 511,\n",
       " '1275': 512,\n",
       " '77': 513,\n",
       " '262': 514,\n",
       " '736': 515,\n",
       " '1310': 516,\n",
       " '357': 517,\n",
       " '1582': 518,\n",
       " '178': 519,\n",
       " '1847': 520,\n",
       " '566': 521,\n",
       " '1635': 522,\n",
       " '1522': 523,\n",
       " '984': 524,\n",
       " '1575': 525,\n",
       " '1132': 526,\n",
       " '1153': 527,\n",
       " '601': 528,\n",
       " '887': 529,\n",
       " '914': 530,\n",
       " '1574': 531,\n",
       " '237': 532,\n",
       " '1239': 533,\n",
       " '20': 534,\n",
       " '1107': 535,\n",
       " '1286': 536,\n",
       " '864': 537,\n",
       " '895': 538,\n",
       " '909': 539,\n",
       " '799': 540,\n",
       " '1129': 541,\n",
       " '1135': 542,\n",
       " '2260': 543,\n",
       " '588': 544,\n",
       " '749': 545,\n",
       " '474': 546,\n",
       " '1397': 547,\n",
       " '397': 548,\n",
       " '916': 549,\n",
       " '590': 550,\n",
       " '1223': 551,\n",
       " '593': 552,\n",
       " '760': 553,\n",
       " '679': 554,\n",
       " '1121': 555,\n",
       " '1667': 556,\n",
       " '99': 557,\n",
       " '719': 558,\n",
       " '1600': 559,\n",
       " '374': 560,\n",
       " '1536': 561,\n",
       " '493': 562,\n",
       " '1027': 563,\n",
       " '1403': 564,\n",
       " '2213': 565,\n",
       " '921': 566,\n",
       " '653': 567,\n",
       " '1274': 568,\n",
       " '744': 569,\n",
       " '784': 570,\n",
       " '310': 571,\n",
       " '500': 572,\n",
       " '1059': 573,\n",
       " '2006': 574,\n",
       " '367': 575,\n",
       " '881': 576,\n",
       " '91': 577,\n",
       " '156': 578,\n",
       " '877': 579,\n",
       " '148': 580,\n",
       " '808': 581,\n",
       " '823': 582,\n",
       " '1405': 583,\n",
       " '30': 584,\n",
       " '491': 585,\n",
       " '2175': 586,\n",
       " '693': 587,\n",
       " '1228': 588,\n",
       " '980': 589,\n",
       " '1675': 590,\n",
       " '788': 591,\n",
       " '1935': 592,\n",
       " '261': 593,\n",
       " '625': 594,\n",
       " '723': 595,\n",
       " '573': 596,\n",
       " '1370': 597,\n",
       " '144': 598,\n",
       " '483': 599,\n",
       " '642': 600,\n",
       " '1013': 601,\n",
       " '95': 602,\n",
       " '1287': 603,\n",
       " '225': 604,\n",
       " '969': 605,\n",
       " '1193': 606,\n",
       " '1648': 607,\n",
       " '1794': 608,\n",
       " '955': 609,\n",
       " '945': 610,\n",
       " '1160': 611,\n",
       " '1525': 612,\n",
       " '979': 613,\n",
       " '1496': 614,\n",
       " '817': 615,\n",
       " '826': 616,\n",
       " '1903': 617,\n",
       " '1087': 618,\n",
       " '1207': 619,\n",
       " '1245': 620,\n",
       " '1085': 621,\n",
       " '1048': 622,\n",
       " '1758': 623,\n",
       " '2415': 624,\n",
       " '369': 625,\n",
       " '197': 626,\n",
       " '684': 627,\n",
       " '798': 628,\n",
       " '1089': 629,\n",
       " '1141': 630,\n",
       " '2483': 631,\n",
       " '281': 632,\n",
       " '2420': 633,\n",
       " '919': 634,\n",
       " '1431': 635,\n",
       " '2436': 636,\n",
       " '947': 637,\n",
       " '105': 638,\n",
       " '2037': 639,\n",
       " '1440': 640,\n",
       " '1154': 641,\n",
       " '349': 642,\n",
       " '356': 643,\n",
       " '741': 644,\n",
       " '1928': 645,\n",
       " '609': 646,\n",
       " '1610': 647,\n",
       " '458': 648,\n",
       " '1322': 649,\n",
       " '682': 650,\n",
       " '1446': 651,\n",
       " '1732': 652,\n",
       " '1361': 653,\n",
       " '1213': 654,\n",
       " '1493': 655,\n",
       " '1803': 656,\n",
       " '674': 657,\n",
       " '7': 658,\n",
       " '661': 659,\n",
       " '2089': 660,\n",
       " '2418': 661,\n",
       " '1050': 662,\n",
       " '2113': 663,\n",
       " '2583': 664,\n",
       " '166': 665,\n",
       " '849': 666,\n",
       " '1168': 667,\n",
       " '2636': 668,\n",
       " '602': 669,\n",
       " '2879': 670,\n",
       " '534': 671,\n",
       " '598': 672,\n",
       " '672': 673,\n",
       " '824': 674,\n",
       " '904': 675,\n",
       " '1438': 676,\n",
       " '1116': 677,\n",
       " '708': 678,\n",
       " '767': 679,\n",
       " '1556': 680,\n",
       " '1094': 681,\n",
       " '1491': 682,\n",
       " '81': 683,\n",
       " '880': 684,\n",
       " '959': 685,\n",
       " '2283': 686,\n",
       " '568': 687,\n",
       " '1553': 688,\n",
       " '1125': 689,\n",
       " '1453': 690,\n",
       " '2059': 691,\n",
       " '2170': 692,\n",
       " '1366': 693,\n",
       " '1770': 694,\n",
       " '1899': 695,\n",
       " '1268': 696,\n",
       " '2732': 697,\n",
       " '2351': 698,\n",
       " '2421': 699,\n",
       " '1389': 700,\n",
       " '506': 701,\n",
       " '754': 702,\n",
       " '100': 703,\n",
       " '613': 704,\n",
       " '972': 705,\n",
       " '288': 706,\n",
       " '1478': 707,\n",
       " '531': 708,\n",
       " '603': 709,\n",
       " '64': 710,\n",
       " '231': 711,\n",
       " '885': 712,\n",
       " '1497': 713,\n",
       " '1934': 714,\n",
       " '1786': 715,\n",
       " '393': 716,\n",
       " '411': 717,\n",
       " '722': 718,\n",
       " '371': 719,\n",
       " '1506': 720,\n",
       " '1545': 721,\n",
       " '1622': 722,\n",
       " '1769': 723,\n",
       " '155': 724,\n",
       " '149': 725,\n",
       " '1001': 726,\n",
       " '856': 727,\n",
       " '1282': 728,\n",
       " '475': 729,\n",
       " '503': 730,\n",
       " '961': 731,\n",
       " '2208': 732,\n",
       " '640': 733,\n",
       " '859': 734,\n",
       " '966': 735,\n",
       " '729': 736,\n",
       " '517': 737,\n",
       " '1041': 738,\n",
       " '2909': 739,\n",
       " '822': 740,\n",
       " '1324': 741,\n",
       " '1534': 742,\n",
       " '2290': 743,\n",
       " '888': 744,\n",
       " '1479': 745,\n",
       " '438': 746,\n",
       " '1894': 747,\n",
       " '2008': 748,\n",
       " '873': 749,\n",
       " '1822': 750,\n",
       " '327': 751,\n",
       " '1461': 752,\n",
       " '1394': 753,\n",
       " '1476': 754,\n",
       " '1706': 755,\n",
       " '1760': 756,\n",
       " '1581': 757,\n",
       " '1170': 758,\n",
       " '502': 759,\n",
       " '1080': 760,\n",
       " '1450': 761,\n",
       " '1588': 762,\n",
       " '1868': 763,\n",
       " '1881': 764,\n",
       " '235': 765,\n",
       " '777': 766,\n",
       " '97': 767,\n",
       " '1092': 768,\n",
       " '2197': 769,\n",
       " '2210': 770,\n",
       " '649': 771,\n",
       " '1568': 772,\n",
       " '2066': 773,\n",
       " '301': 774,\n",
       " '650': 775,\n",
       " '1649': 776,\n",
       " '2250': 777,\n",
       " '1985': 778,\n",
       " '2041': 779,\n",
       " '1022': 780,\n",
       " '1235': 781,\n",
       " '174': 782,\n",
       " '942': 783,\n",
       " '2434': 784,\n",
       " '3284': 785,\n",
       " '3444': 786,\n",
       " '1618': 787,\n",
       " '408': 788,\n",
       " '2409': 789,\n",
       " '710': 790,\n",
       " '1009': 791,\n",
       " '772': 792,\n",
       " '1233': 793,\n",
       " '554': 794,\n",
       " '586': 795,\n",
       " '1139': 796,\n",
       " '1231': 797,\n",
       " '648': 798,\n",
       " '1470': 799,\n",
       " '1993': 800,\n",
       " '2187': 801,\n",
       " '991': 802,\n",
       " '2448': 803,\n",
       " '1415': 804,\n",
       " '1811': 805,\n",
       " '1842': 806,\n",
       " '907': 807,\n",
       " '1354': 808,\n",
       " '2023': 809,\n",
       " '449': 810,\n",
       " '928': 811,\n",
       " '1401': 812,\n",
       " '1206': 813,\n",
       " '809': 814,\n",
       " '811': 815,\n",
       " '3917': 816,\n",
       " '1105': 817,\n",
       " '1723': 818,\n",
       " '1057': 819,\n",
       " '1763': 820,\n",
       " '726': 821,\n",
       " '986': 822,\n",
       " '1666': 823,\n",
       " '1804': 824,\n",
       " '1134': 825,\n",
       " '1819': 826,\n",
       " '2334': 827,\n",
       " '770': 828,\n",
       " '899': 829,\n",
       " '3173': 830,\n",
       " '1877': 831,\n",
       " '2235': 832,\n",
       " '3381': 833,\n",
       " '186': 834,\n",
       " '381': 835,\n",
       " '1272': 836,\n",
       " '3289': 837,\n",
       " '323': 838,\n",
       " '732': 839,\n",
       " '1307': 840,\n",
       " '1832': 841,\n",
       " '2460': 842,\n",
       " '1190': 843,\n",
       " '1500': 844,\n",
       " '497': 845,\n",
       " '1603': 846,\n",
       " '181': 847,\n",
       " '214': 848,\n",
       " '508': 849,\n",
       " '559': 850,\n",
       " '644': 851,\n",
       " '1267': 852,\n",
       " '2229': 853,\n",
       " '482': 854,\n",
       " '520': 855,\n",
       " '1383': 856,\n",
       " '1918': 857,\n",
       " '3452': 858,\n",
       " '614': 859,\n",
       " '1378': 860,\n",
       " '523': 861,\n",
       " '2363': 862,\n",
       " '384': 863,\n",
       " '479': 864,\n",
       " '1345': 865,\n",
       " '2764': 866,\n",
       " '22': 867,\n",
       " '1353': 868,\n",
       " '428': 869,\n",
       " '1801': 870,\n",
       " '2042': 871,\n",
       " '2198': 872,\n",
       " '412': 873,\n",
       " '1854': 874,\n",
       " '54': 875,\n",
       " '446': 876,\n",
       " '109': 877,\n",
       " '1331': 878,\n",
       " '1643': 879,\n",
       " '1672': 880,\n",
       " '1701': 881,\n",
       " '2069': 882,\n",
       " '963': 883,\n",
       " '1276': 884,\n",
       " '1787': 885,\n",
       " '1883': 886,\n",
       " '962': 887,\n",
       " '1106': 888,\n",
       " '1870': 889,\n",
       " '78': 890,\n",
       " '832': 891,\n",
       " '1564': 892,\n",
       " '2172': 893,\n",
       " '2612': 894,\n",
       " '378': 895,\n",
       " '2575': 896,\n",
       " '2746': 897,\n",
       " '655': 898,\n",
       " '2820': 899,\n",
       " '1948': 900,\n",
       " '2000': 901,\n",
       " '1145': 902,\n",
       " '1517': 903,\n",
       " '1990': 904,\n",
       " '194': 905,\n",
       " '1300': 906,\n",
       " '1411': 907,\n",
       " '1445': 908,\n",
       " '1780': 909,\n",
       " '2349': 910,\n",
       " '1793': 911,\n",
       " '2038': 912,\n",
       " '15': 913,\n",
       " '443': 914,\n",
       " '486': 915,\n",
       " '567': 916,\n",
       " '651': 917,\n",
       " '687': 918,\n",
       " '2572': 919,\n",
       " '840': 920,\n",
       " '906': 921,\n",
       " '1158': 922,\n",
       " '1164': 923,\n",
       " '1295': 924,\n",
       " '1551': 925,\n",
       " '2945': 926,\n",
       " '3882': 927,\n",
       " '866': 928,\n",
       " '3331': 929,\n",
       " '21': 930,\n",
       " '60': 931,\n",
       " '1058': 932,\n",
       " '1662': 933,\n",
       " '2046': 934,\n",
       " '3478': 935,\n",
       " '608': 936,\n",
       " '620': 937,\n",
       " '791': 938,\n",
       " '1693': 939,\n",
       " '3171': 940,\n",
       " '704': 941,\n",
       " '897': 942,\n",
       " '925': 943,\n",
       " '1252': 944,\n",
       " '1323': 945,\n",
       " '2328': 946,\n",
       " '636': 947,\n",
       " '1198': 948,\n",
       " '1448': 949,\n",
       " '2214': 950,\n",
       " '216': 951,\n",
       " '967': 952,\n",
       " '1016': 953,\n",
       " '2047': 954,\n",
       " '589': 955,\n",
       " '624': 956,\n",
       " '170': 957,\n",
       " '512': 958,\n",
       " '3931': 959,\n",
       " '560': 960,\n",
       " '600': 961,\n",
       " '855': 962,\n",
       " '1613': 963,\n",
       " '673': 964,\n",
       " '1332': 965,\n",
       " '1507': 966,\n",
       " '1812': 967,\n",
       " '2601': 968,\n",
       " '205': 969,\n",
       " '750': 970,\n",
       " '2487': 971,\n",
       " '4108': 972,\n",
       " '968': 973,\n",
       " '1211': 974,\n",
       " '1421': 975,\n",
       " '1653': 976,\n",
       " '1798': 977,\n",
       " '2309': 978,\n",
       " '3026': 979,\n",
       " '1654': 980,\n",
       " '1789': 981,\n",
       " '1988': 982,\n",
       " '1396': 983,\n",
       " '1636': 984,\n",
       " '2116': 985,\n",
       " '2471': 986,\n",
       " '164': 987,\n",
       " '977': 988,\n",
       " '1180': 989,\n",
       " '1219': 990,\n",
       " '2403': 991,\n",
       " '2595': 992,\n",
       " '218': 993,\n",
       " '937': 994,\n",
       " '1650': 995,\n",
       " '1660': 996,\n",
       " '110': 997,\n",
       " '179': 998,\n",
       " '219': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v2i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.1\n",
    "seed_val = 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "            train_data[\"document\"], train_data[\"labels\"], test_size=test_size,\n",
    "            stratify=train_data[\"labels\"], random_state=seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_bert_datasets(data, labels, max_length, v2i):\n",
    "    input_ids = []\n",
    "\n",
    "    for inputs in data:\n",
    "        padded = np.full(max_length, fill_value=0, dtype=np.int32)\n",
    "        padded[:(len(inputs) + 2)] = [v2i['[CLS]']] + [v2i.get(x, 1) for x in inputs] + [v2i['[SEP]']]\n",
    "        input_ids.append(torch.LongTensor([padded]))\n",
    "\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "\n",
    "    if labels is None:\n",
    "        return Data.TensorDataset(input_ids)\n",
    "    else:\n",
    "        labels = torch.tensor(labels.values)\n",
    "        return Data.TensorDataset(input_ids, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = convert_to_bert_datasets(X_train, y_train, max_len, v2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = Data.DataLoader(train, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  2,  23,   5,  ...,   0,   0,   0],\n",
      "        [  2, 236, 283,  ...,   0,   0,   0],\n",
      "        [  2, 151, 617,  ...,   0,   0,   0],\n",
      "        ...,\n",
      "        [  2,   4, 198,  ...,   0,   0,   0],\n",
      "        [  2,  27,  25,  ...,   0,   0,   0],\n",
      "        [  2,   7,   6,  ...,   0,   0,   0]]), tensor([0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1])]\n"
     ]
    }
   ],
   "source": [
    "for x in train_dataloader:\n",
    "    print(x)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v2i.get(\"72\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  2,  23,   5, 138, 410, 146,   3,  10, 792,  18, 410, 146,  23,   5,\n",
       "        138,  20,  21,  45,  16,   3,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0])"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:-1][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['72',\n",
       " '29',\n",
       " '4',\n",
       " '290',\n",
       " '33',\n",
       " '[SEP]',\n",
       " '10',\n",
       " '772',\n",
       " '43',\n",
       " '290',\n",
       " '33',\n",
       " '72',\n",
       " '29',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '217',\n",
       " '126']"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[38335]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Data_generator_modified import Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************Data_precessing*********************************\n",
      "## dataset size is 100000\n",
      "*******************************Load  dataset ....*******************************\n",
      "## Load Datasets Consume 0:00:06 s ###\n",
      "************************All Train and Test Data loaded !************************\n"
     ]
    }
   ],
   "source": [
    "corpus = Corpus( batch_size , seed_val)\n",
    "train_loader, valid_loader, test_loader = corpus.get_loaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  1,   4,   7,  ...,   0,   0,   0],\n",
      "        [  1,   4,  20,  ...,   0,   0,   0],\n",
      "        [  1,  48, 573,  ...,   0,   0,   0],\n",
      "        ...,\n",
      "        [  1, 953,   5,  ...,   0,   0,   0],\n",
      "        [  1, 262,   9,  ...,   0,   0,   0],\n",
      "        [  1,   5,  32,  ...,   0,   0,   0]])]\n"
     ]
    }
   ],
   "source": [
    "for x in test_loader:\n",
    "    print(x)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = nn.Embedding(16, 3)\n",
    "dropout = nn.Dropout(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight',\n",
       "              tensor([[-5.4280e-01,  6.7888e-01, -4.2775e-01],\n",
       "                      [ 5.4533e-01,  9.4880e-02,  2.6756e-01],\n",
       "                      [-1.2994e+00,  9.4631e-04,  1.8252e-01],\n",
       "                      [ 2.2095e+00, -6.1967e-01,  2.6030e-01],\n",
       "                      [ 3.7188e-01, -5.8912e-01, -1.1403e-01],\n",
       "                      [ 2.3440e+00,  6.3106e-01, -5.6090e-01],\n",
       "                      [ 1.5914e+00,  2.0060e+00, -3.5820e-01],\n",
       "                      [-6.9950e-01, -9.5084e-02, -1.6080e+00],\n",
       "                      [ 1.4059e+00,  1.0950e+00, -1.6617e+00],\n",
       "                      [-1.0668e+00, -1.2655e+00, -1.0872e-01],\n",
       "                      [-4.1067e-01, -7.9915e-01, -1.0813e+00],\n",
       "                      [ 3.5618e-01, -6.2622e-01,  7.8715e-01],\n",
       "                      [-2.5217e-01,  4.2111e-01,  6.4201e-01],\n",
       "                      [-1.0840e+00, -4.7387e-01,  3.8295e-01],\n",
       "                      [ 1.2236e-01,  6.9582e-01, -1.5468e+00],\n",
       "                      [-5.2373e-01,  2.6528e-01,  2.4131e-01]]))])"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(10, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = nn.Embedding(3, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputsa = torch.LongTensor([1, 2, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0885,  0.6042,  1.5657,  0.5765, -2.6056],\n",
       "        [ 0.0842,  0.2499,  0.8024,  1.9202, -1.2176],\n",
       "        [ 0.5256,  0.7225,  0.4041, -0.5163,  0.1360]],\n",
       "       grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb(inputsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_data = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "def create_new_traindata(test_score, train_data, test_data):\n",
    "    tmp = []\n",
    "    dir_path = os.getcwd()\n",
    "    \n",
    "    with open(os.path.join(dir_path, test_score) , \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            content = json.loads(line)\n",
    "            score = content[\"label\"]\n",
    "            if score > 0.95:\n",
    "                tmp.append(1)\n",
    "            elif score < 0.05:\n",
    "                tmp.append(0)\n",
    "            else:\n",
    "                tmp.append(\"unk\")\n",
    "    \n",
    "    with open(os.path.join(dir_path, test_data), \"r\", encoding=\"utf-8\") as fr:\n",
    "        with open(\"tianchi_datasets/track3_round1_testA_label.tsv\", \"w\", encoding=\"utf-8\") as fw:\n",
    "            data = fr.readlines()\n",
    "            for i in range(len(data)):\n",
    "                content = data[i].strip()\n",
    "                label = tmp[i]\n",
    "                if label in [0, 1]:\n",
    "                    lis = content + \"\\t\" + str(label) + \"\\n\"\n",
    "                else:\n",
    "                    lis = \"\"\n",
    "                fw.write(lis)\n",
    "    \n",
    "    train_data = pd.read_csv(os.path.join(dir_path, train_data), sep=\"\\t\", header=None,\n",
    "                                 names=[\"sentence1\", \"sentence2\", \"labels\"])\n",
    "    test_data = pd.read_csv(\"./tianchi_datasets/track3_round1_testA_label.tsv\", sep=\"\\t\", header=None,\n",
    "                                 names=[\"sentence1\", \"sentence2\", \"labels\"])\n",
    "    \n",
    "    new_data = pd.concat([train_data, test_data], axis = 0)\n",
    "    new_data.to_csv(\"./tianchi_datasets/track3_round1_newtrain.tsv\", sep = \"\\t\", header=None, index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E:\\\\lv_python\\\\NLP\\\\阿里天池小布助手'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_score = \"tianchi_datasets/test.json\"\n",
    "train_data = \"tianchi_datasets/track3_round1_train.tsv\"\n",
    "test_data = \"tianchi_datasets/track3_round1_testA.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_new_traindata(test_score, train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"./tianchi_datasets/track3_round1_train.tsv\", sep=\"\\t\", header=None,\n",
    "                                 names=[\"sentence1\", \"sentence2\", \"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1 2 3 4 5 6 7</td>\n",
       "      <td>8 9 10 4 11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12 13 14 15</td>\n",
       "      <td>12 15 11 16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17 18 12 19 20 21 22 23 24</td>\n",
       "      <td>12 23 25 6 26 27 19</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28 29 30 31 11</td>\n",
       "      <td>32 33 34 30 31</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29 35 36 29</td>\n",
       "      <td>29 37 36 29</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>12 19 1162 126 53 66</td>\n",
       "      <td>12 19 79 389 126 53 66</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>275 552 553 433 881 338 1104 101 202 2343 14825</td>\n",
       "      <td>995 551 550 1660 2830 1075 662 935</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>421 330 62 12 80 81 82 76</td>\n",
       "      <td>202 62 12 80 838 76</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>177 455 456 3474 964 1364 55 1364</td>\n",
       "      <td>133 134 2246</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>29 168 12 19 1003 719 23 29 263 276</td>\n",
       "      <td>29 23 12 115 115 263 16</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             sentence1  \\\n",
       "0                                        1 2 3 4 5 6 7   \n",
       "1                                          12 13 14 15   \n",
       "2                           17 18 12 19 20 21 22 23 24   \n",
       "3                                       28 29 30 31 11   \n",
       "4                                          29 35 36 29   \n",
       "...                                                ...   \n",
       "99995                             12 19 1162 126 53 66   \n",
       "99996  275 552 553 433 881 338 1104 101 202 2343 14825   \n",
       "99997                        421 330 62 12 80 81 82 76   \n",
       "99998                177 455 456 3474 964 1364 55 1364   \n",
       "99999              29 168 12 19 1003 719 23 29 263 276   \n",
       "\n",
       "                                sentence2  labels  \n",
       "0                             8 9 10 4 11       0  \n",
       "1                             12 15 11 16       0  \n",
       "2                     12 23 25 6 26 27 19       1  \n",
       "3                          32 33 34 30 31       1  \n",
       "4                             29 37 36 29       1  \n",
       "...                                   ...     ...  \n",
       "99995              12 19 79 389 126 53 66       1  \n",
       "99996  995 551 550 1660 2830 1075 662 935       0  \n",
       "99997                 202 62 12 80 838 76       1  \n",
       "99998                        133 134 2246       1  \n",
       "99999             29 23 12 115 115 263 16       1  \n",
       "\n",
       "[100000 rows x 3 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = pd.read_csv(\"./tianchi_datasets/track3_round1_train_new.tsv\", sep=\"\\t\", header=None,\n",
    "                                 names=[\"sentence1\", \"sentence2\", \"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data1 = pd.concat([train_data, new_data], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data1.to_csv(\"./tianchi_datasets/track3_round1_train_new1.tsv\", sep = \"\\t\", header=None, index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data2 = pd.read_csv(\"./tianchi_datasets/track3_round1_train_new1.tsv\", sep=\"\\t\", header=None,\n",
    "                                 names=[\"sentence1\", \"sentence2\", \"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1 2 3 4 5 6 7</td>\n",
       "      <td>8 9 10 4 11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12 13 14 15</td>\n",
       "      <td>12 15 11 16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17 18 12 19 20 21 22 23 24</td>\n",
       "      <td>12 23 25 6 26 27 19</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28 29 30 31 11</td>\n",
       "      <td>32 33 34 30 31</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29 35 36 29</td>\n",
       "      <td>29 37 36 29</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115401</th>\n",
       "      <td>29 243 2107 2109 208</td>\n",
       "      <td>29 275 272 91 2108</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115402</th>\n",
       "      <td>4915 23 39 9 3544 19</td>\n",
       "      <td>158 478 426 19 1274 120 212 1378 157</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115403</th>\n",
       "      <td>29 81 140 72 12 5 1763 1764 12 459 16</td>\n",
       "      <td>1553 72 12 20811 6722 16</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115404</th>\n",
       "      <td>740 2155 6667</td>\n",
       "      <td>20273 762 852 11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115405</th>\n",
       "      <td>12 317 3434 16</td>\n",
       "      <td>134 853 12 317 1596</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>115406 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    sentence1  \\\n",
       "0                               1 2 3 4 5 6 7   \n",
       "1                                 12 13 14 15   \n",
       "2                  17 18 12 19 20 21 22 23 24   \n",
       "3                              28 29 30 31 11   \n",
       "4                                 29 35 36 29   \n",
       "...                                       ...   \n",
       "115401                   29 243 2107 2109 208   \n",
       "115402                   4915 23 39 9 3544 19   \n",
       "115403  29 81 140 72 12 5 1763 1764 12 459 16   \n",
       "115404                          740 2155 6667   \n",
       "115405                         12 317 3434 16   \n",
       "\n",
       "                                   sentence2  labels  \n",
       "0                                8 9 10 4 11       0  \n",
       "1                                12 15 11 16       0  \n",
       "2                        12 23 25 6 26 27 19       1  \n",
       "3                             32 33 34 30 31       1  \n",
       "4                                29 37 36 29       1  \n",
       "...                                      ...     ...  \n",
       "115401                    29 275 272 91 2108       1  \n",
       "115402  158 478 426 19 1274 120 212 1378 157       0  \n",
       "115403              1553 72 12 20811 6722 16       1  \n",
       "115404                      20273 762 852 11       0  \n",
       "115405                   134 853 12 317 1596       1  \n",
       "\n",
       "[115406 rows x 3 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"document\"] = train_data[\"sentence1\"].str.cat(train_data[\"sentence2\"], sep = \" [SEP] \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = train_data[\"document\"][99999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'29 168 12 19 1003 719 23 29 263 276 [SEP] 29 23 12 115 115 263 16'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[\"document\"][99999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1 2 3 4 5 6 7 [SEP] 8 9 10 4 11'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"./pretrain_model/chinese_roberta_wwm_ext/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 8162, 8832, 8110, 8131, 8135, 8152, 8459, 8160, 8133, 8162, 10864, 11173, 102, 8162, 8133, 8110, 8787, 8787, 10864, 8121, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " '29',\n",
       " '168',\n",
       " '12',\n",
       " '19',\n",
       " '100',\n",
       " '##3',\n",
       " '71',\n",
       " '##9',\n",
       " '23',\n",
       " '29',\n",
       " '263',\n",
       " '276',\n",
       " '[SEP]',\n",
       " '29',\n",
       " '23',\n",
       " '12',\n",
       " '115',\n",
       " '115',\n",
       " '263',\n",
       " '16',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(input_ids[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 'unk',\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 'unk',\n",
       " 0,\n",
       " 'unk',\n",
       " 0,\n",
       " 'unk',\n",
       " 'unk',\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 'unk',\n",
       " 'unk',\n",
       " 0,\n",
       " 0,\n",
       " 'unk',\n",
       " 'unk',\n",
       " 'unk',\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 'unk',\n",
       " 0,\n",
       " 'unk',\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 'unk',\n",
       " 0,\n",
       " 'unk',\n",
       " 1,\n",
       " 'unk',\n",
       " 0,\n",
       " 0,\n",
       " 'unk',\n",
       " 'unk',\n",
       " 0,\n",
       " 'unk',\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 'unk',\n",
       " 0,\n",
       " 'unk',\n",
       " 'unk',\n",
       " 0,\n",
       " 1,\n",
       " 'unk',\n",
       " 0,\n",
       " 'unk',\n",
       " 0,\n",
       " 1,\n",
       " 'unk',\n",
       " 'unk',\n",
       " 'unk',\n",
       " 0,\n",
       " 'unk',\n",
       " 0,\n",
       " 'unk',\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 'unk',\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 'unk',\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 'unk',\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 'unk',\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 'unk',\n",
       " 'unk',\n",
       " 'unk',\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 'unk',\n",
       " 'unk',\n",
       " 1,\n",
       " 'unk',\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 'unk',\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 'unk',\n",
       " 'unk',\n",
       " 'unk',\n",
       " 0,\n",
       " 'unk',\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 'unk',\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 'unk',\n",
       " 1,\n",
       " 'unk',\n",
       " 0,\n",
       " 'unk',\n",
       " 'unk',\n",
       " 'unk',\n",
       " 'unk',\n",
       " 1,\n",
       " 1,\n",
       " 'unk',\n",
       " 'unk',\n",
       " 'unk',\n",
       " 0,\n",
       " 'unk',\n",
       " 1,\n",
       " 'unk',\n",
       " 'unk',\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 'unk',\n",
       " 'unk',\n",
       " 1,\n",
       " 'unk',\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 'unk',\n",
       " 0,\n",
       " 'unk',\n",
       " 1,\n",
       " 0,\n",
       " 'unk',\n",
       " 'unk',\n",
       " 1,\n",
       " 0,\n",
       " 'unk',\n",
       " 0,\n",
       " 0,\n",
       " 'unk',\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 'unk',\n",
       " 0,\n",
       " 'unk',\n",
       " 'unk',\n",
       " 0,\n",
       " 'unk',\n",
       " 'unk',\n",
       " 1,\n",
       " 0,\n",
       " 'unk',\n",
       " 1,\n",
       " 0,\n",
       " 'unk',\n",
       " 1,\n",
       " 0,\n",
       " 'unk',\n",
       " 0,\n",
       " 0,\n",
       " 'unk',\n",
       " 'unk',\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 'unk',\n",
       " 'unk',\n",
       " 'unk',\n",
       " 'unk',\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 'unk',\n",
       " 1,\n",
       " 'unk',\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 'unk',\n",
       " 'unk',\n",
       " 'unk',\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 'unk',\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 'unk',\n",
       " 1,\n",
       " 'unk',\n",
       " 'unk',\n",
       " 0,\n",
       " 'unk',\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 'unk',\n",
       " 0,\n",
       " 1,\n",
       " 'unk',\n",
       " 'unk',\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 'unk',\n",
       " 0,\n",
       " 1,\n",
       " 'unk',\n",
       " 0,\n",
       " 'unk',\n",
       " 'unk',\n",
       " 'unk',\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 'unk',\n",
       " 'unk',\n",
       " 1,\n",
       " 1,\n",
       " 'unk',\n",
       " 'unk',\n",
       " 0,\n",
       " 0,\n",
       " 'unk',\n",
       " 0,\n",
       " 'unk',\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 'unk',\n",
       " 1,\n",
       " 0,\n",
       " 'unk',\n",
       " 'unk',\n",
       " 1,\n",
       " 'unk',\n",
       " 0,\n",
       " 0,\n",
       " 'unk',\n",
       " 0,\n",
       " 0,\n",
       " 'unk',\n",
       " 'unk',\n",
       " 'unk',\n",
       " 0,\n",
       " 1,\n",
       " 'unk',\n",
       " 0,\n",
       " 1,\n",
       " 'unk',\n",
       " 'unk',\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 'unk',\n",
       " 'unk',\n",
       " 0,\n",
       " 0,\n",
       " 'unk',\n",
       " 'unk',\n",
       " 'unk',\n",
       " 'unk',\n",
       " 'unk',\n",
       " 'unk',\n",
       " 0,\n",
       " 0,\n",
       " 'unk',\n",
       " 'unk',\n",
       " 1,\n",
       " 0,\n",
       " 'unk',\n",
       " 0,\n",
       " 'unk',\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 'unk',\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 'unk',\n",
       " 'unk',\n",
       " 1,\n",
       " 0,\n",
       " 'unk',\n",
       " 'unk',\n",
       " 0,\n",
       " 0,\n",
       " 'unk',\n",
       " 'unk',\n",
       " 'unk',\n",
       " 'unk',\n",
       " 0,\n",
       " 'unk',\n",
       " 'unk',\n",
       " 'unk',\n",
       " 'unk',\n",
       " 'unk',\n",
       " 0,\n",
       " 1,\n",
       " 'unk',\n",
       " 0,\n",
       " 0,\n",
       " 'unk',\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 'unk',\n",
       " 'unk',\n",
       " 0,\n",
       " 'unk',\n",
       " 'unk',\n",
       " 0,\n",
       " 'unk',\n",
       " 'unk',\n",
       " 'unk',\n",
       " 0,\n",
       " 'unk',\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 'unk',\n",
       " 'unk',\n",
       " 1,\n",
       " 'unk',\n",
       " 'unk',\n",
       " 'unk',\n",
       " 'unk',\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 'unk',\n",
       " 1,\n",
       " 0,\n",
       " 'unk',\n",
       " 'unk',\n",
       " 'unk',\n",
       " 'unk',\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 'unk',\n",
       " 'unk',\n",
       " 1,\n",
       " 'unk',\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 'unk',\n",
       " 'unk',\n",
       " 1,\n",
       " 'unk',\n",
       " 'unk',\n",
       " 'unk',\n",
       " 'unk',\n",
       " 1,\n",
       " 1,\n",
       " 'unk',\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 'unk',\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 'unk',\n",
       " 'unk',\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 'unk',\n",
       " 0,\n",
       " 'unk',\n",
       " 1,\n",
       " 'unk',\n",
       " 'unk',\n",
       " 1,\n",
       " 'unk',\n",
       " 'unk',\n",
       " 0,\n",
       " 'unk',\n",
       " 0,\n",
       " 'unk',\n",
       " 'unk',\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 'unk',\n",
       " 0,\n",
       " 1,\n",
       " 'unk',\n",
       " 0,\n",
       " 0,\n",
       " 'unk',\n",
       " 'unk',\n",
       " 1,\n",
       " 'unk',\n",
       " 0,\n",
       " 'unk',\n",
       " 1,\n",
       " 'unk',\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 'unk',\n",
       " 'unk',\n",
       " 1,\n",
       " 'unk',\n",
       " 'unk',\n",
       " 'unk',\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 'unk',\n",
       " 1,\n",
       " 'unk',\n",
       " 'unk',\n",
       " 'unk',\n",
       " 'unk',\n",
       " 'unk',\n",
       " 1,\n",
       " 'unk',\n",
       " 1,\n",
       " 0,\n",
       " 'unk',\n",
       " 0,\n",
       " 1,\n",
       " 'unk',\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 'unk',\n",
       " 'unk',\n",
       " 1,\n",
       " 1,\n",
       " 'unk',\n",
       " 'unk',\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 'unk',\n",
       " 'unk',\n",
       " 0,\n",
       " 1,\n",
       " 'unk',\n",
       " 0,\n",
       " 1,\n",
       " 'unk',\n",
       " 0,\n",
       " 'unk',\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 'unk',\n",
       " 'unk',\n",
       " 'unk',\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 'unk',\n",
       " 1,\n",
       " 1,\n",
       " 'unk',\n",
       " 0,\n",
       " 'unk',\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 'unk',\n",
       " 0,\n",
       " 0,\n",
       " 'unk',\n",
       " 'unk',\n",
       " 'unk',\n",
       " 1,\n",
       " 'unk',\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 'unk',\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 'unk',\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 'unk',\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 'unk',\n",
       " 1,\n",
       " 'unk',\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 'unk',\n",
       " 0,\n",
       " 'unk',\n",
       " 'unk',\n",
       " 1,\n",
       " 1,\n",
       " 'unk',\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 'unk',\n",
       " 1,\n",
       " 'unk',\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 'unk',\n",
       " 'unk',\n",
       " 0,\n",
       " 'unk',\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 'unk',\n",
       " 1,\n",
       " 'unk',\n",
       " 1,\n",
       " 'unk',\n",
       " 1,\n",
       " 'unk',\n",
       " 'unk',\n",
       " 1,\n",
       " 0,\n",
       " 'unk',\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 'unk',\n",
       " 0,\n",
       " 1,\n",
       " 'unk',\n",
       " 0,\n",
       " 'unk',\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 'unk',\n",
       " 'unk',\n",
       " 1,\n",
       " 'unk',\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 'unk',\n",
       " 'unk',\n",
       " 1,\n",
       " 1,\n",
       " 'unk',\n",
       " 'unk',\n",
       " 0,\n",
       " 'unk',\n",
       " 0,\n",
       " 'unk',\n",
       " 'unk',\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 'unk',\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 'unk',\n",
       " 0,\n",
       " 'unk',\n",
       " 0,\n",
       " 0,\n",
       " 'unk',\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 'unk',\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 'unk',\n",
       " 0,\n",
       " 1,\n",
       " 'unk',\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 'unk',\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 'unk',\n",
       " 0,\n",
       " 'unk',\n",
       " 1,\n",
       " 'unk',\n",
       " 0,\n",
       " 'unk',\n",
       " 0,\n",
       " 1,\n",
       " 'unk',\n",
       " 'unk',\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 'unk',\n",
       " 'unk',\n",
       " 'unk',\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 'unk',\n",
       " 0,\n",
       " 1,\n",
       " 'unk',\n",
       " 'unk',\n",
       " 'unk',\n",
       " 'unk',\n",
       " 'unk',\n",
       " 0,\n",
       " 0,\n",
       " 'unk',\n",
       " 'unk',\n",
       " 1,\n",
       " 'unk',\n",
       " 1,\n",
       " 0,\n",
       " 'unk',\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 'unk',\n",
       " 1,\n",
       " 1,\n",
       " 'unk',\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 'unk',\n",
       " 0,\n",
       " 'unk',\n",
       " 0,\n",
       " 1,\n",
       " 'unk',\n",
       " 0,\n",
       " 'unk',\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 'unk',\n",
       " 'unk',\n",
       " 'unk',\n",
       " 'unk',\n",
       " 0,\n",
       " 'unk',\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 'unk',\n",
       " 'unk',\n",
       " 1,\n",
       " 'unk',\n",
       " 0,\n",
       " 'unk',\n",
       " 0,\n",
       " 'unk',\n",
       " 0,\n",
       " 'unk',\n",
       " 'unk',\n",
       " 0,\n",
       " 'unk',\n",
       " 'unk',\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 'unk',\n",
       " 0,\n",
       " 1,\n",
       " 'unk',\n",
       " 'unk',\n",
       " 'unk',\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 'unk',\n",
       " 'unk',\n",
       " 'unk',\n",
       " 0,\n",
       " 'unk',\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 'unk',\n",
       " 0,\n",
       " 'unk',\n",
       " 1,\n",
       " 'unk',\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 'unk',\n",
       " 0,\n",
       " 0,\n",
       " 'unk',\n",
       " 'unk',\n",
       " 'unk',\n",
       " 0,\n",
       " 'unk',\n",
       " 'unk',\n",
       " 0,\n",
       " 0,\n",
       " 'unk',\n",
       " 0,\n",
       " 0,\n",
       " 'unk',\n",
       " 0,\n",
       " 0,\n",
       " 'unk',\n",
       " 0,\n",
       " 1,\n",
       " 'unk',\n",
       " 0,\n",
       " 1,\n",
       " 'unk',\n",
       " 1,\n",
       " 'unk',\n",
       " 'unk',\n",
       " 0,\n",
       " 'unk',\n",
       " 0,\n",
       " 0,\n",
       " 'unk',\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 'unk',\n",
       " 1,\n",
       " 'unk',\n",
       " 0,\n",
       " 0,\n",
       " 'unk',\n",
       " 'unk',\n",
       " 'unk',\n",
       " 0,\n",
       " 'unk',\n",
       " 'unk',\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 'unk',\n",
       " 'unk',\n",
       " 1,\n",
       " 0,\n",
       " 'unk',\n",
       " 1,\n",
       " 'unk',\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 'unk',\n",
       " 'unk',\n",
       " 'unk',\n",
       " 0,\n",
       " 'unk',\n",
       " 'unk',\n",
       " 'unk',\n",
       " 'unk',\n",
       " 'unk',\n",
       " 0,\n",
       " 0,\n",
       " 'unk',\n",
       " 'unk',\n",
       " 'unk',\n",
       " 'unk',\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 'unk',\n",
       " 0,\n",
       " 1,\n",
       " 'unk',\n",
       " 'unk',\n",
       " 'unk',\n",
       " 1,\n",
       " 'unk',\n",
       " 'unk',\n",
       " 'unk',\n",
       " 'unk',\n",
       " 'unk',\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 'unk',\n",
       " 0,\n",
       " 'unk',\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 'unk',\n",
       " 'unk',\n",
       " 0,\n",
       " 0,\n",
       " 'unk',\n",
       " 'unk',\n",
       " 'unk',\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 'unk',\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 'unk',\n",
       " 'unk',\n",
       " 0,\n",
       " 'unk',\n",
       " 'unk',\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 'unk',\n",
       " 0,\n",
       " 'unk',\n",
       " 'unk',\n",
       " 'unk',\n",
       " 'unk',\n",
       " 'unk',\n",
       " 'unk',\n",
       " 'unk',\n",
       " 0,\n",
       " 0,\n",
       " 'unk',\n",
       " 0,\n",
       " 1,\n",
       " 'unk',\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 'unk',\n",
       " 'unk',\n",
       " 0,\n",
       " 0,\n",
       " 'unk',\n",
       " 'unk',\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 'unk',\n",
       " 1,\n",
       " ...]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocnli_train = pd.read_csv(\"./tianchi_datasets/track3_round1_train_new.tsv\", sep=\"\\t\", header=None,\n",
    "                                  quoting=3, encoding=\"utf-8\", names=[\"id\", \"sentence1\", \"sentence2\", \"class\"])\n",
    "        ocnli_train[\"document\"] = ocnli_train[\"sentence1\"].str.cat(ocnli_train[\"sentence2\"], sep=\" [SEP] \")\n",
    "        ocnli_test = pd.read_csv(\"./tianchi_datasets/track3_round1_testA.tsv\", sep=\"\\t\", header=None,\n",
    "                                 quoting=3, encoding=\"utf-8\", names=[\"id\", \"sentence1\", \"sentence2\"])\n",
    "        ocnli_test[\"document\"] = ocnli_test[\"sentence1\"].str.cat(ocnli_test[\"sentence2\"], sep=\" [SEP] \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tianchi_datasets/track3_round1_train.tsv'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = os.getcwd()\n",
    "train = pd.read_csv(os.path.join(dir_path, train_data), sep=\"\\t\", header=None,\n",
    "                                 names=[\"sentence1\", \"sentence2\", \"labels\"])\n",
    "test = pd.read_csv(\"./tianchi_datasets/track3_round1_testA.tsv\", sep=\"\\t\", header=None,\n",
    "                                 names=[\"sentence1\", \"sentence2\", \"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(count     100000.000000\n",
       " mean          23.421830\n",
       " std           10.443222\n",
       " min            3.000000\n",
       " 50%           21.000000\n",
       " 99%           56.000000\n",
       " 99.9%         90.000000\n",
       " 99.99%       159.000600\n",
       " max          236.000000\n",
       " Name: sentence1, dtype: float64,\n",
       " count     100000.000000\n",
       " mean          23.410970\n",
       " std           10.431748\n",
       " min            3.000000\n",
       " 50%           21.000000\n",
       " 99%           56.000000\n",
       " 99.9%         91.000000\n",
       " 99.99%       144.000100\n",
       " max          185.000000\n",
       " Name: sentence2, dtype: float64,\n",
       " count     25000.000000\n",
       " mean         22.314120\n",
       " std           9.104125\n",
       " min           4.000000\n",
       " 50%          21.000000\n",
       " 99%          50.000000\n",
       " 99.9%        63.000000\n",
       " 99.99%       91.000000\n",
       " max         225.000000\n",
       " Name: sentence1, dtype: float64,\n",
       " count     25000.000000\n",
       " mean         22.388520\n",
       " std           9.290472\n",
       " min           4.000000\n",
       " 50%          21.000000\n",
       " 99%          50.000000\n",
       " 99.9%        68.000000\n",
       " 99.99%      101.501300\n",
       " max         311.000000\n",
       " Name: sentence2, dtype: float64)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"sentence1\"].str.len().describe(percentiles = [.99, .999, .9999]), train[\"sentence2\"].str.len().describe(percentiles = [.99, .999, .9999])\\\n",
    ",test[\"sentence1\"].str.len().describe(percentiles = [.99, .999, .9999]), test[\"sentence2\"].str.len().describe(percentiles = [.99, .999, .9999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight=torch.from_numpy(np.array([0.1,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight1 = torch.tensor([0.1,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight == weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "    \"it is a good day, I like to stay here\",\n",
    "    \"I am happy to be here\",\n",
    "    \"I am bob\",\n",
    "    \"it is sunny today\",\n",
    "    \"I have a party today\",\n",
    "    \"it is a dog and that is a cat\",\n",
    "    \"there are dog and cat on the tree\",\n",
    "    \"I study hard this morning\",\n",
    "    \"today is a good day\",\n",
    "    \"tomorrow will be a good day\",\n",
    "    \"I like coffee, I like book and I like apple\",\n",
    "    \"I do not like it\",\n",
    "    \"I am kitty, I like bob\",\n",
    "    \"I do not care who like bob, but I like kitty\",\n",
    "    \"It is coffee time, bring your cup\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_words = [d.replace(\",\", \"\").split(\" \") for d in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set(itertools.chain(*docs_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "v2i = {v: i for i, v in enumerate(vocab)}\n",
    "i2v = {i: v for v, i in v2i.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_methods = {\n",
    "        \"log\": lambda x: np.log(1+x),\n",
    "        \"augmented\": lambda x: 0.5 + 0.5 * x / np.max(x, axis=1, keepdims=True),\n",
    "        \"boolean\": lambda x: np.minimum(x, 1),\n",
    "        \"log_avg\": lambda x: (1 + safe_log(x)) / (1 + safe_log(np.mean(x, axis=1, keepdims=True))),\n",
    "    }\n",
    "idf_methods = {\n",
    "        \"log\": lambda x: 1 + np.log(len(docs) / (x+1)),\n",
    "        \"prob\": lambda x: np.maximum(0, np.log((len(docs) - x) / (x+1))),\n",
    "        \"len_norm\": lambda x: x / (np.sum(np.square(x))+1),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "_tf = np.zeros((len(vocab), len(docs)), dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "d = docs_words[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0986122886681098"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_methods.get(\"log\")(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'there',\n",
       " 1: 'cat',\n",
       " 2: 'sunny',\n",
       " 3: 'happy',\n",
       " 4: 'good',\n",
       " 5: 'apple',\n",
       " 6: 'have',\n",
       " 7: 'care',\n",
       " 8: 'your',\n",
       " 9: 'bring',\n",
       " 10: 'am',\n",
       " 11: 'day',\n",
       " 12: 'the',\n",
       " 13: 'it',\n",
       " 14: 'who',\n",
       " 15: 'is',\n",
       " 16: 'this',\n",
       " 17: 'like',\n",
       " 18: 'a',\n",
       " 19: 'study',\n",
       " 20: 'not',\n",
       " 21: 'tree',\n",
       " 22: 'coffee',\n",
       " 23: 'be',\n",
       " 24: 'that',\n",
       " 25: 'party',\n",
       " 26: 'do',\n",
       " 27: 'I',\n",
       " 28: 'today',\n",
       " 29: 'to',\n",
       " 30: 'kitty',\n",
       " 31: 'will',\n",
       " 32: 'cup',\n",
       " 33: 'morning',\n",
       " 34: 'and',\n",
       " 35: 'stay',\n",
       " 36: 'are',\n",
       " 37: 'but',\n",
       " 38: 'hard',\n",
       " 39: 'dog',\n",
       " 40: 'It',\n",
       " 41: 'time',\n",
       " 42: 'tomorrow',\n",
       " 43: 'bob',\n",
       " 44: 'on',\n",
       " 45: 'here',\n",
       " 46: 'book'}"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('it', 1)]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter.most_common(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = np.zeros((len(i2v), 1))\n",
    "for i in range(len(i2v)):\n",
    "        d_count = 0\n",
    "        for d in docs_words:\n",
    "            d_count += 1 if i2v[i] in d else 0\n",
    "        df[i, 0] = d_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tf(method=\"log\"):\n",
    "    # term frequency: how frequent a word appears in a doc\n",
    "    _tf = np.zeros((len(vocab), len(docs)), dtype=np.float64)    # [n_vocab, n_doc]\n",
    "    for i, d in enumerate(docs_words):\n",
    "        counter = Counter(d)\n",
    "        for v in counter.keys():\n",
    "            _tf[v2i[v], i] = counter[v] / counter.most_common(1)[0][1]\n",
    "\n",
    "    weighted_tf = tf_methods.get(method, None)\n",
    "    if weighted_tf is None:\n",
    "        raise ValueError\n",
    "    return weighted_tf(_tf)\n",
    "\n",
    "\n",
    "def get_idf(method=\"log\"):\n",
    "    # inverse document frequency: low idf for a word appears in more docs, mean less important\n",
    "    df = np.zeros((len(i2v), 1))\n",
    "    for i in range(len(i2v)):\n",
    "        d_count = 0\n",
    "        for d in docs_words:\n",
    "            d_count += 1 if i2v[i] in d else 0\n",
    "        df[i, 0] = d_count\n",
    "\n",
    "        \n",
    "    idf_fn = idf_methods.get(method, None)\n",
    "    if idf_fn is None:\n",
    "        raise ValueError\n",
    "    return idf_fn(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf shape(vocab in each docs):  (47, 15)\n",
      "\n",
      "tf samples:\n",
      " [[0.         0.         0.         0.         0.         0.\n",
      "  0.69314718 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.40546511\n",
      "  0.69314718 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]]\n",
      "\n",
      "idf shape(vecb in all docs):  (47, 1)\n",
      "\n",
      "idf samples:\n",
      " [[3.01490302]\n",
      " [2.60943791]]\n",
      "\n",
      "tf_idf shape:  (47, 15)\n",
      "\n",
      "tf_idf sample:\n",
      " [[0.         0.         0.         0.         0.         0.\n",
      "  2.08977153 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         1.05803603\n",
      "  1.80872453 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "tf = get_tf()           # [n_vocab, n_doc]\n",
    "idf = get_idf()         # [n_vocab, 1]\n",
    "tf_idf = tf * idf       # [n_vocab, n_doc]\n",
    "print(\"tf shape(vocab in each docs): \", tf.shape)\n",
    "print(\"\\ntf samples:\\n\", tf[:2])\n",
    "print(\"\\nidf shape(vecb in all docs): \", idf.shape)\n",
    "print(\"\\nidf samples:\\n\", idf[:2])\n",
    "print(\"\\ntf_idf shape: \", tf_idf.shape)\n",
    "print(\"\\ntf_idf sample:\\n\", tf_idf[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tf[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.69314718,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.69314718, 0.        , 0.69314718, 0.        ,\n",
       "       0.69314718, 0.        , 0.69314718, 0.69314718, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.69314718, 0.        , 0.69314718,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.69314718, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.69314718, 0.        ])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.69314718, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = tf_idf[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 1.60931851,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 1.60931851, 0.        , 1.45464719, 0.        ,\n",
       "       1.32827152, 0.        , 1.32827152, 1.32827152, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.97419418, 0.        , 1.80872453,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       2.08977153, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       1.80872453, 0.        ])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0897715283505334"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col[35]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0, 24, 25, 26, 28, 30, 31, 32, 33, 22, 34, 37, 38, 39, 40, 41, 42,\n",
       "       43, 44, 36, 21, 23, 19,  1,  2,  3,  5,  6,  7,  8,  9, 20, 10, 14,\n",
       "       16, 12, 46, 27, 15, 17, 18, 13, 11,  4, 29, 45, 35], dtype=int64)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[2, 2], [1, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.40546511, 0.69314718, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ])"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       1.05803603, 1.80872453, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ])"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.60943791])"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.array([[2],[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2],\n",
       "       [2]])"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4, 4],\n",
       "       [2, 2]])"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"I get a coffee cup\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6])"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum([1, 2, 3], keepdims = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True])"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf.T.dot(tf).ravel() == idf.T.dot(tf).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_words = [d.replace(\",\", \"\").split(\" \") for d in docs]\n",
    "vocab = set(itertools.chain(*docs_words))\n",
    "v2i = {v: i for i, v in enumerate(vocab)}\n",
    "i2v = {i: v for v, i in v2i.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"./tianchi_datasets/track3_round1_train.tsv\")\n",
    "test_data = pd.read_csv(\"./tianchi_datasets/track3_round1_testA.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"document\"] = train_data[\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"./tianchi_datasets/track3_round1_train.tsv\", sep=\"\\t\", header=None,nrows = 10000,\n",
    "                                  quoting=3, encoding=\"utf-8\", names=[\"sentence1\", \"sentence2\", \"class\"])\n",
    "train_data[\"document\"] = train_data[\"sentence1\"].str.cat(train_data[\"sentence2\"], sep=\" \")\n",
    "test_data = pd.read_csv(\"./tianchi_datasets/track3_round1_testA.tsv\", sep=\"\\t\", header=None, nrows = 100,\n",
    "                                 quoting=3, encoding=\"utf-8\", names=[\"sentence1\", \"sentence2\"])\n",
    "test_data[\"document\"] = test_data[\"sentence1\"].str.cat(test_data[\"sentence2\"], sep=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "q = []\n",
    "for word1, word2 in itertools.zip_longest(train_data[\"document\"], test_data[\"document\"]):\n",
    "    docs.append(word1)\n",
    "    if word2 is None:\n",
    "        continue\n",
    "    q.append(word2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_words = [d.replace(\",\", \"\").split(\" \") for d in docs]\n",
    "vocab = set(itertools.chain(*docs_words))\n",
    "v2i = {v: i for i, v in enumerate(vocab)}\n",
    "i2v = {i: v for v, i in v2i.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_methods = {\n",
    "        \"log\": lambda x: np.log(1+x),\n",
    "        \"augmented\": lambda x: 0.5 + 0.5 * x / np.max(x, axis=1, keepdims=True),\n",
    "        \"boolean\": lambda x: np.minimum(x, 1),\n",
    "        \"log_avg\": lambda x: (1 + safe_log(x)) / (1 + safe_log(np.mean(x, axis=1, keepdims=True))),\n",
    "    }\n",
    "idf_methods = {\n",
    "        \"log\": lambda x: 1 + np.log(len(docs) / (x+1)),\n",
    "        \"prob\": lambda x: np.maximum(0, np.log((len(docs) - x) / (x+1))),\n",
    "        \"len_norm\": lambda x: x / (np.sum(np.square(x))+1),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tf(method=\"log\"):\n",
    "    # term frequency: how frequent a word appears in a doc\n",
    "    _tf = np.zeros((len(vocab), len(docs)), dtype=np.float64)    # [n_vocab, n_doc]\n",
    "    for i, d in enumerate(docs_words):\n",
    "        counter = Counter(d)\n",
    "        for v in counter.keys():\n",
    "            _tf[v2i[v], i] = counter[v] / counter.most_common(1)[0][1]\n",
    "\n",
    "    weighted_tf = tf_methods.get(method, None)\n",
    "    if weighted_tf is None:\n",
    "        raise ValueError\n",
    "    return weighted_tf(_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_idf(method=\"log\"):\n",
    "    # inverse document frequency: low idf for a word appears in more docs, mean less important\n",
    "    df = np.zeros((len(i2v), 1))\n",
    "    for i in range(len(i2v)):\n",
    "        d_count = 0\n",
    "        for d in docs_words:\n",
    "            d_count += 1 if i2v[i] in d else 0\n",
    "        df[i, 0] = d_count\n",
    "\n",
    "    idf_fn = idf_methods.get(method, None)\n",
    "    if idf_fn is None:\n",
    "        raise ValueError\n",
    "    return idf_fn(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(q, _tf_idf):\n",
    "    unit_q = q / np.sqrt(np.sum(np.square(q), axis=0, keepdims=True))\n",
    "    unit_ds = _tf_idf / np.sqrt(np.sum(np.square(_tf_idf), axis=0, keepdims=True))\n",
    "    similarity = unit_ds.T.dot(unit_q).ravel()\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [],
   "source": [
    "def docs_score(q, len_norm=False):\n",
    "    global idf, tf_idf\n",
    "    q_words = q.replace(\",\", \"\").split(\" \")\n",
    "\n",
    "    # add unknown words\n",
    "    unknown_v = 0\n",
    "    for v in set(q_words):\n",
    "        if v not in v2i:\n",
    "            v2i[v] = len(v2i)\n",
    "            i2v[len(v2i)-1] = v\n",
    "            unknown_v += 1\n",
    "    if unknown_v > 0:\n",
    "        _idf = np.concatenate((idf, np.zeros((unknown_v, 1), dtype=np.float)), axis=0)\n",
    "        _tf_idf = np.concatenate((tf_idf, np.zeros((unknown_v, tf_idf.shape[1]), dtype=np.float)), axis=0)\n",
    "    else:\n",
    "        _idf, _tf_idf = idf, tf_idf\n",
    "    counter = Counter(q_words)\n",
    "    q_tf = np.zeros((len(_idf), 1), dtype=np.float)     # [n_vocab, 1]\n",
    "    for v in counter.keys():\n",
    "        q_tf[v2i[v], 0] = counter[v]\n",
    "\n",
    "    q_vec = q_tf * _idf            # [n_vocab, 1]\n",
    "\n",
    "    q_scores = cosine_similarity(q_vec, _tf_idf)\n",
    "    if len_norm:\n",
    "        len_docs = [len(d) for d in docs_words]\n",
    "        q_scores = q_scores / np.array(len_docs)\n",
    "    \n",
    "    idf, tf_idf = _idf, _tf_idf\n",
    "    return q_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = get_tf()           # [n_vocab, n_doc]\n",
    "idf = get_idf()         # [n_vocab, 1]\n",
    "tf_idf = tf * idf       # [n_vocab, n_doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:03<00:00, 29.83it/s]\n"
     ]
    }
   ],
   "source": [
    "tmp = []\n",
    "for content in tqdm(q):\n",
    "    scores = docs_score(content)\n",
    "    score = max(scores)\n",
    "    idx = np.argmax(scores)\n",
    "    if score > 0.7:\n",
    "        label = train_data[\"class\"][idx]\n",
    "    else:\n",
    "        label = 2\n",
    "    tmp.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[\"label\"] = tmp\n",
    "newtest_1 = test_data[test_data[\"label\"] < 2]\n",
    "newtest = pd.read_csv(\"./tianchi_datasets/track3_round1_testA_label.tsv\", sep = \"\\t\", \n",
    "                     header = None, names = [\"sentence1\", \"sentence2\", \"label\"])\n",
    "newtest2 = pd.merge(newtest_1, newtest,  how = \"inner\")\n",
    "newtrain = pd.concat([train_data, newtest2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newtrain.to_csv(\"./tianchi_datasets/track3_round1_newtrain.tsv\", sep=\"\\t\", header=None, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'newtest1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-551-0d43faa66fca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnewtest1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'newtest1' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"./tianchi_datasets/track3_round1_train.tsv\", sep=\"\\t\", header=None,\n",
    "                                  quoting=3, encoding=\"utf-8\", names=[\"sentence1\", \"sentence2\", \"label\"])\n",
    "train_data[\"document\"] = train_data[\"sentence1\"].str.cat(train_data[\"sentence2\"], sep=\" [SEP] \")\n",
    "test_data = pd.read_csv(\"./tianchi_datasets/track3_round1_testA.tsv\", sep=\"\\t\", header=None,\n",
    "                                 quoting=3, encoding=\"utf-8\", names=[\"sentence1\", \"sentence2\"])\n",
    "test_data[\"document\"] = test_data[\"sentence1\"].str.cat(test_data[\"sentence2\"], sep=\" [SEP] \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"./pretrain_model/chinese_roberta_wwm_ext\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_max_length(data):\n",
    "    # 计算最大输入长度\n",
    "    max_len = 0\n",
    "    for x in tqdm(data[\"document\"]):\n",
    "        max_len = max(max_len, len(tokenizer(x)[\"input_ids\"]))\n",
    "    return max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 100000/100000 [00:51<00:00, 1948.89it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "97"
      ]
     },
     "execution_count": 567,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cal_max_length(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(\"./tianchi_datasets/track3_round1_testA.tsv\", sep=\"\\t\", header=None,\n",
    "                                 quoting=3, encoding=\"utf-8\", names=[\"sentence1\", \"sentence2\"])\n",
    "test_data[\"document\"] = test_data[\"sentence1\"].str.cat(test_data[\"sentence2\"], sep=\" [SEP] \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 25000/25000 [00:11<00:00, 2142.60it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "152"
      ]
     },
     "execution_count": 571,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cal_max_length(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 122, 123, 124, 125, 126, 127, 128, 102, 129, 130, 8108, 125, 8111, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 573,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(train_data[\"document\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.utils.data as Data\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "def convert_to_bert_dataset(data, labels, tokenizer, max_length, name):\n",
    "    input_ids = []\n",
    "    token_type_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for x in tqdm(data, desc=\"convert_to_bert_{}_dataset\".format(name)):\n",
    "        encoded_input = tokenizer.encode_plus(x, add_special_tokens=True,\n",
    "                                              max_length=max_length, padding=\"max_length\",\n",
    "                                              return_attention_mask=True,\n",
    "                                              return_tensors=\"pt\", truncation=True)\n",
    "\n",
    "        input_ids.append(encoded_input[\"input_ids\"])\n",
    "        attention_masks.append(encoded_input[\"attention_mask\"])\n",
    "        # 对于bert,输入有token_type_ids,其他模型没有\n",
    "        try:\n",
    "            token_type_ids.append(encoded_input[\"token_type_ids\"])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # convert lists to tensor, bert在Pytorch中只接受torch格式的输入\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "\n",
    "    if len(token_type_ids) != 0:\n",
    "        token_type_ids = torch.cat(token_type_ids, dim=0)\n",
    "\n",
    "    # 对于测试集，没有labels\n",
    "    if labels is None:\n",
    "        # 对于非bert类输入,没有token_type_ids\n",
    "        if len(token_type_ids) == 0:\n",
    "            return Data.TensorDataset(input_ids, attention_masks)\n",
    "        return Data.TensorDataset(input_ids, attention_masks, token_type_ids)\n",
    "\n",
    "    else:\n",
    "        labels = torch.tensor(labels.values)\n",
    "        if len(token_type_ids) == 0:\n",
    "            return Data.TensorDataset(input_ids, attention_masks, labels)\n",
    "        return Data.TensorDataset(input_ids, attention_masks, token_type_ids, labels)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Corpus():\n",
    "\n",
    "    def __init__(self, tokenizer, batch_size, seed_val):\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.seed_val = seed_val\n",
    "        # 保持验证集和测试集数量大致相同,约为1500,1500/53386 = 0.0281\n",
    "        # 0.1表示10折交叉验证,训练集和验证集比例\n",
    "        self.test_scale = 0.1\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        print(f\"{'Data_precessing':*^80}\")\n",
    "        \"\"\"\n",
    "        “当文本文件中带有英文双引号时，直接用pd.read_csv进行读取会导致行数减少，\n",
    "        此时应该对read_csv设置参数quoting=3或者quoting=csv.QUOTE_NONE”\n",
    "        不设置quoting，默认会去除英文双引号，只留下英文双引号内的内容，设置quoting = 3，会如实读取内容。\n",
    "        \"\"\"\n",
    "        train_data = pd.read_csv(\"./tianchi_datasets/track3_round1_newtrain.tsv\", sep=\"\\t\", header=None,\n",
    "                                  quoting=3, encoding=\"utf-8\", names=[\"sentence1\", \"sentence2\", \"labels\"])\n",
    "        train_data[\"document\"] = train_data[\"sentence1\"].str.cat(train_data[\"sentence2\"], sep=\" [SEP] \")\n",
    "        test_data = pd.read_csv(\"./tianchi_datasets/track3_round1_testA.tsv\", sep=\"\\t\", header=None,\n",
    "                                 quoting=3, encoding=\"utf-8\", names=[\"sentence1\", \"sentence2\"])\n",
    "        test_data[\"document\"] = test_data[\"sentence1\"].str.cat(test_data[\"sentence2\"], sep=\" [SEP] \")\n",
    "\n",
    "        print(\"## dataset size is {}\".format(len(train_data)))\n",
    "\n",
    "        # 计算max length, 计算耗时,直接保存结果，大概需要耗时30s\n",
    "        self.max_length = 152\n",
    "        \"\"\"\n",
    "        max_len = max(self.cal_max_length(ocnli_train), self.cal_max_length(ocnli_test))\n",
    "        print(\"!!!### max token length of FineTune model : \", max_len)\n",
    "        \"\"\"\n",
    "\n",
    "        print(f\"{'Load OCNLI dataset ....':*^80}\")\n",
    "        self.train_loader, self.valid_loader, self.test_loader,\\\n",
    "            = self.load_generator(self.test_scale,\n",
    "            train_data[[\"document\", \"labels\"]], test_data[[\"document\"]], self.batch_size)\n",
    "\n",
    "        print(f\"{'All Train and Test Data loaded !':*^80}\")\n",
    "\n",
    "    def load_generator(self, test_size, train_data, test_data, batch_size):\n",
    "        \"\"\"生成训练、验证、测试集合的dataloader\"\"\"\n",
    "\n",
    "\n",
    "        # 划分训练集和验证集,同时根据label值分层抽样\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            train_data[\"document\"], train_data[\"labels\"], test_size=test_size,\n",
    "            stratify= train_data[\"labels\"], random_state=self.seed_val)\n",
    "\n",
    "        train = convert_to_bert_dataset(X_train, y_train, self.tokenizer, self.max_length, \"train\")\n",
    "        validation = convert_to_bert_dataset(X_val, y_val, self.tokenizer, self.max_length, \"valid\")\n",
    "        test = convert_to_bert_dataset(test_data[\"document\"], None, self.tokenizer, self.max_length, \"test\")\n",
    "\n",
    "        train_dataloader = Data.DataLoader(\n",
    "            train, batch_size=batch_size, shuffle=True)\n",
    "        valid_dataloader = Data.DataLoader(\n",
    "            validation, batch_size=batch_size, shuffle=False)\n",
    "        ##因为要逐条预测\n",
    "        test_dataloader = Data.DataLoader(test, batch_size=1, shuffle=False)\n",
    "\n",
    "        return train_dataloader, valid_dataloader, test_dataloader\n",
    "\n",
    "\n",
    "\n",
    "    def cal_max_length(self, data):\n",
    "        # 计算最大输入长度\n",
    "        max_len = 0\n",
    "        for x in tqdm(data[\"document\"]):\n",
    "            max_len = max(max_len, len(self.tokenizer(x)[\"input_ids\"]))\n",
    "        return max_len\n",
    "\n",
    "\n",
    "    def get_loaders(self):\n",
    "        return self.train_loader, self.valid_loader, self.test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[\"label\"] = list(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "seed_val = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************Data_precessing*********************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "convert_to_bert_train_dataset:   0%|                                                        | 0/103865 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## dataset size is 115406\n",
      "****************************Load OCNLI dataset ....*****************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "convert_to_bert_train_dataset: 100%|█████████████████████████████████████████| 103865/103865 [00:54<00:00, 1904.39it/s]\n",
      "convert_to_bert_valid_dataset: 100%|███████████████████████████████████████████| 11541/11541 [00:07<00:00, 1623.92it/s]\n",
      "convert_to_bert_test_dataset: 100%|████████████████████████████████████████████| 25000/25000 [00:12<00:00, 2008.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************All Train and Test Data loaded !************************\n"
     ]
    }
   ],
   "source": [
    "corpus = Corpus(tokenizer, batch_size, seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, valid_loader, test_loader = corpus.get_loaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 588,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  101,  8356,  8160,  8162, 10658, 12404, 12570, 11173,   102,  8246,\n",
      "          8160, 12873,  8162, 10658, 10541,   102,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  8162, 10345,  8148,  9389,  8737,  8108,  8428,  8159,  8111,\n",
      "           102,  8162, 10924,  8352,  8544,   102,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  8966,  8920,  9412,  8159, 12510,  8159,  8267,  8203,  8468,\n",
      "          8131,  8203,  8135,   102, 10975,  8966,  8839,  8203,  8468,  8133,\n",
      "          9845,  8158, 11023,  8152, 11758,  8161,  8132,  8419,  8203,  9621,\n",
      "          9621,  8428,  8144,  9564,  8144,   102,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  8110,  8131, 12721, 12225, 11542,  8129,  8121,   102,  9801,\n",
      "          8110,  8131, 12721, 12225,  9022,  8121,   102,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0]]), tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]]), tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])]\n",
      "tensor([1, 1, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "for x in train_loader:\n",
    "    print(x[:-1])\n",
    "    print(x[-1])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = x[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  8356,  8160,  8162, 10658, 12404, 12570, 11173,   102,  8246,\n",
       "          8160, 12873,  8162, 10658, 10541,   102,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0],\n",
       "        [  101,  8162, 10345,  8148,  9389,  8737,  8108,  8428,  8159,  8111,\n",
       "           102,  8162, 10924,  8352,  8544,   102,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0],\n",
       "        [  101,  8966,  8920,  9412,  8159, 12510,  8159,  8267,  8203,  8468,\n",
       "          8131,  8203,  8135,   102, 10975,  8966,  8839,  8203,  8468,  8133,\n",
       "          9845,  8158, 11023,  8152, 11758,  8161,  8132,  8419,  8203,  9621,\n",
       "          9621,  8428,  8144,  9564,  8144,   102,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0],\n",
       "        [  101,  8110,  8131, 12721, 12225, 11542,  8129,  8121,   102,  9801,\n",
       "          8110,  8131, 12721, 12225,  9022,  8121,   102,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]])"
      ]
     },
     "execution_count": 593,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.rand(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.66778585, 0.04161166, 0.86152516],\n",
       "       [0.414541  , 0.90704763, 0.31789615]])"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1], dtype=int64)"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(a, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_new_traindata(test_score, train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [],
   "source": [
    "class aa:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.a = 1\n",
    "        self.change()\n",
    "    \n",
    "    def change(self):\n",
    "        self.b = 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = aa()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 598,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_input:\n",
      " tensor([[-0.6854,  0.6218,  0.7670],\n",
      "        [-0.8204, -0.2649, -1.2816],\n",
      "        [ 0.3827,  0.5260, -0.1332]])\n",
      "soft_output:\n",
      " tensor([[0.1115, 0.4121, 0.4764],\n",
      "        [0.2964, 0.5166, 0.1869],\n",
      "        [0.3635, 0.4195, 0.2170]])\n",
      "log_output:\n",
      " tensor([[-2.1937, -0.8866, -0.7414],\n",
      "        [-1.2159, -0.6604, -1.6771],\n",
      "        [-1.0120, -0.8687, -1.5278]])\n",
      "logsoftmax_output:\n",
      " tensor([[-2.1937, -0.8866, -0.7414],\n",
      "        [-1.2159, -0.6604, -1.6771],\n",
      "        [-1.0120, -0.8687, -1.5278]])\n",
      "nlloss_output:\n",
      " tensor(1.1919)\n",
      "crossentropyloss_output:\n",
      " tensor(1.3069)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "x_input=torch.randn(3,3)#随机生成输入 \n",
    "print('x_input:\\n',x_input) \n",
    "y_target=torch.tensor([1,2,0])#设置输出具体值 print('y_target\\n',y_target)\n",
    "\n",
    "#计算输入softmax，此时可以看到每一行加到一起结果都是1\n",
    "softmax_func=nn.Softmax(dim=1)\n",
    "soft_output=softmax_func(x_input)\n",
    "print('soft_output:\\n',soft_output)\n",
    "\n",
    "#在softmax的基础上取log\n",
    "log_output=torch.log(soft_output)\n",
    "print('log_output:\\n',log_output)\n",
    "\n",
    "#对比softmax与log的结合与nn.LogSoftmaxloss(负对数似然损失)的输出结果，发现两者是一致的。\n",
    "logsoftmax_func=nn.LogSoftmax(dim=1)\n",
    "logsoftmax_output=logsoftmax_func(x_input)\n",
    "print('logsoftmax_output:\\n',logsoftmax_output)\n",
    "\n",
    "#pytorch中关于NLLLoss的默认参数配置为：reducetion=True、size_average=True\n",
    "nllloss_func=nn.NLLLoss()\n",
    "nlloss_output=nllloss_func(logsoftmax_output,y_target)\n",
    "print('nlloss_output:\\n',nlloss_output)\n",
    "\n",
    "#直接使用pytorch中的loss_func=nn.CrossEntropyLoss()看与经过NLLLoss的计算是不是一样\n",
    "crossentropyloss=nn.CrossEntropyLoss(weight = torch.tensor([0.4, 0.6, 1.0]))\n",
    "crossentropyloss_output=crossentropyloss(x_input,y_target)\n",
    "print('crossentropyloss_output:\\n',crossentropyloss_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************************classification_report******************************\n"
     ]
    }
   ],
   "source": [
    "print(f\"{'classification_report':*^80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "    \"it is a good day, I like to stay here\",\n",
    "    \"I am happy to be here\",\n",
    "    \"I am bob\",\n",
    "    \"it is sunny today\",\n",
    "    \"I have a party today\",\n",
    "    \"it is a dog and that is a cat\",\n",
    "    \"there are dog and cat on the tree\",\n",
    "    \"I study hard this morning\",\n",
    "    \"today is a good day\",\n",
    "    \"tomorrow will be a good day\",\n",
    "    \"I like coffee, I like book and I like apple\",\n",
    "    \"I do not like it\",\n",
    "    \"I am kitty, I like bob\",\n",
    "    \"I do not care who like bob, but I like kitty\",\n",
    "    \"It is coffee time, bring your cup\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = vectorizer.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<15x44 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 77 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idf:  [('am', 2.386294361119891), ('and', 2.386294361119891), ('apple', 3.0794415416798357), ('are', 3.0794415416798357), ('be', 2.6739764335716716), ('bob', 2.386294361119891), ('book', 3.0794415416798357), ('bring', 3.0794415416798357), ('but', 3.0794415416798357), ('care', 3.0794415416798357), ('cat', 2.6739764335716716), ('coffee', 2.6739764335716716), ('cup', 3.0794415416798357), ('day', 2.386294361119891), ('do', 2.6739764335716716), ('dog', 2.6739764335716716), ('good', 2.386294361119891), ('happy', 3.0794415416798357), ('hard', 3.0794415416798357), ('have', 3.0794415416798357), ('here', 2.6739764335716716), ('is', 1.9808292530117262), ('it', 1.9808292530117262), ('kitty', 2.6739764335716716), ('like', 1.9808292530117262), ('morning', 3.0794415416798357), ('not', 2.6739764335716716), ('on', 3.0794415416798357), ('party', 3.0794415416798357), ('stay', 3.0794415416798357), ('study', 3.0794415416798357), ('sunny', 3.0794415416798357), ('that', 3.0794415416798357), ('the', 3.0794415416798357), ('there', 3.0794415416798357), ('this', 3.0794415416798357), ('time', 3.0794415416798357), ('to', 2.6739764335716716), ('today', 2.386294361119891), ('tomorrow', 3.0794415416798357), ('tree', 3.0794415416798357), ('who', 3.0794415416798357), ('will', 3.0794415416798357), ('your', 3.0794415416798357)]\n"
     ]
    }
   ],
   "source": [
    "print(\"idf: \", [(n, idf) for idf, n in zip(vectorizer.idf_, vectorizer.get_feature_names())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorizer.idf_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v2i:  {'it': 22, 'is': 21, 'good': 16, 'day': 13, 'like': 24, 'to': 37, 'stay': 29, 'here': 20, 'am': 0, 'happy': 17, 'be': 4, 'bob': 5, 'sunny': 31, 'today': 38, 'have': 19, 'party': 28, 'dog': 15, 'and': 1, 'that': 32, 'cat': 10, 'there': 34, 'are': 3, 'on': 27, 'the': 33, 'tree': 40, 'study': 30, 'hard': 18, 'this': 35, 'morning': 25, 'tomorrow': 39, 'will': 42, 'coffee': 11, 'book': 6, 'apple': 2, 'do': 14, 'not': 26, 'kitty': 23, 'care': 9, 'who': 41, 'but': 8, 'time': 36, 'bring': 7, 'your': 43, 'cup': 12}\n"
     ]
    }
   ],
   "source": [
    "print(\"v2i: \", vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "top 3 docs for 'I get a coffee cup':\n",
      "['It is coffee time, bring your cup', 'I like coffee, I like book and I like apple', 'I do not care who like bob, but I like kitty']\n",
      "\n",
      "top 3 docs for 'I get a banana':\n",
      "['It is coffee time, bring your cup', 'I do not care who like bob, but I like kitty', 'I am kitty, I like bob']\n"
     ]
    }
   ],
   "source": [
    "q_words = [\"I get a coffee cup\", \"I get a banana\"]\n",
    "for q in q_words:\n",
    "    qtf_idf = vectorizer.transform([q])\n",
    "    res = cosine_similarity(tf_idf, qtf_idf)\n",
    "    res = res.ravel().argsort()[-3:]\n",
    "    print(\"\\ntop 3 docs for '{}':\\n{}\".format(q, [docs[i] for i in res[::-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.21398863],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.56058105]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = res.ravel().argsort()[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13, 10, 14], dtype=int64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<15x44 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 77 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"./tianchi_datasets/track3_round1_train.tsv\", sep=\"\\t\", header=None,\n",
    "                                  quoting=3, encoding=\"utf-8\", names=[\"sentence1\", \"sentence2\", \"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1], dtype=int64), array([63564, 36436], dtype=int64))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(train_data[\"labels\"], return_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46875"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.75/(0.85 + 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.53125"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - 0.46875"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3333333333333333"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1764705882352942"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/0.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1271186440677967"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1.33 / 1.18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1333333333333333"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.85/0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1: 0.75 0:0.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.38629436, 2.38629436, 3.07944154, 3.07944154, 2.67397643,\n",
       "       2.38629436, 3.07944154, 3.07944154, 3.07944154, 3.07944154,\n",
       "       2.67397643, 2.67397643, 3.07944154, 2.38629436, 2.67397643,\n",
       "       2.67397643, 2.38629436, 3.07944154, 3.07944154, 3.07944154,\n",
       "       2.67397643, 1.98082925, 1.98082925, 2.67397643, 1.98082925,\n",
       "       3.07944154, 2.67397643, 3.07944154, 3.07944154, 3.07944154,\n",
       "       3.07944154, 3.07944154, 3.07944154, 3.07944154, 3.07944154,\n",
       "       3.07944154, 3.07944154, 2.67397643, 2.38629436, 3.07944154,\n",
       "       3.07944154, 3.07944154, 3.07944154, 3.07944154])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import itertools\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "train_data = pd.read_csv(\"./tianchi_datasets/track3_round1_train.tsv\", sep=\"\\t\", header=None,nrows = 100,\n",
    "                                  quoting=3, encoding=\"utf-8\", names=[\"sentence1\", \"sentence2\", \"labels\"])\n",
    "train_data[\"document\"] = train_data[\"sentence1\"].str.cat(train_data[\"sentence2\"], sep=\" \")\n",
    "test_data = pd.read_csv(\"./tianchi_datasets/track3_round1_testA.tsv\", sep=\"\\t\", header=None, nrows = 100,\n",
    "                                 quoting=3, encoding=\"utf-8\", names=[\"sentence1\", \"sentence2\"])\n",
    "test_data[\"document\"] = test_data[\"sentence1\"].str.cat(test_data[\"sentence2\"], sep=\" \")\n",
    "\n",
    "\n",
    "docs = []\n",
    "q_words = []\n",
    "for word1, word2 in itertools.zip_longest(train_data[\"document\"], test_data[\"document\"]):\n",
    "    docs.append(word1)\n",
    "    if word2 is None:\n",
    "        continue\n",
    "    q_words.append(word2)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tf_idf = vectorizer.fit_transform(docs)\n",
    "\n",
    "tmp = []\n",
    "for q in q_words:\n",
    "    qtf_idf = vectorizer.transform([q])\n",
    "    res = cosine_similarity(tf_idf, qtf_idf).ravel()\n",
    "    score = max(res)\n",
    "    idx = np.argmax(res)\n",
    "    if score > 0.7:\n",
    "        label = train_data[\"labels\"][idx]\n",
    "    else:\n",
    "        label = 2\n",
    "    tmp.append(label)\n",
    "    \n",
    "test_data[\"labels\"] = tmp\n",
    "test_data1 = test_data[test_data[\"labels\"] < 2]\n",
    "new_data = pd.merge(test_data1, train_data, how = \"outer\")\n",
    "new_data.to_csv(\"./tianchi_datasets/track3_round1_newtrain1.tsv\", sep=\"\\t\", header=None, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[\"labels\"] = tmp\n",
    "test_data1 = test_data[test_data[\"labels\"] < 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = pd.merge(test_data1, train_data, how = \"outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data.to_csv(\"./tianchi_datasets/track3_round1_newtrain1.tsv\", sep=\"\\t\", header=None, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "test_score = \"tianchi_datasets/test_03_07.json\"\n",
    "train_data = \"tianchi_datasets/track3_round1_train.tsv\"\n",
    "test_data = \"tianchi_datasets/track3_round1_testA.tsv\"\n",
    "def create_new_traindata(test_score, train_data, test_data):\n",
    "    tmp = []\n",
    "    dir_path = os.getcwd()\n",
    "\n",
    "    with open(os.path.join(dir_path, test_score), \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            content = json.loads(line)\n",
    "            score = content[\"label\"]\n",
    "            idx = content[\"id\"]\n",
    "            if score > 0.95:\n",
    "                tmp.append([idx, 1])\n",
    "            elif score < 0.05:\n",
    "                tmp.append([idx, 0])\n",
    "            else:\n",
    "                tmp.append([idx, \"unk\"])\n",
    "\n",
    "    with open(os.path.join(dir_path, test_data), \"r\", encoding=\"utf-8\") as fr:\n",
    "        with open(\"tianchi_datasets/track3_round1_testA_label1.tsv\", \"w\", encoding=\"utf-8\") as fw:\n",
    "            data = fr.readlines()\n",
    "            for i in range(len(data)):\n",
    "                content = data[i].strip()\n",
    "                label = tmp[i][1]\n",
    "                idx = tmp[i][0]\n",
    "                if label in [0, 1]:\n",
    "                    lis = content + \"\\t\" + str(label) + \"\\n\"\n",
    "                else:\n",
    "                    lis = \"\"\n",
    "                fw.write(lis)\n",
    "\n",
    "    train = pd.read_csv(os.path.join(dir_path, train_data), sep=\"\\t\", header=None,\n",
    "                             names=[\"sentence1\", \"sentence2\", \"labels\"])\n",
    "    test = pd.read_csv(\"./tianchi_datasets/track3_round1_testA_label1.tsv\", sep=\"\\t\", header=None,\n",
    "                            names=[\"sentence1\", \"sentence2\", \"labels\"])\n",
    "\n",
    "    new_data = pd.concat([train, test], axis=0)\n",
    "    new_data.to_csv(\"./tianchi_datasets/track3_round1_newtrain1.tsv\", sep=\"\\t\", header=None, index=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_new_traindata(test_score, train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 604.37it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import itertools\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "train_data = pd.read_csv(\"./tianchi_datasets/track3_round1_train.tsv\", sep=\"\\t\", header=None,\n",
    "                         quoting=3, encoding=\"utf-8\", names=[\"sentence1\", \"sentence2\", \"labels\"])\n",
    "train_data[\"document\"] = train_data[\"sentence1\"].str.cat(train_data[\"sentence2\"], sep=\" \")\n",
    "test_data = pd.read_csv(\"./tianchi_datasets/track3_round1_testA.tsv\", sep=\"\\t\", header=None, \n",
    "                        quoting=3, encoding=\"utf-8\", names=[\"sentence1\", \"sentence2\"])\n",
    "test_data[\"document\"] = test_data[\"sentence1\"].str.cat(test_data[\"sentence2\"], sep=\" \")\n",
    "\n",
    "docs = []\n",
    "q_words = []\n",
    "for word1, word2 in itertools.zip_longest(train_data[\"document\"], test_data[\"document\"]):\n",
    "    docs.append(word1)\n",
    "    if word2 is None:\n",
    "        continue\n",
    "    q_words.append(word2)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tf_idf = vectorizer.fit_transform(docs)\n",
    "\n",
    "tmp = []\n",
    "for q in tqdm(q_words):\n",
    "    qtf_idf = vectorizer.transform([q])\n",
    "    res = cosine_similarity(tf_idf, qtf_idf).ravel()\n",
    "    score = max(res)\n",
    "    idx = np.argmax(res)\n",
    "    if score > 0.7:\n",
    "        label = train_data[\"labels\"][idx]\n",
    "    else:\n",
    "        label = 2\n",
    "    tmp.append(label)\n",
    "\n",
    "test_data[\"labels\"] = tmp\n",
    "test_data1 = test_data[test_data[\"labels\"] < 2]\n",
    "new_data = pd.concat([train_data, test_data1], axis = 0)\n",
    "new_data = new_data[[\"sentence1\", \"sentence2\", \"labels\"]]\n",
    "new_data.to_csv(\"./tianchi_datasets/track3_round1_newtrain3.tsv\", sep=\"\\t\", header=None, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = new_data[[\"sentence1\", \"sentence2\", \"labels\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data.to_csv(\"./tianchi_datasets/track3_round1_newtrain3.tsv\", sep=\"\\t\", header=None, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.read_csv(\"./tianchi_datasets/track3_round1_newtrain1.tsv\", sep=\"\\t\", header=None,\n",
    "                         quoting=3, encoding=\"utf-8\", names=[\"sentence1\", \"sentence2\", \"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pd.read_csv(\"./tianchi_datasets/track3_round1_newtrain2.tsv\", sep=\"\\t\", header=None,\n",
    "                         quoting=3, encoding=\"utf-8\", names=[\"sentence1\", \"sentence2\", \"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = pd.merge(data1, data2, how = \"outer\")\n",
    "new_data.to_csv(\"./tianchi_datasets/track3_round1_newtrain3.tsv\", sep=\"\\t\", header=None, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1 2 3 4 5 6 7</td>\n",
       "      <td>8 9 10 4 11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12 13 14 15</td>\n",
       "      <td>12 15 11 16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17 18 12 19 20 21 22 23 24</td>\n",
       "      <td>12 23 25 6 26 27 19</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28 29 30 31 11</td>\n",
       "      <td>32 33 34 30 31</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29 35 36 29</td>\n",
       "      <td>29 37 36 29</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121100</th>\n",
       "      <td>282 283 283 282 158 228 58 180</td>\n",
       "      <td>282 283 283 282 158 228 1645 56 58 180</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121101</th>\n",
       "      <td>12 19 6788 23 24</td>\n",
       "      <td>12 19 307 3151 23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121102</th>\n",
       "      <td>72 29 146 146</td>\n",
       "      <td>176 29 227 134 146 146</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121103</th>\n",
       "      <td>12 251 6277 6278 16</td>\n",
       "      <td>12 251 11350</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121104</th>\n",
       "      <td>282 283 283 282 140 632 329 330 1105</td>\n",
       "      <td>329 330 2418 282 283 283 282 1829</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>121105 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   sentence1  \\\n",
       "0                              1 2 3 4 5 6 7   \n",
       "1                                12 13 14 15   \n",
       "2                 17 18 12 19 20 21 22 23 24   \n",
       "3                             28 29 30 31 11   \n",
       "4                                29 35 36 29   \n",
       "...                                      ...   \n",
       "121100        282 283 283 282 158 228 58 180   \n",
       "121101                      12 19 6788 23 24   \n",
       "121102                         72 29 146 146   \n",
       "121103                   12 251 6277 6278 16   \n",
       "121104  282 283 283 282 140 632 329 330 1105   \n",
       "\n",
       "                                     sentence2  labels  \n",
       "0                                  8 9 10 4 11       0  \n",
       "1                                  12 15 11 16       0  \n",
       "2                          12 23 25 6 26 27 19       1  \n",
       "3                               32 33 34 30 31       1  \n",
       "4                                  29 37 36 29       1  \n",
       "...                                        ...     ...  \n",
       "121100  282 283 283 282 158 228 1645 56 58 180       0  \n",
       "121101                       12 19 307 3151 23       0  \n",
       "121102                  176 29 227 134 146 146       1  \n",
       "121103                            12 251 11350       1  \n",
       "121104       329 330 2418 282 283 283 282 1829       1  \n",
       "\n",
       "[121105 rows x 3 columns]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1 2 3 4 5 6 7</td>\n",
       "      <td>8 9 10 4 11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12 13 14 15</td>\n",
       "      <td>12 15 11 16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17 18 12 19 20 21 22 23 24</td>\n",
       "      <td>12 23 25 6 26 27 19</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28 29 30 31 11</td>\n",
       "      <td>32 33 34 30 31</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29 35 36 29</td>\n",
       "      <td>29 37 36 29</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120894</th>\n",
       "      <td>4915 23 39 9 3544 19</td>\n",
       "      <td>158 478 426 19 1274 120 212 1378 157</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120895</th>\n",
       "      <td>729 192 169 718 169 718 2061</td>\n",
       "      <td>20253 416 11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120896</th>\n",
       "      <td>29 81 140 72 12 5 1763 1764 12 459 16</td>\n",
       "      <td>1553 72 12 20811 6722 16</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120897</th>\n",
       "      <td>740 2155 6667</td>\n",
       "      <td>20273 762 852 11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120898</th>\n",
       "      <td>12 317 3434 16</td>\n",
       "      <td>134 853 12 317 1596</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120899 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    sentence1  \\\n",
       "0                               1 2 3 4 5 6 7   \n",
       "1                                 12 13 14 15   \n",
       "2                  17 18 12 19 20 21 22 23 24   \n",
       "3                              28 29 30 31 11   \n",
       "4                                 29 35 36 29   \n",
       "...                                       ...   \n",
       "120894                   4915 23 39 9 3544 19   \n",
       "120895           729 192 169 718 169 718 2061   \n",
       "120896  29 81 140 72 12 5 1763 1764 12 459 16   \n",
       "120897                          740 2155 6667   \n",
       "120898                         12 317 3434 16   \n",
       "\n",
       "                                   sentence2  labels  \n",
       "0                                8 9 10 4 11       0  \n",
       "1                                12 15 11 16       0  \n",
       "2                        12 23 25 6 26 27 19       1  \n",
       "3                             32 33 34 30 31       1  \n",
       "4                                29 37 36 29       1  \n",
       "...                                      ...     ...  \n",
       "120894  158 478 426 19 1274 120 212 1378 157       0  \n",
       "120895                          20253 416 11       0  \n",
       "120896              1553 72 12 20811 6722 16       1  \n",
       "120897                      20273 762 852 11       0  \n",
       "120898                   134 853 12 317 1596       1  \n",
       "\n",
       "[120899 rows x 3 columns]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1 2 3 4 5 6 7</td>\n",
       "      <td>8 9 10 4 11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12 13 14 15</td>\n",
       "      <td>12 15 11 16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17 18 12 19 20 21 22 23 24</td>\n",
       "      <td>12 23 25 6 26 27 19</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28 29 30 31 11</td>\n",
       "      <td>32 33 34 30 31</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29 35 36 29</td>\n",
       "      <td>29 37 36 29</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100768</th>\n",
       "      <td>28 12 317 43 533</td>\n",
       "      <td>12 44 253 317 66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100769</th>\n",
       "      <td>2692 230 507 20516</td>\n",
       "      <td>12 168 230 507 16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100770</th>\n",
       "      <td>12 698 10 23 850 1575 315</td>\n",
       "      <td>12 698 850 1575 315</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100771</th>\n",
       "      <td>522 2746 1277 1277 48 336</td>\n",
       "      <td>169 1192 522 6 1277 1277 48 336</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100772</th>\n",
       "      <td>12 317 3434 16</td>\n",
       "      <td>134 853 12 317 1596</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100773 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         sentence1                        sentence2  labels\n",
       "0                    1 2 3 4 5 6 7                      8 9 10 4 11       0\n",
       "1                      12 13 14 15                      12 15 11 16       0\n",
       "2       17 18 12 19 20 21 22 23 24              12 23 25 6 26 27 19       1\n",
       "3                   28 29 30 31 11                   32 33 34 30 31       1\n",
       "4                      29 35 36 29                      29 37 36 29       1\n",
       "...                            ...                              ...     ...\n",
       "100768            28 12 317 43 533                 12 44 253 317 66       0\n",
       "100769          2692 230 507 20516                12 168 230 507 16       0\n",
       "100770   12 698 10 23 850 1575 315              12 698 850 1575 315       1\n",
       "100771   522 2746 1277 1277 48 336  169 1192 522 6 1277 1277 48 336       1\n",
       "100772              12 317 3434 16              134 853 12 317 1596       1\n",
       "\n",
       "[100773 rows x 3 columns]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1 2 3 4 5 6 7</td>\n",
       "      <td>8 9 10 4 11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12 13 14 15</td>\n",
       "      <td>12 15 11 16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17 18 12 19 20 21 22 23 24</td>\n",
       "      <td>12 23 25 6 26 27 19</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28 29 30 31 11</td>\n",
       "      <td>32 33 34 30 31</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29 35 36 29</td>\n",
       "      <td>29 37 36 29</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100562</th>\n",
       "      <td>28 12 317 43 533</td>\n",
       "      <td>12 44 253 317 66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100563</th>\n",
       "      <td>2692 230 507 20516</td>\n",
       "      <td>12 168 230 507 16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100564</th>\n",
       "      <td>12 698 10 23 850 1575 315</td>\n",
       "      <td>12 698 850 1575 315</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100565</th>\n",
       "      <td>522 2746 1277 1277 48 336</td>\n",
       "      <td>169 1192 522 6 1277 1277 48 336</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100566</th>\n",
       "      <td>12 317 3434 16</td>\n",
       "      <td>134 853 12 317 1596</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100567 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         sentence1                        sentence2  labels\n",
       "0                    1 2 3 4 5 6 7                      8 9 10 4 11       0\n",
       "1                      12 13 14 15                      12 15 11 16       0\n",
       "2       17 18 12 19 20 21 22 23 24              12 23 25 6 26 27 19       1\n",
       "3                   28 29 30 31 11                   32 33 34 30 31       1\n",
       "4                      29 35 36 29                      29 37 36 29       1\n",
       "...                            ...                              ...     ...\n",
       "100562            28 12 317 43 533                 12 44 253 317 66       0\n",
       "100563          2692 230 507 20516                12 168 230 507 16       0\n",
       "100564   12 698 10 23 850 1575 315              12 698 850 1575 315       1\n",
       "100565   522 2746 1277 1277 48 336  169 1192 522 6 1277 1277 48 336       1\n",
       "100566              12 317 3434 16              134 853 12 317 1596       1\n",
       "\n",
       "[100567 rows x 3 columns]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.merge(data1, data2, how = \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1 2 3 4 5 6 7</td>\n",
       "      <td>8 9 10 4 11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12 13 14 15</td>\n",
       "      <td>12 15 11 16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17 18 12 19 20 21 22 23 24</td>\n",
       "      <td>12 23 25 6 26 27 19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28 29 30 31 11</td>\n",
       "      <td>32 33 34 30 31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29 35 36 29</td>\n",
       "      <td>29 37 36 29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100667</th>\n",
       "      <td>28 12 317 43 533</td>\n",
       "      <td>12 44 253 317 66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100668</th>\n",
       "      <td>2692 230 507 20516</td>\n",
       "      <td>12 168 230 507 16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100669</th>\n",
       "      <td>12 698 10 23 850 1575 315</td>\n",
       "      <td>12 698 850 1575 315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100670</th>\n",
       "      <td>522 2746 1277 1277 48 336</td>\n",
       "      <td>169 1192 522 6 1277 1277 48 336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100671</th>\n",
       "      <td>12 317 3434 16</td>\n",
       "      <td>134 853 12 317 1596</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100672 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         sentence1                        sentence2\n",
       "0                    1 2 3 4 5 6 7                      8 9 10 4 11\n",
       "1                      12 13 14 15                      12 15 11 16\n",
       "2       17 18 12 19 20 21 22 23 24              12 23 25 6 26 27 19\n",
       "3                   28 29 30 31 11                   32 33 34 30 31\n",
       "4                      29 35 36 29                      29 37 36 29\n",
       "...                            ...                              ...\n",
       "100667            28 12 317 43 533                 12 44 253 317 66\n",
       "100668          2692 230 507 20516                12 168 230 507 16\n",
       "100669   12 698 10 23 850 1575 315              12 698 850 1575 315\n",
       "100670   522 2746 1277 1277 48 336  169 1192 522 6 1277 1277 48 336\n",
       "100671              12 317 3434 16              134 853 12 317 1596\n",
       "\n",
       "[100672 rows x 2 columns]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = pd.merge(data1[[\"sentence1\", \"sentence2\"]], data2[[\"sentence1\", \"sentence2\"]], how = \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "import TEXTCNN_Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "spec not found for the module 'TEXTCNN_Pytorch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-194-7583af63726b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mreload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTEXTCNN_Pytorch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32md:\\anaconda\\envs\\gluon1\\lib\\imp.py\u001b[0m in \u001b[0;36mreload\u001b[1;34m(module)\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m     \"\"\"\n\u001b[1;32m--> 315\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    316\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda\\envs\\gluon1\\lib\\importlib\\__init__.py\u001b[0m in \u001b[0;36mreload\u001b[1;34m(module)\u001b[0m\n\u001b[0;32m    166\u001b[0m         \u001b[0mspec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__spec__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_find_spec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpkgpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mspec\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 168\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mModuleNotFoundError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"spec not found for the module {name!r}\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    169\u001b[0m         \u001b[0m_bootstrap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m         \u001b[1;31m# The module may have replaced itself in sys.modules!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: spec not found for the module 'TEXTCNN_Pytorch'"
     ]
    }
   ],
   "source": [
    "reload(TEXTCNN_Pytorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TEXTCNN_Pytorch import TEXTCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Data_generator_vocab import Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "seed_val = 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************Data_precessing*********************************\n",
      "## dataset size is 121105\n",
      "*******************************Load  dataset ....*******************************\n",
      "## Load Datasets Consume 0:00:06 s ###\n",
      "************************All Train and Test Data loaded !************************\n"
     ]
    }
   ],
   "source": [
    "corpus = Corpus(batch_size, seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'Data_generator_vocab' from 'E:\\\\lv_python\\\\NLP\\\\阿里天池小布助手\\\\Data_generator_vocab.py'>"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imp import reload\n",
    "import Data_generator_vocab\n",
    "reload(Data_generator_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '[PAD]',\n",
       " 1: '[CLS]',\n",
       " 2: '[UNK]',\n",
       " 3: '[SEP]',\n",
       " 4: '12',\n",
       " 5: '29',\n",
       " 6: '19',\n",
       " 7: '23',\n",
       " 8: '16',\n",
       " 9: '11',\n",
       " 10: '10',\n",
       " 11: '9',\n",
       " 12: '32',\n",
       " 13: '39',\n",
       " 14: '161',\n",
       " 15: '126',\n",
       " 16: '48',\n",
       " 17: '243',\n",
       " 18: '43',\n",
       " 19: '106',\n",
       " 20: '5',\n",
       " 21: '6',\n",
       " 22: '72',\n",
       " 23: '66',\n",
       " 24: '8',\n",
       " 25: '13',\n",
       " 26: '283',\n",
       " 27: '282',\n",
       " 28: '459',\n",
       " 29: '28',\n",
       " 30: '370',\n",
       " 31: '317',\n",
       " 32: '62',\n",
       " 33: '76',\n",
       " 34: '247',\n",
       " 35: '140',\n",
       " 36: '276',\n",
       " 37: '50',\n",
       " 38: '46',\n",
       " 39: '169',\n",
       " 40: '80',\n",
       " 41: '300',\n",
       " 42: '47',\n",
       " 43: '202',\n",
       " 44: '59',\n",
       " 45: '217',\n",
       " 46: '431',\n",
       " 47: '45',\n",
       " 48: '127',\n",
       " 49: '134',\n",
       " 50: '426',\n",
       " 51: '536',\n",
       " 52: '67',\n",
       " 53: '447',\n",
       " 54: '230',\n",
       " 55: '24',\n",
       " 56: '133',\n",
       " 57: '522',\n",
       " 58: '52',\n",
       " 59: '533',\n",
       " 60: '432',\n",
       " 61: '227',\n",
       " 62: '251',\n",
       " 63: '226',\n",
       " 64: '253',\n",
       " 65: '453',\n",
       " 66: '168',\n",
       " 67: '358',\n",
       " 68: '629',\n",
       " 69: '25',\n",
       " 70: '272',\n",
       " 71: '176',\n",
       " 72: '267',\n",
       " 73: '102',\n",
       " 74: '55',\n",
       " 75: '462',\n",
       " 76: '153',\n",
       " 77: '79',\n",
       " 78: '421',\n",
       " 79: '173',\n",
       " 80: '130',\n",
       " 81: '334',\n",
       " 82: '347',\n",
       " 83: '101',\n",
       " 84: '524',\n",
       " 85: '525',\n",
       " 86: '335',\n",
       " 87: '44',\n",
       " 88: '360',\n",
       " 89: '200',\n",
       " 90: '592',\n",
       " 91: '647',\n",
       " 92: '163',\n",
       " 93: '53',\n",
       " 94: '485',\n",
       " 95: '107',\n",
       " 96: '254',\n",
       " 97: '246',\n",
       " 98: '128',\n",
       " 99: '433',\n",
       " 100: '574',\n",
       " 101: '304',\n",
       " 102: '142',\n",
       " 103: '478',\n",
       " 104: '344',\n",
       " 105: '146',\n",
       " 106: '70',\n",
       " 107: '518',\n",
       " 108: '275',\n",
       " 109: '58',\n",
       " 110: '180',\n",
       " 111: '725',\n",
       " 112: '229',\n",
       " 113: '177',\n",
       " 114: '122',\n",
       " 115: '451',\n",
       " 116: '248',\n",
       " 117: '312',\n",
       " 118: '263',\n",
       " 119: '882',\n",
       " 120: '56',\n",
       " 121: '232',\n",
       " 122: '212',\n",
       " 123: '114',\n",
       " 124: '444',\n",
       " 125: '68',\n",
       " 126: '470',\n",
       " 127: '1006',\n",
       " 128: '481',\n",
       " 129: '782',\n",
       " 130: '118',\n",
       " 131: '457',\n",
       " 132: '213',\n",
       " 133: '1017',\n",
       " 134: '115',\n",
       " 135: '249',\n",
       " 136: '4',\n",
       " 137: '355',\n",
       " 138: '90',\n",
       " 139: '837',\n",
       " 140: '35',\n",
       " 141: '192',\n",
       " 142: '33',\n",
       " 143: '307',\n",
       " 144: '379',\n",
       " 145: '49',\n",
       " 146: '456',\n",
       " 147: '108',\n",
       " 148: '162',\n",
       " 149: '415',\n",
       " 150: '158',\n",
       " 151: '193',\n",
       " 152: '800',\n",
       " 153: '348',\n",
       " 154: '342',\n",
       " 155: '422',\n",
       " 156: '455',\n",
       " 157: '2',\n",
       " 158: '159',\n",
       " 159: '416',\n",
       " 160: '116',\n",
       " 161: '74',\n",
       " 162: '65',\n",
       " 163: '541',\n",
       " 164: '83',\n",
       " 165: '172',\n",
       " 166: '903',\n",
       " 167: '120',\n",
       " 168: '87',\n",
       " 169: '718',\n",
       " 170: '296',\n",
       " 171: '215',\n",
       " 172: '454',\n",
       " 173: '472',\n",
       " 174: '252',\n",
       " 175: '208',\n",
       " 176: '941',\n",
       " 177: '698',\n",
       " 178: '34',\n",
       " 179: '3',\n",
       " 180: '565',\n",
       " 181: '223',\n",
       " 182: '228',\n",
       " 183: '546',\n",
       " 184: '222',\n",
       " 185: '135',\n",
       " 186: '285',\n",
       " 187: '279',\n",
       " 188: '69',\n",
       " 189: '1188',\n",
       " 190: '773',\n",
       " 191: '781',\n",
       " 192: '394',\n",
       " 193: '794',\n",
       " 194: '313',\n",
       " 195: '314',\n",
       " 196: '277',\n",
       " 197: '785',\n",
       " 198: '606',\n",
       " 199: '595',\n",
       " 200: '336',\n",
       " 201: '42',\n",
       " 202: '350',\n",
       " 203: '14',\n",
       " 204: '264',\n",
       " 205: '354',\n",
       " 206: '922',\n",
       " 207: '121',\n",
       " 208: '618',\n",
       " 209: '266',\n",
       " 210: '92',\n",
       " 211: '1475',\n",
       " 212: '366',\n",
       " 213: '220',\n",
       " 214: '552',\n",
       " 215: '1195',\n",
       " 216: '813',\n",
       " 217: '634',\n",
       " 218: '75',\n",
       " 219: '448',\n",
       " 220: '776',\n",
       " 221: '553',\n",
       " 222: '1124',\n",
       " 223: '124',\n",
       " 224: '329',\n",
       " 225: '345',\n",
       " 226: '917',\n",
       " 227: '390',\n",
       " 228: '1642',\n",
       " 229: '935',\n",
       " 230: '221',\n",
       " 231: '535',\n",
       " 232: '471',\n",
       " 233: '392',\n",
       " 234: '406',\n",
       " 235: '771',\n",
       " 236: '274',\n",
       " 237: '117',\n",
       " 238: '311',\n",
       " 239: '82',\n",
       " 240: '521',\n",
       " 241: '442',\n",
       " 242: '511',\n",
       " 243: '353',\n",
       " 244: '1596',\n",
       " 245: '804',\n",
       " 246: '1299',\n",
       " 247: '332',\n",
       " 248: '330',\n",
       " 249: '255',\n",
       " 250: '499',\n",
       " 251: '540',\n",
       " 252: '757',\n",
       " 253: '435',\n",
       " 254: '818',\n",
       " 255: '548',\n",
       " 256: '31',\n",
       " 257: '787',\n",
       " 258: '57',\n",
       " 259: '467',\n",
       " 260: '583',\n",
       " 261: '812',\n",
       " 262: '189',\n",
       " 263: '147',\n",
       " 264: '819',\n",
       " 265: '1203',\n",
       " 266: '89',\n",
       " 267: '1012',\n",
       " 268: '1652',\n",
       " 269: '434',\n",
       " 270: '331',\n",
       " 271: '309',\n",
       " 272: '594',\n",
       " 273: '476',\n",
       " 274: '338',\n",
       " 275: '41',\n",
       " 276: '762',\n",
       " 277: '141',\n",
       " 278: '579',\n",
       " 279: '532',\n",
       " 280: '571',\n",
       " 281: '1259',\n",
       " 282: '299',\n",
       " 283: '191',\n",
       " 284: '1661',\n",
       " 285: '1053',\n",
       " 286: '1002',\n",
       " 287: '464',\n",
       " 288: '665',\n",
       " 289: '73',\n",
       " 290: '136',\n",
       " 291: '1003',\n",
       " 292: '971',\n",
       " 293: '260',\n",
       " 294: '1277',\n",
       " 295: '71',\n",
       " 296: '398',\n",
       " 297: '396',\n",
       " 298: '831',\n",
       " 299: '308',\n",
       " 300: '333',\n",
       " 301: '439',\n",
       " 302: '635',\n",
       " 303: '2319',\n",
       " 304: '918',\n",
       " 305: '495',\n",
       " 306: '361',\n",
       " 307: '561',\n",
       " 308: '1499',\n",
       " 309: '1592',\n",
       " 310: '297',\n",
       " 311: '187',\n",
       " 312: '119',\n",
       " 313: '175',\n",
       " 314: '295',\n",
       " 315: '691',\n",
       " 316: '131',\n",
       " 317: '93',\n",
       " 318: '591',\n",
       " 319: '461',\n",
       " 320: '494',\n",
       " 321: '1227',\n",
       " 322: '241',\n",
       " 323: '209',\n",
       " 324: '1644',\n",
       " 325: '328',\n",
       " 326: '1555',\n",
       " 327: '1584',\n",
       " 328: '224',\n",
       " 329: '550',\n",
       " 330: '2162',\n",
       " 331: '915',\n",
       " 332: '938',\n",
       " 333: '615',\n",
       " 334: '1178',\n",
       " 335: '250',\n",
       " 336: '965',\n",
       " 337: '473',\n",
       " 338: '964',\n",
       " 339: '734',\n",
       " 340: '575',\n",
       " 341: '424',\n",
       " 342: '240',\n",
       " 343: '657',\n",
       " 344: '854',\n",
       " 345: '643',\n",
       " 346: '538',\n",
       " 347: '678',\n",
       " 348: '1004',\n",
       " 349: '477',\n",
       " 350: '1182',\n",
       " 351: '137',\n",
       " 352: '1359',\n",
       " 353: '623',\n",
       " 354: '86',\n",
       " 355: '1051',\n",
       " 356: '440',\n",
       " 357: '933',\n",
       " 358: '692',\n",
       " 359: '138',\n",
       " 360: '1024',\n",
       " 361: '1319',\n",
       " 362: '956',\n",
       " 363: '280',\n",
       " 364: '671',\n",
       " 365: '1192',\n",
       " 366: '519',\n",
       " 367: '998',\n",
       " 368: '63',\n",
       " 369: '365',\n",
       " 370: '988',\n",
       " 371: '806',\n",
       " 372: '1430',\n",
       " 373: '705',\n",
       " 374: '1021',\n",
       " 375: '132',\n",
       " 376: '145',\n",
       " 377: '1352',\n",
       " 378: '1068',\n",
       " 379: '981',\n",
       " 380: '1864',\n",
       " 381: '889',\n",
       " 382: '94',\n",
       " 383: '380',\n",
       " 384: '1325',\n",
       " 385: '1338',\n",
       " 386: '505',\n",
       " 387: '322',\n",
       " 388: '373',\n",
       " 389: '403',\n",
       " 390: '337',\n",
       " 391: '711',\n",
       " 392: '739',\n",
       " 393: '236',\n",
       " 394: '1110',\n",
       " 395: '233',\n",
       " 396: '1645',\n",
       " 397: '641',\n",
       " 398: '1290',\n",
       " 399: '878',\n",
       " 400: '37',\n",
       " 401: '1426',\n",
       " 402: '341',\n",
       " 403: '36',\n",
       " 404: '578',\n",
       " 405: '853',\n",
       " 406: '836',\n",
       " 407: '690',\n",
       " 408: '3034',\n",
       " 409: '1162',\n",
       " 410: '382',\n",
       " 411: '1759',\n",
       " 412: '761',\n",
       " 413: '783',\n",
       " 414: '290',\n",
       " 415: '1104',\n",
       " 416: '1032',\n",
       " 417: '1593',\n",
       " 418: '829',\n",
       " 419: '646',\n",
       " 420: '675',\n",
       " 421: '1042',\n",
       " 422: '40',\n",
       " 423: '1101',\n",
       " 424: '98',\n",
       " 425: '123',\n",
       " 426: '652',\n",
       " 427: '1640',\n",
       " 428: '1236',\n",
       " 429: '1100',\n",
       " 430: '239',\n",
       " 431: '539',\n",
       " 432: '830',\n",
       " 433: '2643',\n",
       " 434: '1263',\n",
       " 435: '884',\n",
       " 436: '989',\n",
       " 437: '1038',\n",
       " 438: '1377',\n",
       " 439: '707',\n",
       " 440: '302',\n",
       " 441: '611',\n",
       " 442: '190',\n",
       " 443: '1365',\n",
       " 444: '706',\n",
       " 445: '753',\n",
       " 446: '599',\n",
       " 447: '852',\n",
       " 448: '626',\n",
       " 449: '612',\n",
       " 450: '790',\n",
       " 451: '1133',\n",
       " 452: '1130',\n",
       " 453: '875',\n",
       " 454: '1958',\n",
       " 455: '291',\n",
       " 456: '861',\n",
       " 457: '669',\n",
       " 458: '2169',\n",
       " 459: '689',\n",
       " 460: '850',\n",
       " 461: '389',\n",
       " 462: '940',\n",
       " 463: '1495',\n",
       " 464: '683',\n",
       " 465: '1619',\n",
       " 466: '2538',\n",
       " 467: '1347',\n",
       " 468: '423',\n",
       " 469: '542',\n",
       " 470: '1174',\n",
       " 471: '1326',\n",
       " 472: '1698',\n",
       " 473: '270',\n",
       " 474: '315',\n",
       " 475: '862',\n",
       " 476: '395',\n",
       " 477: '1',\n",
       " 478: '662',\n",
       " 479: '810',\n",
       " 480: '2124',\n",
       " 481: '157',\n",
       " 482: '507',\n",
       " 483: '468',\n",
       " 484: '526',\n",
       " 485: '1570',\n",
       " 486: '77',\n",
       " 487: '1425',\n",
       " 488: '1847',\n",
       " 489: '466',\n",
       " 490: '1424',\n",
       " 491: '2641',\n",
       " 492: '1209',\n",
       " 493: '1226',\n",
       " 494: '1249',\n",
       " 495: '1329',\n",
       " 496: '413',\n",
       " 497: '1680',\n",
       " 498: '920',\n",
       " 499: '1364',\n",
       " 500: '196',\n",
       " 501: '359',\n",
       " 502: '914',\n",
       " 503: '405',\n",
       " 504: '292',\n",
       " 505: '319',\n",
       " 506: '437',\n",
       " 507: '113',\n",
       " 508: '949',\n",
       " 509: '1275',\n",
       " 510: '1512',\n",
       " 511: '1239',\n",
       " 512: '265',\n",
       " 513: '990',\n",
       " 514: '1107',\n",
       " 515: '1522',\n",
       " 516: '357',\n",
       " 517: '864',\n",
       " 518: '1097',\n",
       " 519: '887',\n",
       " 520: '1153',\n",
       " 521: '1575',\n",
       " 522: '352',\n",
       " 523: '1132',\n",
       " 524: '984',\n",
       " 525: '474',\n",
       " 526: '1582',\n",
       " 527: '20',\n",
       " 528: '262',\n",
       " 529: '679',\n",
       " 530: '1311',\n",
       " 531: '601',\n",
       " 532: '178',\n",
       " 533: '593',\n",
       " 534: '719',\n",
       " 535: '799',\n",
       " 536: '1574',\n",
       " 537: '736',\n",
       " 538: '1286',\n",
       " 539: '1310',\n",
       " 540: '588',\n",
       " 541: '99',\n",
       " 542: '397',\n",
       " 543: '237',\n",
       " 544: '566',\n",
       " 545: '1635',\n",
       " 546: '1397',\n",
       " 547: '1223',\n",
       " 548: '1667',\n",
       " 549: '895',\n",
       " 550: '916',\n",
       " 551: '2260',\n",
       " 552: '1121',\n",
       " 553: '590',\n",
       " 554: '749',\n",
       " 555: '760',\n",
       " 556: '491',\n",
       " 557: '788',\n",
       " 558: '1600',\n",
       " 559: '500',\n",
       " 560: '1129',\n",
       " 561: '744',\n",
       " 562: '1403',\n",
       " 563: '653',\n",
       " 564: '909',\n",
       " 565: '808',\n",
       " 566: '823',\n",
       " 567: '2006',\n",
       " 568: '1027',\n",
       " 569: '2213',\n",
       " 570: '30',\n",
       " 571: '1059',\n",
       " 572: '1135',\n",
       " 573: '1536',\n",
       " 574: '1675',\n",
       " 575: '374',\n",
       " 576: '723',\n",
       " 577: '1274',\n",
       " 578: '877',\n",
       " 579: '921',\n",
       " 580: '493',\n",
       " 581: '1193',\n",
       " 582: '2175',\n",
       " 583: '367',\n",
       " 584: '784',\n",
       " 585: '1405',\n",
       " 586: '148',\n",
       " 587: '483',\n",
       " 588: '1228',\n",
       " 589: '261',\n",
       " 590: '310',\n",
       " 591: '1370',\n",
       " 592: '91',\n",
       " 593: '1935',\n",
       " 594: '980',\n",
       " 595: '642',\n",
       " 596: '156',\n",
       " 597: '969',\n",
       " 598: '881',\n",
       " 599: '955',\n",
       " 600: '1496',\n",
       " 601: '573',\n",
       " 602: '1758',\n",
       " 603: '144',\n",
       " 604: '817',\n",
       " 605: '225',\n",
       " 606: '1013',\n",
       " 607: '1648',\n",
       " 608: '693',\n",
       " 609: '1160',\n",
       " 610: '625',\n",
       " 611: '979',\n",
       " 612: '1903',\n",
       " 613: '281',\n",
       " 614: '1207',\n",
       " 615: '1794',\n",
       " 616: '95',\n",
       " 617: '826',\n",
       " 618: '1085',\n",
       " 619: '1610',\n",
       " 620: '369',\n",
       " 621: '1154',\n",
       " 622: '1245',\n",
       " 623: '945',\n",
       " 624: '798',\n",
       " 625: '919',\n",
       " 626: '1287',\n",
       " 627: '2483',\n",
       " 628: '1087',\n",
       " 629: '684',\n",
       " 630: '1089',\n",
       " 631: '2415',\n",
       " 632: '197',\n",
       " 633: '947',\n",
       " 634: '356',\n",
       " 635: '609',\n",
       " 636: '1141',\n",
       " 637: '2037',\n",
       " 638: '2113',\n",
       " 639: '1048',\n",
       " 640: '1928',\n",
       " 641: '1440',\n",
       " 642: '1050',\n",
       " 643: '682',\n",
       " 644: '1525',\n",
       " 645: '2436',\n",
       " 646: '105',\n",
       " 647: '2420',\n",
       " 648: '568',\n",
       " 649: '674',\n",
       " 650: '1431',\n",
       " 651: '2583',\n",
       " 652: '458',\n",
       " 653: '1446',\n",
       " 654: '534',\n",
       " 655: '2089',\n",
       " 656: '598',\n",
       " 657: '349',\n",
       " 658: '1213',\n",
       " 659: '1732',\n",
       " 660: '1493',\n",
       " 661: '1803',\n",
       " 662: '672',\n",
       " 663: '741',\n",
       " 664: '1322',\n",
       " 665: '849',\n",
       " 666: '2418',\n",
       " 667: '166',\n",
       " 668: '1361',\n",
       " 669: '1116',\n",
       " 670: '1553',\n",
       " 671: '1478',\n",
       " 672: '602',\n",
       " 673: '2636',\n",
       " 674: '2879',\n",
       " 675: '661',\n",
       " 676: '1168',\n",
       " 677: '2059',\n",
       " 678: '708',\n",
       " 679: '1268',\n",
       " 680: '7',\n",
       " 681: '904',\n",
       " 682: '2732',\n",
       " 683: '1438',\n",
       " 684: '1899',\n",
       " 685: '880',\n",
       " 686: '824',\n",
       " 687: '1094',\n",
       " 688: '1556',\n",
       " 689: '1453',\n",
       " 690: '81',\n",
       " 691: '2351',\n",
       " 692: '2421',\n",
       " 693: '1491',\n",
       " 694: '1545',\n",
       " 695: '972',\n",
       " 696: '1389',\n",
       " 697: '1934',\n",
       " 698: '2170',\n",
       " 699: '722',\n",
       " 700: '754',\n",
       " 701: '767',\n",
       " 702: '475',\n",
       " 703: '1770',\n",
       " 704: '2283',\n",
       " 705: '959',\n",
       " 706: '411',\n",
       " 707: '149',\n",
       " 708: '506',\n",
       " 709: '1001',\n",
       " 710: '1366',\n",
       " 711: '1125',\n",
       " 712: '961',\n",
       " 713: '1282',\n",
       " 714: '64',\n",
       " 715: '613',\n",
       " 716: '231',\n",
       " 717: '288',\n",
       " 718: '1786',\n",
       " 719: '603',\n",
       " 720: '1497',\n",
       " 721: '856',\n",
       " 722: '966',\n",
       " 723: '100',\n",
       " 724: '371',\n",
       " 725: '531',\n",
       " 726: '649',\n",
       " 727: '1041',\n",
       " 728: '859',\n",
       " 729: '885',\n",
       " 730: '650',\n",
       " 731: '1769',\n",
       " 732: '2208',\n",
       " 733: '517',\n",
       " 734: '1588',\n",
       " 735: '2008',\n",
       " 736: '1461',\n",
       " 737: '1622',\n",
       " 738: '1534',\n",
       " 739: '503',\n",
       " 740: '1324',\n",
       " 741: '393',\n",
       " 742: '2909',\n",
       " 743: '729',\n",
       " 744: '1092',\n",
       " 745: '155',\n",
       " 746: '1170',\n",
       " 747: '2290',\n",
       " 748: '822',\n",
       " 749: '873',\n",
       " 750: '1894',\n",
       " 751: '438',\n",
       " 752: '1760',\n",
       " 753: '1706',\n",
       " 754: '1506',\n",
       " 755: '327',\n",
       " 756: '888',\n",
       " 757: '3284',\n",
       " 758: '1568',\n",
       " 759: '502',\n",
       " 760: '1394',\n",
       " 761: '1476',\n",
       " 762: '1581',\n",
       " 763: '2210',\n",
       " 764: '1868',\n",
       " 765: '174',\n",
       " 766: '1822',\n",
       " 767: '1985',\n",
       " 768: '235',\n",
       " 769: '710',\n",
       " 770: '648',\n",
       " 771: '2066',\n",
       " 772: '2197',\n",
       " 773: '2250',\n",
       " 774: '777',\n",
       " 775: '1649',\n",
       " 776: '1235',\n",
       " 777: '1881',\n",
       " 778: '1009',\n",
       " 779: '1080',\n",
       " 780: '640',\n",
       " 781: '1479',\n",
       " 782: '3444',\n",
       " 783: '1450',\n",
       " 784: '772',\n",
       " 785: '449',\n",
       " 786: '907',\n",
       " 787: '1022',\n",
       " 788: '2434',\n",
       " 789: '301',\n",
       " 790: '408',\n",
       " 791: '1618',\n",
       " 792: '1233',\n",
       " 793: '2448',\n",
       " 794: '2041',\n",
       " 795: '97',\n",
       " 796: '554',\n",
       " 797: '1415',\n",
       " 798: '1470',\n",
       " 799: '1842',\n",
       " 800: '811',\n",
       " 801: '1401',\n",
       " 802: '1763',\n",
       " 803: '726',\n",
       " 804: '2187',\n",
       " 805: '586',\n",
       " 806: '770',\n",
       " 807: '2409',\n",
       " 808: '1139',\n",
       " 809: '1811',\n",
       " 810: '809',\n",
       " 811: '928',\n",
       " 812: '1231',\n",
       " 813: '3917',\n",
       " 814: '899',\n",
       " 815: '1206',\n",
       " 816: '1877',\n",
       " 817: '1105',\n",
       " 818: '1832',\n",
       " 819: '1993',\n",
       " 820: '1307',\n",
       " 821: '1819',\n",
       " 822: '323',\n",
       " 823: '1272',\n",
       " 824: '2460',\n",
       " 825: '991',\n",
       " 826: '1354',\n",
       " 827: '1603',\n",
       " 828: '942',\n",
       " 829: '1666',\n",
       " 830: '1723',\n",
       " 831: '1801',\n",
       " 832: '3289',\n",
       " 833: '381',\n",
       " 834: '1804',\n",
       " 835: '2023',\n",
       " 836: '986',\n",
       " 837: '3452',\n",
       " 838: '1500',\n",
       " 839: '3173',\n",
       " 840: '3381',\n",
       " 841: '479',\n",
       " 842: '559',\n",
       " 843: '614',\n",
       " 844: '497',\n",
       " 845: '2334',\n",
       " 846: '1057',\n",
       " 847: '1378',\n",
       " 848: '186',\n",
       " 849: '1134',\n",
       " 850: '2042',\n",
       " 851: '2069',\n",
       " 852: '508',\n",
       " 853: '1345',\n",
       " 854: '214',\n",
       " 855: '1267',\n",
       " 856: '482',\n",
       " 857: '1353',\n",
       " 858: '2764',\n",
       " 859: '78',\n",
       " 860: '181',\n",
       " 861: '520',\n",
       " 862: '732',\n",
       " 863: '1300',\n",
       " 864: '1918',\n",
       " 865: '22',\n",
       " 866: '644',\n",
       " 867: '1383',\n",
       " 868: '2363',\n",
       " 869: '523',\n",
       " 870: '1190',\n",
       " 871: '2235',\n",
       " 872: '384',\n",
       " 873: '963',\n",
       " 874: '428',\n",
       " 875: '412',\n",
       " 876: '1331',\n",
       " 877: '54',\n",
       " 878: '446',\n",
       " 879: '1870',\n",
       " 880: '2229',\n",
       " 881: '2198',\n",
       " 882: '1564',\n",
       " 883: '1672',\n",
       " 884: '2172',\n",
       " 885: '1276',\n",
       " 886: '1643',\n",
       " 887: '1662',\n",
       " 888: '1990',\n",
       " 889: '2945',\n",
       " 890: '109',\n",
       " 891: '3171',\n",
       " 892: '1517',\n",
       " 893: '1948',\n",
       " 894: '2746',\n",
       " 895: '962',\n",
       " 896: '1787',\n",
       " 897: '1145',\n",
       " 898: '655',\n",
       " 899: '1854',\n",
       " 900: '2575',\n",
       " 901: '1701',\n",
       " 902: '2349',\n",
       " 903: '2612',\n",
       " 904: '1411',\n",
       " 905: '1551',\n",
       " 906: '1883',\n",
       " 907: '2820',\n",
       " 908: '378',\n",
       " 909: '443',\n",
       " 910: '1323',\n",
       " 911: '840',\n",
       " 912: '1445',\n",
       " 913: '2038',\n",
       " 914: '1106',\n",
       " 915: '2000',\n",
       " 916: '194',\n",
       " 917: '486',\n",
       " 918: '608',\n",
       " 919: '1164',\n",
       " 920: '687',\n",
       " 921: '1780',\n",
       " 922: '1793',\n",
       " 923: '2214',\n",
       " 924: '3882',\n",
       " 925: '15',\n",
       " 926: '791',\n",
       " 927: '1058',\n",
       " 928: '1448',\n",
       " 929: '3331',\n",
       " 930: '866',\n",
       " 931: '1211',\n",
       " 932: '170',\n",
       " 933: '832',\n",
       " 934: '906',\n",
       " 935: '3478',\n",
       " 936: '1158',\n",
       " 937: '60',\n",
       " 938: '1421',\n",
       " 939: '2046',\n",
       " 940: '2572',\n",
       " 941: '620',\n",
       " 942: '1198',\n",
       " 943: '2328',\n",
       " 944: '512',\n",
       " 945: '567',\n",
       " 946: '967',\n",
       " 947: '1812',\n",
       " 948: '2047',\n",
       " 949: '651',\n",
       " 950: '704',\n",
       " 951: '1252',\n",
       " 952: '1693',\n",
       " 953: '2403',\n",
       " 954: '21',\n",
       " 955: '600',\n",
       " 956: '897',\n",
       " 957: '1016',\n",
       " 958: '589',\n",
       " 959: '1295',\n",
       " 960: '1798',\n",
       " 961: '2116',\n",
       " 962: '1636',\n",
       " 963: '216',\n",
       " 964: '795',\n",
       " 965: '1507',\n",
       " 966: '1650',\n",
       " 967: '2111',\n",
       " 968: '3931',\n",
       " 969: '205',\n",
       " 970: '636',\n",
       " 971: '1180',\n",
       " 972: '1219',\n",
       " 973: '2601',\n",
       " 974: '2885',\n",
       " 975: '855',\n",
       " 976: '1251',\n",
       " 977: '1396',\n",
       " 978: '1943',\n",
       " 979: '4108',\n",
       " 980: '750',\n",
       " 981: '958',\n",
       " 982: '968',\n",
       " 983: '3026',\n",
       " 984: '673',\n",
       " 985: '1613',\n",
       " 986: '1653',\n",
       " 987: '1756',\n",
       " 988: '560',\n",
       " 989: '624',\n",
       " 990: '1562',\n",
       " 991: '2471',\n",
       " 992: '2595',\n",
       " 993: '218',\n",
       " 994: '925',\n",
       " 995: '1123',\n",
       " 996: '1332',\n",
       " 997: '1654',\n",
       " 998: '219',\n",
       " 999: '926',\n",
       " ...}"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for x in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20604"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus.v2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[PAD]',\n",
       " '[CLS]',\n",
       " '[UNK]',\n",
       " '[SEP]',\n",
       " '12',\n",
       " '29',\n",
       " '19',\n",
       " '23',\n",
       " '16',\n",
       " '11',\n",
       " '10',\n",
       " '9',\n",
       " '32',\n",
       " '39',\n",
       " '161',\n",
       " '126',\n",
       " '48',\n",
       " '243',\n",
       " '43',\n",
       " '106',\n",
       " '5',\n",
       " '6',\n",
       " '72',\n",
       " '66',\n",
       " '8',\n",
       " '13',\n",
       " '283',\n",
       " '282',\n",
       " '459',\n",
       " '28',\n",
       " '370',\n",
       " '317',\n",
       " '62',\n",
       " '76',\n",
       " '247',\n",
       " '140',\n",
       " '276',\n",
       " '50',\n",
       " '46',\n",
       " '169',\n",
       " '80',\n",
       " '300',\n",
       " '47',\n",
       " '202',\n",
       " '59',\n",
       " '217',\n",
       " '431',\n",
       " '45',\n",
       " '127',\n",
       " '134',\n",
       " '426',\n",
       " '536',\n",
       " '67',\n",
       " '447',\n",
       " '230',\n",
       " '24',\n",
       " '133',\n",
       " '522',\n",
       " '52',\n",
       " '533',\n",
       " '432',\n",
       " '227',\n",
       " '251',\n",
       " '226',\n",
       " '253',\n",
       " '453',\n",
       " '168',\n",
       " '358',\n",
       " '629',\n",
       " '25',\n",
       " '272',\n",
       " '176',\n",
       " '267',\n",
       " '102',\n",
       " '55',\n",
       " '462',\n",
       " '153',\n",
       " '79',\n",
       " '421',\n",
       " '173',\n",
       " '130',\n",
       " '334',\n",
       " '347',\n",
       " '101',\n",
       " '524',\n",
       " '525',\n",
       " '335',\n",
       " '44',\n",
       " '360',\n",
       " '200',\n",
       " '592',\n",
       " '647',\n",
       " '163',\n",
       " '53',\n",
       " '485',\n",
       " '107',\n",
       " '254',\n",
       " '246',\n",
       " '128',\n",
       " '433',\n",
       " '574',\n",
       " '304',\n",
       " '142',\n",
       " '478',\n",
       " '344',\n",
       " '146',\n",
       " '70',\n",
       " '518',\n",
       " '275',\n",
       " '58',\n",
       " '180',\n",
       " '725',\n",
       " '229',\n",
       " '177',\n",
       " '122',\n",
       " '451',\n",
       " '248',\n",
       " '312',\n",
       " '263',\n",
       " '882',\n",
       " '56',\n",
       " '232',\n",
       " '212',\n",
       " '114',\n",
       " '444',\n",
       " '68',\n",
       " '470',\n",
       " '1006',\n",
       " '481',\n",
       " '782',\n",
       " '118',\n",
       " '457',\n",
       " '213',\n",
       " '1017',\n",
       " '115',\n",
       " '249',\n",
       " '4',\n",
       " '355',\n",
       " '90',\n",
       " '837',\n",
       " '35',\n",
       " '192',\n",
       " '33',\n",
       " '307',\n",
       " '379',\n",
       " '49',\n",
       " '456',\n",
       " '108',\n",
       " '162',\n",
       " '415',\n",
       " '158',\n",
       " '193',\n",
       " '800',\n",
       " '348',\n",
       " '342',\n",
       " '422',\n",
       " '455',\n",
       " '2',\n",
       " '159',\n",
       " '416',\n",
       " '116',\n",
       " '74',\n",
       " '65',\n",
       " '541',\n",
       " '83',\n",
       " '172',\n",
       " '903',\n",
       " '120',\n",
       " '87',\n",
       " '718',\n",
       " '296',\n",
       " '215',\n",
       " '454',\n",
       " '472',\n",
       " '252',\n",
       " '208',\n",
       " '941',\n",
       " '698',\n",
       " '34',\n",
       " '3',\n",
       " '565',\n",
       " '223',\n",
       " '228',\n",
       " '546',\n",
       " '222',\n",
       " '135',\n",
       " '285',\n",
       " '279',\n",
       " '69',\n",
       " '1188',\n",
       " '773',\n",
       " '781',\n",
       " '394',\n",
       " '794',\n",
       " '313',\n",
       " '314',\n",
       " '277',\n",
       " '785',\n",
       " '606',\n",
       " '595',\n",
       " '336',\n",
       " '42',\n",
       " '350',\n",
       " '14',\n",
       " '264',\n",
       " '354',\n",
       " '922',\n",
       " '121',\n",
       " '618',\n",
       " '266',\n",
       " '92',\n",
       " '1475',\n",
       " '366',\n",
       " '220',\n",
       " '552',\n",
       " '1195',\n",
       " '813',\n",
       " '634',\n",
       " '75',\n",
       " '448',\n",
       " '776',\n",
       " '553',\n",
       " '1124',\n",
       " '124',\n",
       " '329',\n",
       " '345',\n",
       " '917',\n",
       " '390',\n",
       " '1642',\n",
       " '935',\n",
       " '221',\n",
       " '535',\n",
       " '471',\n",
       " '392',\n",
       " '406',\n",
       " '771',\n",
       " '274',\n",
       " '117',\n",
       " '311',\n",
       " '82',\n",
       " '521',\n",
       " '442',\n",
       " '511',\n",
       " '353',\n",
       " '1596',\n",
       " '804',\n",
       " '1299',\n",
       " '332',\n",
       " '330',\n",
       " '255',\n",
       " '499',\n",
       " '540',\n",
       " '757',\n",
       " '435',\n",
       " '818',\n",
       " '548',\n",
       " '31',\n",
       " '787',\n",
       " '57',\n",
       " '467',\n",
       " '583',\n",
       " '812',\n",
       " '189',\n",
       " '147',\n",
       " '819',\n",
       " '1203',\n",
       " '89',\n",
       " '1012',\n",
       " '1652',\n",
       " '434',\n",
       " '331',\n",
       " '309',\n",
       " '594',\n",
       " '476',\n",
       " '338',\n",
       " '41',\n",
       " '762',\n",
       " '141',\n",
       " '579',\n",
       " '532',\n",
       " '571',\n",
       " '1259',\n",
       " '299',\n",
       " '191',\n",
       " '1661',\n",
       " '1053',\n",
       " '1002',\n",
       " '464',\n",
       " '665',\n",
       " '73',\n",
       " '136',\n",
       " '1003',\n",
       " '971',\n",
       " '260',\n",
       " '1277',\n",
       " '71',\n",
       " '398',\n",
       " '396',\n",
       " '831',\n",
       " '308',\n",
       " '333',\n",
       " '439',\n",
       " '635',\n",
       " '2319',\n",
       " '918',\n",
       " '495',\n",
       " '361',\n",
       " '561',\n",
       " '1499',\n",
       " '1592',\n",
       " '297',\n",
       " '187',\n",
       " '119',\n",
       " '175',\n",
       " '295',\n",
       " '691',\n",
       " '131',\n",
       " '93',\n",
       " '591',\n",
       " '461',\n",
       " '494',\n",
       " '1227',\n",
       " '241',\n",
       " '209',\n",
       " '1644',\n",
       " '328',\n",
       " '1555',\n",
       " '1584',\n",
       " '224',\n",
       " '550',\n",
       " '2162',\n",
       " '915',\n",
       " '938',\n",
       " '615',\n",
       " '1178',\n",
       " '250',\n",
       " '965',\n",
       " '473',\n",
       " '964',\n",
       " '734',\n",
       " '575',\n",
       " '424',\n",
       " '240',\n",
       " '657',\n",
       " '854',\n",
       " '643',\n",
       " '538',\n",
       " '678',\n",
       " '1004',\n",
       " '477',\n",
       " '1182',\n",
       " '137',\n",
       " '1359',\n",
       " '623',\n",
       " '86',\n",
       " '1051',\n",
       " '440',\n",
       " '933',\n",
       " '692',\n",
       " '138',\n",
       " '1024',\n",
       " '1319',\n",
       " '956',\n",
       " '280',\n",
       " '671',\n",
       " '1192',\n",
       " '519',\n",
       " '998',\n",
       " '63',\n",
       " '365',\n",
       " '988',\n",
       " '806',\n",
       " '1430',\n",
       " '705',\n",
       " '1021',\n",
       " '132',\n",
       " '145',\n",
       " '1352',\n",
       " '1068',\n",
       " '981',\n",
       " '1864',\n",
       " '889',\n",
       " '94',\n",
       " '380',\n",
       " '1325',\n",
       " '1338',\n",
       " '505',\n",
       " '322',\n",
       " '373',\n",
       " '403',\n",
       " '337',\n",
       " '711',\n",
       " '739',\n",
       " '236',\n",
       " '1110',\n",
       " '233',\n",
       " '1645',\n",
       " '641',\n",
       " '1290',\n",
       " '878',\n",
       " '37',\n",
       " '1426',\n",
       " '341',\n",
       " '36',\n",
       " '578',\n",
       " '853',\n",
       " '836',\n",
       " '690',\n",
       " '3034',\n",
       " '1162',\n",
       " '382',\n",
       " '1759',\n",
       " '761',\n",
       " '783',\n",
       " '290',\n",
       " '1104',\n",
       " '1032',\n",
       " '1593',\n",
       " '829',\n",
       " '646',\n",
       " '675',\n",
       " '1042',\n",
       " '40',\n",
       " '1101',\n",
       " '98',\n",
       " '123',\n",
       " '652',\n",
       " '1640',\n",
       " '1236',\n",
       " '1100',\n",
       " '239',\n",
       " '539',\n",
       " '830',\n",
       " '2643',\n",
       " '1263',\n",
       " '884',\n",
       " '989',\n",
       " '1038',\n",
       " '1377',\n",
       " '707',\n",
       " '302',\n",
       " '611',\n",
       " '190',\n",
       " '1365',\n",
       " '706',\n",
       " '753',\n",
       " '599',\n",
       " '852',\n",
       " '626',\n",
       " '612',\n",
       " '790',\n",
       " '1133',\n",
       " '1130',\n",
       " '875',\n",
       " '1958',\n",
       " '291',\n",
       " '861',\n",
       " '669',\n",
       " '2169',\n",
       " '689',\n",
       " '850',\n",
       " '389',\n",
       " '940',\n",
       " '1495',\n",
       " '683',\n",
       " '1619',\n",
       " '2538',\n",
       " '1347',\n",
       " '423',\n",
       " '542',\n",
       " '1174',\n",
       " '1326',\n",
       " '1698',\n",
       " '270',\n",
       " '315',\n",
       " '862',\n",
       " '395',\n",
       " '1',\n",
       " '662',\n",
       " '810',\n",
       " '2124',\n",
       " '157',\n",
       " '507',\n",
       " '468',\n",
       " '526',\n",
       " '1570',\n",
       " '77',\n",
       " '1425',\n",
       " '1847',\n",
       " '466',\n",
       " '1424',\n",
       " '2641',\n",
       " '1209',\n",
       " '1226',\n",
       " '1249',\n",
       " '1329',\n",
       " '413',\n",
       " '1680',\n",
       " '920',\n",
       " '1364',\n",
       " '196',\n",
       " '359',\n",
       " '914',\n",
       " '405',\n",
       " '292',\n",
       " '319',\n",
       " '437',\n",
       " '113',\n",
       " '949',\n",
       " '1275',\n",
       " '1512',\n",
       " '1239',\n",
       " '265',\n",
       " '990',\n",
       " '1107',\n",
       " '1522',\n",
       " '357',\n",
       " '864',\n",
       " '1097',\n",
       " '887',\n",
       " '1153',\n",
       " '1575',\n",
       " '352',\n",
       " '1132',\n",
       " '984',\n",
       " '474',\n",
       " '1582',\n",
       " '20',\n",
       " '262',\n",
       " '679',\n",
       " '1311',\n",
       " '601',\n",
       " '178',\n",
       " '593',\n",
       " '719',\n",
       " '799',\n",
       " '1574',\n",
       " '736',\n",
       " '1286',\n",
       " '1310',\n",
       " '588',\n",
       " '99',\n",
       " '397',\n",
       " '237',\n",
       " '566',\n",
       " '1635',\n",
       " '1397',\n",
       " '1223',\n",
       " '1667',\n",
       " '895',\n",
       " '916',\n",
       " '2260',\n",
       " '1121',\n",
       " '590',\n",
       " '749',\n",
       " '760',\n",
       " '491',\n",
       " '788',\n",
       " '1600',\n",
       " '500',\n",
       " '1129',\n",
       " '744',\n",
       " '1403',\n",
       " '653',\n",
       " '909',\n",
       " '808',\n",
       " '823',\n",
       " '2006',\n",
       " '1027',\n",
       " '2213',\n",
       " '30',\n",
       " '1059',\n",
       " '1135',\n",
       " '1536',\n",
       " '1675',\n",
       " '374',\n",
       " '723',\n",
       " '1274',\n",
       " '877',\n",
       " '921',\n",
       " '493',\n",
       " '1193',\n",
       " '2175',\n",
       " '367',\n",
       " '784',\n",
       " '1405',\n",
       " '148',\n",
       " '483',\n",
       " '1228',\n",
       " '261',\n",
       " '310',\n",
       " '1370',\n",
       " '91',\n",
       " '1935',\n",
       " '980',\n",
       " '642',\n",
       " '156',\n",
       " '969',\n",
       " '881',\n",
       " '955',\n",
       " '1496',\n",
       " '573',\n",
       " '1758',\n",
       " '144',\n",
       " '817',\n",
       " '225',\n",
       " '1013',\n",
       " '1648',\n",
       " '693',\n",
       " '1160',\n",
       " '625',\n",
       " '979',\n",
       " '1903',\n",
       " '281',\n",
       " '1207',\n",
       " '1794',\n",
       " '95',\n",
       " '826',\n",
       " '1085',\n",
       " '1610',\n",
       " '369',\n",
       " '1154',\n",
       " '1245',\n",
       " '945',\n",
       " '798',\n",
       " '919',\n",
       " '1287',\n",
       " '2483',\n",
       " '1087',\n",
       " '684',\n",
       " '1089',\n",
       " '2415',\n",
       " '197',\n",
       " '947',\n",
       " '356',\n",
       " '609',\n",
       " '1141',\n",
       " '2037',\n",
       " '2113',\n",
       " '1048',\n",
       " '1928',\n",
       " '1440',\n",
       " '1050',\n",
       " '682',\n",
       " '1525',\n",
       " '2436',\n",
       " '105',\n",
       " '2420',\n",
       " '568',\n",
       " '674',\n",
       " '1431',\n",
       " '2583',\n",
       " '458',\n",
       " '1446',\n",
       " '534',\n",
       " '2089',\n",
       " '598',\n",
       " '349',\n",
       " '1213',\n",
       " '1732',\n",
       " '1493',\n",
       " '1803',\n",
       " '672',\n",
       " '741',\n",
       " '1322',\n",
       " '849',\n",
       " '2418',\n",
       " '166',\n",
       " '1361',\n",
       " '1116',\n",
       " '1553',\n",
       " '1478',\n",
       " '602',\n",
       " '2636',\n",
       " '2879',\n",
       " '661',\n",
       " '1168',\n",
       " '2059',\n",
       " '708',\n",
       " '1268',\n",
       " '7',\n",
       " '904',\n",
       " '2732',\n",
       " '1438',\n",
       " '1899',\n",
       " '880',\n",
       " '824',\n",
       " '1094',\n",
       " '1556',\n",
       " '1453',\n",
       " '81',\n",
       " '2351',\n",
       " '2421',\n",
       " '1491',\n",
       " '1545',\n",
       " '972',\n",
       " '1389',\n",
       " '1934',\n",
       " '2170',\n",
       " '722',\n",
       " '754',\n",
       " '767',\n",
       " '475',\n",
       " '1770',\n",
       " '2283',\n",
       " '959',\n",
       " '411',\n",
       " '149',\n",
       " '506',\n",
       " '1001',\n",
       " '1366',\n",
       " '1125',\n",
       " '961',\n",
       " '1282',\n",
       " '64',\n",
       " '613',\n",
       " '231',\n",
       " '288',\n",
       " '1786',\n",
       " '603',\n",
       " '1497',\n",
       " '856',\n",
       " '966',\n",
       " '100',\n",
       " '371',\n",
       " '531',\n",
       " '649',\n",
       " '1041',\n",
       " '859',\n",
       " '885',\n",
       " '650',\n",
       " '1769',\n",
       " '2208',\n",
       " '517',\n",
       " '1588',\n",
       " '2008',\n",
       " '1461',\n",
       " '1622',\n",
       " '1534',\n",
       " '503',\n",
       " '1324',\n",
       " '393',\n",
       " '2909',\n",
       " '729',\n",
       " '1092',\n",
       " '155',\n",
       " '1170',\n",
       " '2290',\n",
       " '822',\n",
       " '873',\n",
       " '1894',\n",
       " '438',\n",
       " '1760',\n",
       " '1706',\n",
       " '1506',\n",
       " '327',\n",
       " '888',\n",
       " '3284',\n",
       " '1568',\n",
       " '502',\n",
       " '1394',\n",
       " '1476',\n",
       " '1581',\n",
       " '2210',\n",
       " '1868',\n",
       " '174',\n",
       " '1822',\n",
       " '1985',\n",
       " '235',\n",
       " '710',\n",
       " '648',\n",
       " '2066',\n",
       " '2197',\n",
       " '2250',\n",
       " '777',\n",
       " '1649',\n",
       " '1235',\n",
       " '1881',\n",
       " '1009',\n",
       " '1080',\n",
       " '640',\n",
       " '1479',\n",
       " '3444',\n",
       " '1450',\n",
       " '772',\n",
       " '449',\n",
       " '907',\n",
       " '1022',\n",
       " '2434',\n",
       " '301',\n",
       " '408',\n",
       " '1618',\n",
       " '1233',\n",
       " '2448',\n",
       " '2041',\n",
       " '97',\n",
       " '554',\n",
       " '1415',\n",
       " '1470',\n",
       " '1842',\n",
       " '811',\n",
       " '1401',\n",
       " '1763',\n",
       " '726',\n",
       " '2187',\n",
       " '586',\n",
       " '770',\n",
       " '2409',\n",
       " '1139',\n",
       " '1811',\n",
       " '809',\n",
       " '928',\n",
       " '1231',\n",
       " '3917',\n",
       " '899',\n",
       " '1206',\n",
       " '1877',\n",
       " '1105',\n",
       " '1832',\n",
       " '1993',\n",
       " '1307',\n",
       " '1819',\n",
       " '323',\n",
       " '1272',\n",
       " '2460',\n",
       " '991',\n",
       " '1354',\n",
       " '1603',\n",
       " '942',\n",
       " '1666',\n",
       " '1723',\n",
       " '1801',\n",
       " '3289',\n",
       " '381',\n",
       " '1804',\n",
       " '2023',\n",
       " '986',\n",
       " '3452',\n",
       " '1500',\n",
       " '3173',\n",
       " '3381',\n",
       " '479',\n",
       " '559',\n",
       " '614',\n",
       " '497',\n",
       " '2334',\n",
       " '1057',\n",
       " '1378',\n",
       " '186',\n",
       " '1134',\n",
       " '2042',\n",
       " '2069',\n",
       " '508',\n",
       " '1345',\n",
       " '214',\n",
       " '1267',\n",
       " '482',\n",
       " '1353',\n",
       " '2764',\n",
       " '78',\n",
       " '181',\n",
       " '520',\n",
       " '732',\n",
       " '1300',\n",
       " '1918',\n",
       " '22',\n",
       " '644',\n",
       " '1383',\n",
       " '2363',\n",
       " '523',\n",
       " '1190',\n",
       " '2235',\n",
       " '384',\n",
       " '963',\n",
       " '428',\n",
       " '412',\n",
       " '1331',\n",
       " '54',\n",
       " '446',\n",
       " '1870',\n",
       " '2229',\n",
       " '2198',\n",
       " '1564',\n",
       " '1672',\n",
       " '2172',\n",
       " '1276',\n",
       " '1643',\n",
       " '1662',\n",
       " '1990',\n",
       " '2945',\n",
       " '109',\n",
       " '3171',\n",
       " '1517',\n",
       " '1948',\n",
       " '2746',\n",
       " '962',\n",
       " '1787',\n",
       " '1145',\n",
       " '655',\n",
       " '1854',\n",
       " '2575',\n",
       " '1701',\n",
       " '2349',\n",
       " '2612',\n",
       " '1411',\n",
       " '1551',\n",
       " '1883',\n",
       " '2820',\n",
       " '378',\n",
       " '443',\n",
       " '1323',\n",
       " '840',\n",
       " '1445',\n",
       " '2038',\n",
       " '1106',\n",
       " '2000',\n",
       " '194',\n",
       " '486',\n",
       " '608',\n",
       " '1164',\n",
       " '687',\n",
       " '1780',\n",
       " '1793',\n",
       " '2214',\n",
       " '3882',\n",
       " '15',\n",
       " '791',\n",
       " '1058',\n",
       " '1448',\n",
       " '3331',\n",
       " '866',\n",
       " '1211',\n",
       " '170',\n",
       " '832',\n",
       " '906',\n",
       " '3478',\n",
       " '1158',\n",
       " '60',\n",
       " '1421',\n",
       " '2046',\n",
       " '2572',\n",
       " '620',\n",
       " '1198',\n",
       " '2328',\n",
       " '512',\n",
       " '567',\n",
       " '967',\n",
       " '1812',\n",
       " '2047',\n",
       " '651',\n",
       " '704',\n",
       " '1252',\n",
       " '1693',\n",
       " '2403',\n",
       " '21',\n",
       " '600',\n",
       " '897',\n",
       " '1016',\n",
       " '589',\n",
       " '1295',\n",
       " '1798',\n",
       " '2116',\n",
       " '1636',\n",
       " '216',\n",
       " '795',\n",
       " '1507',\n",
       " '1650',\n",
       " '2111',\n",
       " '3931',\n",
       " '205',\n",
       " '636',\n",
       " '1180',\n",
       " '1219',\n",
       " '2601',\n",
       " '2885',\n",
       " '855',\n",
       " '1251',\n",
       " '1396',\n",
       " '1943',\n",
       " '4108',\n",
       " '750',\n",
       " '958',\n",
       " '968',\n",
       " '3026',\n",
       " '673',\n",
       " '1613',\n",
       " '1653',\n",
       " '1756',\n",
       " '560',\n",
       " '624',\n",
       " '1562',\n",
       " '2471',\n",
       " '2595',\n",
       " '218',\n",
       " '925',\n",
       " '1123',\n",
       " '1332',\n",
       " '1654',\n",
       " '219',\n",
       " '926',\n",
       " ...]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_num = 2\n",
    "maxlen = corpus.max_len\n",
    "max_features = len(corpus.vocab)\n",
    "embedding_dims = 300\n",
    "epochs = 5\n",
    "dropout_rate = 0.2\n",
    "feature_size = 128\n",
    "kernel_sizes = [2, 3, 4, 5] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TEXTCNN(maxlen = maxlen,\n",
    "                max_features = max_features,\n",
    "                embedding_dims = embedding_dims,\n",
    "                class_num = class_num,\n",
    "                kernel_sizes = kernel_sizes,\n",
    "                dropout_rate = dropout_rate,\n",
    "                feature_size = feature_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, valid_loader, test_loader = corpus.get_loaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in train_loader:\n",
    "    inputs = x[:-1]\n",
    "    y = x[-1]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0427,  1.7961],\n",
       "        [-0.2734,  0.8580],\n",
       "        [-0.1631,  0.8901],\n",
       "        [-0.7502,  0.1731]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(*inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../tianchi_datasets/track3_round1_newtrain3.tsv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-2b9bd9acb893>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m train_data = pd.read_csv(\"../tianchi_datasets/track3_round1_newtrain3.tsv\", sep=\"\\t\", header=None,\n\u001b[1;32m----> 3\u001b[1;33m                                  names=[\"sentence1\", \"sentence2\", \"labels\"])\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"document\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"sentence1\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"sentence2\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\" \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m test_data = pd.read_csv(\"../tianchi_datasets/track3_round1_testA.tsv\", sep=\"\\t\", header=None,\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    684\u001b[0m     )\n\u001b[0;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 452\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    934\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    935\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 936\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    937\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    938\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1166\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1167\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1168\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1169\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1170\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1996\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1997\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1998\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1999\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2000\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../tianchi_datasets/track3_round1_newtrain3.tsv'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "train_data = pd.read_csv(\"../tianchi_datasets/track3_round1_newtrain3.tsv\", sep=\"\\t\", header=None,\n",
    "                                 names=[\"sentence1\", \"sentence2\", \"labels\"])\n",
    "train_data[\"document\"] = train_data[\"sentence1\"].str.cat(train_data[\"sentence2\"], sep = \" \")\n",
    "test_data = pd.read_csv(\"../tianchi_datasets/track3_round1_testA.tsv\", sep=\"\\t\", header=None,\n",
    "                                names=[\"sentence1\", \"sentence2\"])\n",
    "test_data[\"document\"] = test_data[\"sentence1\"].str.cat(test_data[\"sentence2\"], sep=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************Data_precessing*********************************\n",
      "## dataset size is 121105\n",
      "*******************************Load  dataset ....*******************************\n",
      "## Load Datasets Consume 0:00:07 s ###\n",
      "************************All Train and Test Data loaded !************************\n"
     ]
    }
   ],
   "source": [
    "from Data_generator_vocab import Corpus\n",
    "HYPERS = {\n",
    "        \"BATCH_SIZE\":32 * 4,\n",
    "        \"LR\": 5e-5,\n",
    "        \"EPOCHS\": 7,\n",
    "    }\n",
    "seed_val = 2021\n",
    "corpus = Corpus(HYPERS[\"BATCH_SIZE\"], seed_val)\n",
    "class_num = 2\n",
    "maxlen = corpus.max_len\n",
    "max_features = len(corpus.vocab)\n",
    "embedding_dims = 400\n",
    "dropout_rate = 0.2\n",
    "feature_size = 1024\n",
    "kernel_sizes = [2, 3, 4, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20604"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from bert4keras.snippets import truncate_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert4keras.tokenizers import load_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./bert4keras/counts.json\", \"r\") as fr:\n",
    "    counts = json.load(fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_count = 5\n",
    "maxlen = 32\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    \"\"\"加载数据\n",
    "    单条格式：(文本1 ids, 文本2 ids, 标签id)\n",
    "    \"\"\"\n",
    "    D = []\n",
    "    with open(filename) as f:\n",
    "        for l in f:\n",
    "            l = l.strip().split('\\t')\n",
    "            if len(l) == 3:\n",
    "                a, b, c = l[0], l[1], int(l[2])\n",
    "            else:\n",
    "                a, b, c = l[0], l[1], -5  # 未标注数据，标签为-5\n",
    "            a = [int(i) for i in a.split(' ')]\n",
    "            b = [int(i) for i in b.split(' ')]\n",
    "            truncate_sequences(maxlen, -1, a, b)\n",
    "            D.append((a, b, c))\n",
    "    return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data(\n",
    "    './tianchi_datasets/track3_round1_newtrain3.tsv'\n",
    ")\n",
    "test_data = load_data(\"./tianchi_datasets/track3_round1_testA.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = {}\n",
    "for d in data + test_data:\n",
    "    for i in d[0] + d[1]:\n",
    "        tokens[i] = tokens.get(i, 0) + 1\n",
    "\n",
    "tokens = {i: j for i, j in tokens.items() if j >= min_count}\n",
    "tokens = sorted(tokens.items(), key=lambda s: -s[1])\n",
    "#tokens = {\n",
    "    #t[0]: i + 7\n",
    "    #for i, t in enumerate(tokens)\n",
    "#}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_path = \"./pretrain_model/chinese_roberta_wwm_ext/vocab.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./bert4keras/counts.json\", \"r\") as fr:\n",
    "    counts = json.load(fr)\n",
    "del counts['[CLS]']\n",
    "del counts['[SEP]']\n",
    "token_dict = load_vocab(dict_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs = [\n",
    "    counts.get(i, 0) for i, j in sorted(token_dict.items(), key=lambda s: s[1])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_tokens = list(np.argsort(freqs)[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7512"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens1 = {}\n",
    "for d in data + test_data:\n",
    "    for i in d[0] + d[1]:\n",
    "        tokens1[i] = tokens1.get(i, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7512"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert4keras.models import build_transformer_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "?build_transformer_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from bert4keras.backend import keras, K\n",
    "from bert4keras.tokenizers import load_vocab\n",
    "from bert4keras.models import build_transformer_model\n",
    "from bert4keras.optimizers import Adam\n",
    "from bert4keras.snippets import sequence_padding, DataGenerator\n",
    "from bert4keras.snippets import truncate_sequences\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_count = 5\n",
    "maxlen = 32\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_mask(text_ids):\n",
    "    \"\"\"随机mask\n",
    "    \"\"\"\n",
    "    input_ids, output_ids = [], []\n",
    "    rands = np.random.random(len(text_ids))\n",
    "    for r, i in zip(rands, text_ids):\n",
    "        if r < 0.15 * 0.8:\n",
    "            input_ids.append(4)\n",
    "            output_ids.append(i)\n",
    "        elif r < 0.15 * 0.9:\n",
    "            input_ids.append(i)\n",
    "            output_ids.append(i)\n",
    "        elif r < 0.15:\n",
    "            input_ids.append(np.random.choice(len(tokens)) + 7)\n",
    "            output_ids.append(i)\n",
    "        else:\n",
    "            input_ids.append(i)\n",
    "            output_ids.append(0)\n",
    "    return input_ids, output_ids\n",
    "\n",
    "\n",
    "def sample_convert(text1, text2, label, random=False):\n",
    "    \"\"\"转换为MLM格式\n",
    "    \"\"\"\n",
    "    text1_ids = [tokens.get(t, 1) for t in text1]\n",
    "    text2_ids = [tokens.get(t, 1) for t in text2]\n",
    "    if random:\n",
    "        if np.random.random() < 0.5:\n",
    "            text1_ids, text2_ids = text2_ids, text1_ids\n",
    "        text1_ids, out1_ids = random_mask(text1_ids)\n",
    "        text2_ids, out2_ids = random_mask(text2_ids)\n",
    "    else:\n",
    "        out1_ids = [0] * len(text1_ids)\n",
    "        out2_ids = [0] * len(text2_ids)\n",
    "    token_ids = [2] + text1_ids + [3] + text2_ids + [3]\n",
    "    segment_ids = [0] * len(token_ids)\n",
    "    output_ids = [label + 5] + out1_ids + [0] + out2_ids + [0]\n",
    "    return token_ids, segment_ids, output_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_generator(DataGenerator):\n",
    "    \"\"\"数据生成器\n",
    "    \"\"\"\n",
    "    def __iter__(self, random=False):\n",
    "        batch_token_ids, batch_segment_ids, batch_output_ids = [], [], []\n",
    "        for is_end, (text1, text2, label) in self.sample(random):\n",
    "            token_ids, segment_ids, output_ids = sample_convert(\n",
    "                text1, text2, label, random\n",
    "            )\n",
    "            batch_token_ids.append(token_ids)\n",
    "            batch_segment_ids.append(segment_ids)\n",
    "            batch_output_ids.append(output_ids)\n",
    "            if len(batch_token_ids) == self.batch_size or is_end:\n",
    "                batch_token_ids = sequence_padding(batch_token_ids)\n",
    "                batch_segment_ids = sequence_padding(batch_segment_ids)\n",
    "                batch_output_ids = sequence_padding(batch_output_ids)\n",
    "                yield [batch_token_ids, batch_segment_ids], batch_output_ids\n",
    "                batch_token_ids, batch_segment_ids, batch_output_ids = [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_generator = data_generator(data, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.data_generator at 0x2159fb999e8>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nwith open(\"v2i.json\", \"w\") as fw:\\n    json.dump(v2i, fw)\\n\\nwith open(\\'test.json\\', \\'r\\') as f:\\n    data = json.load(f)\\n'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from bert4keras.backend import keras, K\n",
    "from bert4keras.tokenizers import load_vocab\n",
    "from bert4keras.models import build_transformer_model\n",
    "from bert4keras.optimizers import Adam\n",
    "from bert4keras.snippets import sequence_padding, DataGenerator\n",
    "from bert4keras.snippets import truncate_sequences\n",
    "from tqdm import tqdm\n",
    "\n",
    "min_count = 5\n",
    "maxlen = 32\n",
    "batch_size = 64\n",
    "\n",
    "dict_path = \"./pretrain_model/chinese_roberta_wwm_ext/vocab.txt\"\n",
    "\n",
    "def load_data(filename):\n",
    "    \"\"\"加载数据\n",
    "    单条格式：(文本1 ids, 文本2 ids, 标签id)\n",
    "    \"\"\"\n",
    "    D = []\n",
    "    with open(filename) as f:\n",
    "        for l in f:\n",
    "            l = l.strip().split('\\t')\n",
    "            if len(l) == 3:\n",
    "                a, b, c = l[0], l[1], int(l[2])\n",
    "            else:\n",
    "                a, b, c = l[0], l[1], -5  # 未标注数据，标签为-5\n",
    "            a = [int(i) for i in a.split(' ')]\n",
    "            b = [int(i) for i in b.split(' ')]\n",
    "            truncate_sequences(maxlen, -1, a, b)\n",
    "            D.append((a, b, c))\n",
    "    return D\n",
    "train_data = load_data(\n",
    "    './tianchi_datasets/track3_round1_train.tsv'\n",
    ")\n",
    "test_data = load_data(\n",
    "    './tianchi_datasets/track3_round1_testA.tsv'\n",
    ")\n",
    "\n",
    "\n",
    "def load_data(filename):\n",
    "    \"\"\"加载数据\n",
    "    单条格式：(文本1 ids, 文本2 ids, 标签id)\n",
    "    \"\"\"\n",
    "    D = []\n",
    "    with open(filename) as f:\n",
    "        for l in f:\n",
    "            l = l.strip().split('\\t')\n",
    "            if len(l) == 3:\n",
    "                a, b, c = l[0], l[1], int(l[2])\n",
    "            else:\n",
    "                a, b, c = l[0], l[1], -5  # 未标注数据，标签为-5\n",
    "            a = [int(i) for i in a.split(' ')]\n",
    "            b = [int(i) for i in b.split(' ')]\n",
    "            truncate_sequences(maxlen, -1, a, b)\n",
    "            D.append((a, b, c))\n",
    "    return D\n",
    "\n",
    "# 统计词频\n",
    "tokens = {}\n",
    "for d in data + test_data:\n",
    "    for i in d[0] + d[1]:\n",
    "        tokens[i] = tokens.get(i, 0) + 1\n",
    "\n",
    "tokens = {i: j for i, j in tokens.items() if j >= min_count}\n",
    "tokens = sorted(tokens.items(), key=lambda s: -s[1])\n",
    "\n",
    "token_dict = load_vocab(dict_path)\n",
    "counts = json.load(open('./bert4keras/counts.json'))\n",
    "del counts['[CLS]']\n",
    "del counts['[SEP]']\n",
    "keep_tokens = {j:counts.get(i, 0) for i, j in sorted(token_dict.items(), key = lambda s:s[1])}\n",
    "keep_tokens = sorted(keep_tokens.items(), key = lambda s:-s[1])\n",
    "keep_tokens1 = keep_tokens[:len(tokens)]\n",
    "v2i = {tokens[i][0]:keep_tokens1[i][0] for i in range(len(tokens))}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "with open(\"v2i.json\", \"w\") as fw:\n",
    "    json.dump(v2i, fw)\n",
    "\n",
    "with open('test.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7512"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "v2i['[UNK]'] = 100\n",
    "v2i['[CLS]'] = 101\n",
    "v2i['[SEP]'] = 102"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7515"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(v2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,j in v2i.items():\n",
    "    if j in [100, 101, 102, 103]:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"v2i.json\", \"w\") as fw:\n",
    "    json.dump(v2i, fw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('v2i.json', 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7515"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### 从 ./pretrain_model/chinese_roberta_wwm_ext/ 加载预训练模型, 重新训练 ###\n",
      "## Model ChineseRobertaAttention loaded. ##\n",
      "## Tokenizer BertTokenizer loaded. ##\n",
      "*****************************加载训练集、验证集、测试集 Loading******************************\n",
      "********************************Data_precessing*********************************\n",
      "## dataset size is 121105\n",
      "*******************************Load  dataset ....*******************************\n",
      "## Load Datasets Consume 0:00:06 s ###\n",
      "************************All Train and Test Data loaded !************************\n"
     ]
    }
   ],
   "source": [
    "# %load finetune_test.py\n",
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "FileName: finetune.py\n",
    "Description:\n",
    "Author: Stark Lv\n",
    "Date: 2021/2/27 6:13 PM\n",
    "Version: 0.1\n",
    "\"\"\"\n",
    "\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from models import MODELS\n",
    "import torch\n",
    "import logging\n",
    "from torch import nn\n",
    "#from Data_generator import Corpus\n",
    "from Data_generator_vocab import Corpus\n",
    "#from Data_generator_tokenizer import Corpus\n",
    "import time\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "def format_time(elapsed):\n",
    "    elapsed_rounded = int(round(elapsed))\n",
    "    return str(datetime.timedelta(seconds = elapsed_rounded))\n",
    "\n",
    "class fintune():\n",
    "\n",
    "    def __init__(self, model_name = \"BertBaseLinear\", retrain_model_path = None):\n",
    "\n",
    "        #retrain_model_path,是否重新开始训练,若从已训练好的模型继续训练,则输入模型,否则为None\n",
    "        self.retrain_model_path = retrain_model_path\n",
    "        self.model_name = model_name\n",
    "        self.set_device() #创建了self.device, self.n_gpu\n",
    "        self.set_random_seed() #设置随机数种子,保证所有结果可以复现\n",
    "\n",
    "\n",
    "        #加载model\n",
    "        self.model, self.tokenizer = self.load_model_tokenizer(model_name)\n",
    "\n",
    "        # move model to GPU\n",
    "        if self.n_gpu > 1:\n",
    "            device_ids = [0, 1, 2, 3]\n",
    "            self.model =torch.nn.DataParallel(self.model, device_ids= device_ids)\n",
    "        self.model = self.model.to(self.device)\n",
    "\n",
    "        #loss function & optimizer\n",
    "        # 带权重的损失函数, 若positive_label为1的f1_score为 0.75，positive_label为0的f1_score为0.85\n",
    "        #self.loss_func = nn.CrossEntropyLoss(weight= torch.tensor([1.1765, 1.3333])).to(self.device)\n",
    "        self.loss_func = nn.CrossEntropyLoss(weight=torch.tensor([0.5, 0.5])).to(self.device)\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "                self.model.parameters(),\n",
    "                lr = HYPERS[\"LR\"],\n",
    "                eps = 1e-8)\n",
    "\n",
    "        # 加载训练集、验证集、测试集\n",
    "        print(f\"{'加载训练集、验证集、测试集 Loading':*^80}\")\n",
    "        self.corpus = Corpus(HYPERS[\"BATCH_SIZE\"], self.seed_val)\n",
    "        self.train_loader, self.valid_loader, self.test_loader = self.corpus.get_loaders()\n",
    "\n",
    "\n",
    "\n",
    "    def load_model_tokenizer(self, model_name):\n",
    "        #model_path = os.path.join(os.getcwd(), MODELS[model_name][\"path\"])\n",
    "        model_path = MODELS[model_name][\"path\"]\n",
    "        assert model_name in MODELS\n",
    "\n",
    "        if self.retrain_model_path is None:\n",
    "\n",
    "            print(f\"### 从 {model_path} 加载预训练模型, 重新训练 ###\")\n",
    "            model = MODELS[model_name][\"class\"](model_path)\n",
    "        else:\n",
    "            print(f\"{'加载已预训练好的模型,继续训练':*^80}\")\n",
    "            model = torch.load(\"./fintuned_model/\" + self.retrain_model_path)\n",
    "\n",
    "        print(f\"## Model {self.model_name} loaded. ##\")\n",
    "\n",
    "        tokenizer = MODELS[model_name][\"tokenizer\"].from_pretrained(model_path)\n",
    "\n",
    "        ## __class__.__name__输出类的名字\n",
    "        print(f\"## Tokenizer {tokenizer.__class__.__name__} loaded. ##\")\n",
    "\n",
    "        return model, tokenizer\n",
    "\n",
    "    def set_random_seed(self):\n",
    "        # Set the seed value all over the place to make this reproducible.\n",
    "        self.seed_val = 2021\n",
    "        np.random.seed(self.seed_val)\n",
    "        torch.manual_seed(self.seed_val)\n",
    "\n",
    "    def set_device(self):\n",
    "        ###设置日志时间的输出格式,说明在那段时间用过GPU\n",
    "        LOG_FORMAT = \"%(asctime)s - %(levelname)s - %(message)s\"\n",
    "        DATE_FORMAT = \"%m/%d/%Y %H:%M:%S %p\"\n",
    "        logging.basicConfig(filename='NLP_GPU.log', level=logging.DEBUG, format=LOG_FORMAT, datefmt=DATE_FORMAT)\n",
    "\n",
    "        # 设置运行环境GPU\n",
    "        self.device = torch.device(\"cuda:0\")\n",
    "        self.n_gpu = torch.cuda.device_count()\n",
    "        logging.info(msg = f\"\\n Using GPU, {self.n_gpu} device available\")\n",
    "\n",
    "    def run_epoch(self):\n",
    "\n",
    "        #list to store a number of quantities such as\n",
    "        #training and validation loss, validation accuracy, and timings.\n",
    "        training_stats = []\n",
    "\n",
    "\n",
    "        #Total number of training steps is [number of batches × number of epochs]\n",
    "        total_steps = len(self.train_loader) * HYPERS[\"EPOCHS\"]\n",
    "        # create the learning rate scheduler\n",
    "        self.scheduler = get_linear_schedule_with_warmup(\n",
    "            self.optimizer, num_warmup_steps = 100, num_training_steps = total_steps\n",
    "        )\n",
    "\n",
    "        total_t0 = time.time()\n",
    "\n",
    "        print(f\"### Trainng {HYPERS['EPOCHS']} EPOCHS start ###\")\n",
    "        for epoch in tqdm(range(1, HYPERS[\"EPOCHS\"] + 1), unit = \"epoch\",\n",
    "                          desc = f\"Training All {HYPERS['EPOCHS']} Epochs\"):\n",
    "            # Measure how long the per training epoch takes\n",
    "            t0 = time.time()\n",
    "\n",
    "            # Put the model into training model\n",
    "            print(f\"### EPOCH {epoch} train  start\")\n",
    "            self.model.train()\n",
    "\n",
    "            epoch_train_loss = self.train(epoch)\n",
    "            # measure avg batch loss in specific epoch\n",
    "            avg_train_loss = epoch_train_loss / len(self.train_loader)\n",
    "            # Measure how long this epoch took.\n",
    "            training_time = format_time(time.time() - t0)\n",
    "            print(f\"## per batch train loss for epoch {epoch} is {avg_train_loss} \",\n",
    "                  f\"total train time : {training_time}\")\n",
    "\n",
    "            t0 = time.time()\n",
    "\n",
    "            # eval mode\n",
    "            # 在eval模式下不会应用DROPOUT和BATCHNORM\n",
    "            print(f\"### EPOCH {epoch} val start\")\n",
    "            self.model.eval()\n",
    "\n",
    "            epoch_eval_loss, eval_auc = self.eval(metric=roc_auc_score)\n",
    "            avg_eval_loss = epoch_eval_loss / len(self.valid_loader)\n",
    "            validation_time = format_time(time.time() - t0)\n",
    "            print(f\"## per batch valid loss for epoch {epoch} is {avg_eval_loss} \",\n",
    "                  f\"total valid time : {validation_time}, Validation auc score {eval_auc}\")\n",
    "\n",
    "            # Record all statistics from this epoch.\n",
    "            training_stats.append(\n",
    "                {\n",
    "                    'Epoch': epoch,\n",
    "                    'Train Loss': epoch_train_loss,\n",
    "                    'Avg Train Loss': avg_train_loss,\n",
    "                    'Train Time': training_time,\n",
    "                    'Valid Loss': epoch_eval_loss,\n",
    "                    'Avg Valid Loss': avg_eval_loss,\n",
    "                    'Valid Time': validation_time,\n",
    "                    'Valid AUC': eval_auc,\n",
    "                    'Test Results Dir': 'Model{}_BATCH{}_LR{}/'.format(self.model_name,\n",
    "                     HYPERS['BATCH_SIZE'], HYPERS['LR'])\n",
    "                }\n",
    "            )\n",
    "\n",
    "        test_t0 = time.time()\n",
    "        self.test()\n",
    "        print(\"Test Time Consume {}\".format(format_time(time.time() - test_t0)))\n",
    "        self.save_stats(training_stats)\n",
    "        print(f\"{'Training complete !':*^80}\")\n",
    "        print(f\"### Total Process took {format_time(time.time() - total_t0)} ####\")\n",
    "\n",
    "\n",
    "    def train(self, epoch):\n",
    "        total_train_loss = 0\n",
    "\n",
    "        for batch in tqdm(self.train_loader, desc=f\"Epoch {epoch}>>>>train\", unit=\"batch\"):\n",
    "            # input_ids, attention_mask, token_type_id和Label混在一起\n",
    "            inputs = [data.to(self.device) for data in batch[:-1]]\n",
    "            labels = batch[-1].to(self.device)\n",
    "            outputs = self.model(*inputs)\n",
    "\n",
    "\n",
    "            batch_loss = self.loss_func(outputs, labels)\n",
    "            total_train_loss += batch_loss.to(\"cpu\").data.numpy()\n",
    "            # clear any previously calculated gradients before performing a backward pass.\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            # perform a backward pass to calculate the gradients\n",
    "            batch_loss.backward()\n",
    "\n",
    "            # normalization of the gradients to 1.0 to avoid exploding gradients\n",
    "            nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "\n",
    "            # update parameters and take a step using the computed gradient\n",
    "            self.optimizer.step()\n",
    "\n",
    "            #update the learning rate \n",
    "            self.scheduler.step()\n",
    "\n",
    "            #print(f\"per batch loss {batch_loss.to('cpu').data.numpy()}\")\n",
    "\n",
    "        return total_train_loss\n",
    "\n",
    "    def eval(self, metric):\n",
    "        total_eval_loss = 0\n",
    "        predicted_scores = np.array([])\n",
    "        target_labels = np.array([])\n",
    "        #predicted_labels = np.array([])\n",
    "\n",
    "        for batch in self.valid_loader:\n",
    "            inputs = [data.to(self.device) for data in batch[:-1]]\n",
    "            labels = batch[-1].to(self.device)\n",
    "\n",
    "            # Tell pytorch not to bother with constructing the compute graph during\n",
    "            # the forward pass, since this is only needed for backprop (training).\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(*inputs)\n",
    "                loss = self.loss_func(outputs, labels)\n",
    "                total_eval_loss += loss.to(\"cpu\").data.numpy()\n",
    "\n",
    "                # move logits and labels to CPU,存储在CPU上面的变量不能和存储在GPU上面的变量进行运算\n",
    "                logits = F.softmax(outputs.to(\"cpu\"), dim = 1).data.numpy()\n",
    "                label_ids = labels.to(\"cpu\").data.numpy()\n",
    "                y_pred = logits[:, 1]\n",
    "                #y_pred1 = np.argmax(logits, axis = 1)\n",
    "                predicted_scores = np.append(predicted_scores, y_pred)\n",
    "                #predicted_labels = np.append(predicted_labels, y_pred1)\n",
    "                target_labels = np.append(target_labels, label_ids)\n",
    "\n",
    "\n",
    "        task_auc = metric(target_labels, predicted_scores)\n",
    "\n",
    "        #print(f\"{'classification_report':*^80}\")\n",
    "        #print(classification_report(target_labels, predicted_labels))\n",
    "        return total_eval_loss , task_auc\n",
    "\n",
    "    def test(self):\n",
    "\n",
    "        prediction_scores = np.array([])\n",
    "        print(f\"{'Predict task':*^80}\")\n",
    "        for batch in tqdm(self.test_loader, desc = \"Predict Scores Loading\", unit = \"batch\"):\n",
    "            inputs = [data.to(self.device) for data in batch]\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(*inputs)\n",
    "                logits = F.softmax(outputs.to(\"cpu\"), dim=1).data.numpy()\n",
    "                y_pred = logits[:, 1]\n",
    "                prediction_scores = np.append(prediction_scores, y_pred)\n",
    "\n",
    "        self.save_predictions(prediction_scores)\n",
    "\n",
    "    def save_predictions(self, predicts):\n",
    "        file_path = \"./finetuned_results/\" + 'Model{}_BATCH{}_EPOCH{}_LR{}/'.format(self.model_name,\n",
    "                  HYPERS['BATCH_SIZE'],HYPERS['EPOCHS'], HYPERS['LR'])\n",
    "        if not os.path.exists(file_path):\n",
    "            os.mkdir(file_path)\n",
    "\n",
    "        #输出当前时间\n",
    "        dt = str(datetime.datetime.now()).split(\" \")[0]\n",
    "        filename = \"{}_{}_(对应id和score).json\".format(self.model_name, dt)\n",
    "        with open(file_path + filename, \"w\") as fw:\n",
    "            for idx, predict in enumerate(predicts):\n",
    "                predict = {\"id\": idx, \"label\": predict}\n",
    "                json.dump(predict, fw)\n",
    "                if idx == len(predicts) - 1:\n",
    "                    break\n",
    "                fw.write(\"\\n\")\n",
    "\n",
    "        filename2 = \"{}_{}_result.tsv\".format(self.model_name, dt)\n",
    "        with open(file_path + filename2, \"w\") as fw:\n",
    "            for idx, predict in enumerate(predicts):\n",
    "                fw.write(str(predict))\n",
    "                if idx == len(predicts) - 1:\n",
    "                    break\n",
    "                fw.write(\"\\n\")\n",
    "\n",
    "    def save_stats(self ,stats):\n",
    "        df = pd.DataFrame(stats)\n",
    "        filename = 'Stats_{}_BATCH{}_Epoch{}_LR{}.csv'.format(self.model_name, HYPERS['BATCH_SIZE'],\n",
    "                            HYPERS['EPOCHS'], HYPERS['LR'])\n",
    "        df.to_csv(\"./finetuned_results/\" + filename, sep = \",\", encoding=\"utf-8\", index = False)\n",
    "\n",
    "    def save_model(self):\n",
    "        # 保存训练好的模型\n",
    "        model_name = 'Model{}_BATCH{}_Epoch{}_LR{}_TIME{}.pkl'.format(self.model_name, HYPERS['BATCH_SIZE'],\n",
    "        HYPERS['EPOCHS'], HYPERS['LR'],time.strftime(\"%Y_%m_%d_%H_%M\",time.localtime()))\n",
    "        torch.save(self.model, \"./finetuned_model/\" + model_name)\n",
    "        print(\"Fintuned model saved\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ## 使用编号为4,5的两张显卡\n",
    "    #os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "    #os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4, 5, 6, 7\"\n",
    "    # global hyper parameters\n",
    "    HYPERS = {\n",
    "        \"BATCH_SIZE\":4,\n",
    "        \"LR\": 5e-5,\n",
    "        \"EPOCHS\": 7,\n",
    "    }\n",
    "    app = fintune(model_name = \"ChineseRobertaAttention\", retrain_model_path = None)\n",
    "    #app.run_epoch()\n",
    "    #app.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = app.corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "v2i = corpus.v2i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7515"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(v2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, valid_loader, test_loader = corpus.get_loaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    inputs = batch[:-1]\n",
    "    y = batch[-1]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v2i['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 101, 6821, 1762,  782, 2157,  517,  100, 8969, 4638, 2496, 8024,  102,\n",
       "          4638, 2496, 8024, 6121,  102,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "         [ 101, 8024, 1313, 4385, 1313,  102, 8024, 1313,  102,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "         [ 101, 1962, 3300,  702, 6089,  102, 8024,  704, 6821,  704, 6574, 6598,\n",
       "           102,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "         [ 101, 5310, 2898, 4638, 3441, 5739,  102, 3983,  908,  102,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0]])]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from bert4keras.backend import keras, K\n",
    "from bert4keras.tokenizers import load_vocab\n",
    "from bert4keras.models import build_transformer_model\n",
    "from bert4keras.optimizers import Adam\n",
    "from bert4keras.snippets import sequence_padding, DataGenerator\n",
    "from bert4keras.snippets import truncate_sequences\n",
    "from tqdm import tqdm\n",
    "\n",
    "min_count = 5\n",
    "maxlen = 32\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3 % 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1011'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bin(11).replace(\"0b\",'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29.897352853986263"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "math.log(10**9, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2 ** 29 > 10 ** 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from bert4keras.backend import keras, K\n",
    "from bert4keras.tokenizers import load_vocab\n",
    "from bert4keras.models import build_transformer_model\n",
    "from bert4keras.optimizers import Adam\n",
    "from bert4keras.snippets import sequence_padding, DataGenerator\n",
    "from bert4keras.snippets import truncate_sequences\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_count = 5\n",
    "maxlen = 32\n",
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    \"\"\"加载数据\n",
    "    单条格式：(文本1 ids, 文本2 ids, 标签id)\n",
    "    \"\"\"\n",
    "    D = []\n",
    "    with open(filename) as f:\n",
    "        for l in f:\n",
    "            l = l.strip().split('\\t')\n",
    "            if len(l) == 3:\n",
    "                a, b, c = l[0], l[1], int(l[2])\n",
    "            else:\n",
    "                a, b, c = l[0], l[1], -5  # 未标注数据，标签为-5\n",
    "            a = [int(i) for i in a.split(' ')]\n",
    "            b = [int(i) for i in b.split(' ')]\n",
    "            truncate_sequences(maxlen, -1, a, b)\n",
    "            D.append((a, b, c))\n",
    "    return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data(\n",
    "    './tianchi_datasets/track3_round1_train.tsv'\n",
    ")\n",
    "train_data = [d for i, d in enumerate(data) if i % 10 != 0]\n",
    "valid_data = [d for i, d in enumerate(data) if i % 10 == 0]\n",
    "test_data = load_data(\n",
    "    './tianchi_datasets/track3_round1_testA.tsv'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1, 2, 3, 4, 5, 6, 7], [8, 9, 10, 4, 11], 0)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_generator(DataGenerator):\n",
    "    \"\"\"数据生成器\n",
    "    \"\"\"\n",
    "    def __iter__(self, random=False):\n",
    "        batch_token_ids, batch_segment_ids, batch_output_ids = [], [], []\n",
    "        for is_end, (text1, text2, label) in self.sample(random):\n",
    "            token_ids, segment_ids, output_ids = sample_convert(\n",
    "                text1, text2, label, random\n",
    "            )\n",
    "            batch_token_ids.append(token_ids)\n",
    "            batch_segment_ids.append(segment_ids)\n",
    "            batch_output_ids.append(output_ids)\n",
    "            if len(batch_token_ids) == self.batch_size or is_end:\n",
    "                batch_token_ids = sequence_padding(batch_token_ids)\n",
    "                batch_segment_ids = sequence_padding(batch_segment_ids)\n",
    "                batch_output_ids = sequence_padding(batch_output_ids)\n",
    "                yield [batch_token_ids, batch_segment_ids], batch_output_ids\n",
    "                batch_token_ids, batch_segment_ids, batch_output_ids = [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_1, text2, label = data[0][0], data[0][1], data[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.80493773, 0.41796254])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.random(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids , output_ids = [], [] \n",
    "rands = np.random.random(len(text_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r, i in zip(rands, text_1):\n",
    "    if r < 0.15 * 0.8:\n",
    "        input_ids.append(4)\n",
    "        output_ids.append(i)\n",
    "    elif r < 0.15 * 0.9:\n",
    "        input_ids.append(i)\n",
    "        output_ids.append(i)\n",
    "    elif r < 0.15:\n",
    "        input_ids.append(np.random.choice(len(tokens)) + 7)\n",
    "        output_ids.append(i)\n",
    "    else:\n",
    "        input_ids.append(i)\n",
    "        output_ids.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 4, 6, 7]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 5, 0, 0]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from bert4keras.tokenizers import load_vocab\n",
    "from bert4keras.snippets import truncate_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function truncate_sequences in module bert4keras.snippets:\n",
      "\n",
      "truncate_sequences(maxlen, index, *sequences)\n",
      "    截断总长度至不超过maxlen\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(truncate_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_count = 5\n",
    "maxlen = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    \"\"\"加载数据\n",
    "    单条格式：(文本1 ids, 文本2 ids, 标签id)\n",
    "    \"\"\"\n",
    "    D = []\n",
    "    with open(filename) as f:\n",
    "        for l in f:\n",
    "            l = l.strip().split('\\t')\n",
    "            if len(l) == 3:\n",
    "                a, b, c = l[0], l[1], int(l[2])\n",
    "            else:\n",
    "                a, b, c = l[0], l[1], -5  # 未标注数据，标签为-5\n",
    "            a = [int(i) for i in a.split(' ')]\n",
    "            b = [int(i) for i in b.split(' ')]\n",
    "            truncate_sequences(maxlen, -1, a, b)\n",
    "            D.append((a, b, c))\n",
    "    return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_data(\n",
    "    './tianchi_datasets/track3_round1_train.tsv'\n",
    ")\n",
    "test_data = load_data(\n",
    "    './tianchi_datasets/track3_round1_testA.tsv'\n",
    ")\n",
    "dict_path = \"./pretrain_model/chinese_roberta_wwm_ext/vocab.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = {}\n",
    "for d in train_data + test_data:\n",
    "    for i in d[0] + d[1]:\n",
    "        tokens[i] = tokens.get(i, 0) + 1\n",
    "\n",
    "tokens = {i: j for i, j in tokens.items() if j >= min_count}\n",
    "tokens = sorted(tokens.items(), key=lambda s: -s[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_dict = load_vocab(dict_path)\n",
    "with open('./counts.json', 'r') as f:\n",
    "    counts = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "del counts['[CLS]']\n",
    "del counts['[SEP]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_tokens = {i:counts.get(i, 0) for i, j in sorted(token_dict.items())}\n",
    "keep_tokens1 = sorted(keep_tokens.items(), key = lambda s:-s[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_tokens1 = keep_tokens1[:len(tokens)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "v2i_chinese = {tokens[i][0]:keep_tokens1[i][0] for i in range(len(tokens))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1, 2, 3, 4, 5, 6, 7], [8, 9, 10, 4, 11], 0)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6925"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(v2i_chinese)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"./tianchi_datasets/track3_round1_newtrain3.tsv\", sep=\"\\t\", header=None,\n",
    "                                  quoting=3, encoding=\"utf-8\", names=[\"sentence1\", \"sentence2\", \"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_chinese(text_ids):\n",
    "    s = \"\".join([v2i_chinese.get(int(num), '[PAD]') for num in text_ids.split(\" \")])\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"sentence1_cn\"] = train_data[\"sentence1\"].apply(convert_to_chinese())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>labels</th>\n",
       "      <th>sentence1_cn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1 2 3 4 5 6 7</td>\n",
       "      <td>8 9 10 4 11</td>\n",
       "      <td>0</td>\n",
       "      <td>音术从政学国声</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12 13 14 15</td>\n",
       "      <td>12 15 11 16</td>\n",
       "      <td>0</td>\n",
       "      <td>，以社毒</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17 18 12 19 20 21 22 23 24</td>\n",
       "      <td>12 23 25 6 26 27 19</td>\n",
       "      <td>1</td>\n",
       "      <td>泽迎，。古固厂、多</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28 29 30 31 11</td>\n",
       "      <td>32 33 34 30 31</td>\n",
       "      <td>1</td>\n",
       "      <td>用的测件是</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29 35 36 29</td>\n",
       "      <td>29 37 36 29</td>\n",
       "      <td>1</td>\n",
       "      <td>的重领的</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121100</th>\n",
       "      <td>282 283 283 282 158 228 58 180</td>\n",
       "      <td>282 283 283 282 158 228 1645 56 58 180</td>\n",
       "      <td>0</td>\n",
       "      <td>会个个会系数区建</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121101</th>\n",
       "      <td>12 19 6788 23 24</td>\n",
       "      <td>12 19 307 3151 23</td>\n",
       "      <td>0</td>\n",
       "      <td>，。j、多</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121102</th>\n",
       "      <td>72 29 146 146</td>\n",
       "      <td>176 29 227 134 146 146</td>\n",
       "      <td>1</td>\n",
       "      <td>：的性性</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121103</th>\n",
       "      <td>12 251 6277 6278 16</td>\n",
       "      <td>12 251 11350</td>\n",
       "      <td>1</td>\n",
       "      <td>，经cleur一</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121104</th>\n",
       "      <td>282 283 283 282 140 632 329 330 1105</td>\n",
       "      <td>329 330 2418 282 283 283 282 1829</td>\n",
       "      <td>1</td>\n",
       "      <td>会个个会于急题问诗</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>121105 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   sentence1  \\\n",
       "0                              1 2 3 4 5 6 7   \n",
       "1                                12 13 14 15   \n",
       "2                 17 18 12 19 20 21 22 23 24   \n",
       "3                             28 29 30 31 11   \n",
       "4                                29 35 36 29   \n",
       "...                                      ...   \n",
       "121100        282 283 283 282 158 228 58 180   \n",
       "121101                      12 19 6788 23 24   \n",
       "121102                         72 29 146 146   \n",
       "121103                   12 251 6277 6278 16   \n",
       "121104  282 283 283 282 140 632 329 330 1105   \n",
       "\n",
       "                                     sentence2  labels sentence1_cn  \n",
       "0                                  8 9 10 4 11       0      音术从政学国声  \n",
       "1                                  12 15 11 16       0         ，以社毒  \n",
       "2                          12 23 25 6 26 27 19       1    泽迎，。古固厂、多  \n",
       "3                               32 33 34 30 31       1        用的测件是  \n",
       "4                                  29 37 36 29       1         的重领的  \n",
       "...                                        ...     ...          ...  \n",
       "121100  282 283 283 282 158 228 1645 56 58 180       0     会个个会系数区建  \n",
       "121101                       12 19 307 3151 23       0        ，。j、多  \n",
       "121102                  176 29 227 134 146 146       1         ：的性性  \n",
       "121103                            12 251 11350       1     ，经cleur一  \n",
       "121104       329 330 2418 282 283 283 282 1829       1    会个个会于急题问诗  \n",
       "\n",
       "[121105 rows x 4 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_dict['[PAD]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1 2 3 4 5 6 7', '12 13 14 15', '17 18 12 19 20 21 22 23 24', ...,\n",
       "       '72 29 146 146', '12 251 6277 6278 16',\n",
       "       '282 283 283 282 140 632 329 330 1105'], dtype=object)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[v2i_chinese[num] for num in [12, 13, 14, 15]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['，', '以', '社', '毒']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[v2i_chinese[num] for num in [12, 13, 14, 15]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"v2i.json\", \"w\") as fw:\n",
    "    json.dump(data1, fw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1[\"[PAD]\"] = 0\n",
    "data1[\"[MASK]\"] = 103"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7517"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UNK]\n",
      "[CLS]\n",
      "[SEP]\n"
     ]
    }
   ],
   "source": [
    "for i, j in data1.items():\n",
    "    if j in [0, 100, 101, 102, 103]:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"./tianchi_datasets/track3_round1_newtrain3.tsv\", sep=\"\\t\", header=None,\n",
    "                                 names=[\"sentence1\", \"sentence2\", \"labels\"])\n",
    "train_data[\"document\"] = train_data[\"sentence1\"].str.cat(train_data[\"sentence2\"], sep = \" [SEP] \")\n",
    "train_data[\"document\"] = train_data[\"document\"].map(lambda x:[word for word in x.split(\" \")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>labels</th>\n",
       "      <th>document</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1 2 3 4 5 6 7</td>\n",
       "      <td>8 9 10 4 11</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, [SEP], 8, 9, 10, 4, 11]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12 13 14 15</td>\n",
       "      <td>12 15 11 16</td>\n",
       "      <td>0</td>\n",
       "      <td>[12, 13, 14, 15, [SEP], 12, 15, 11, 16]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17 18 12 19 20 21 22 23 24</td>\n",
       "      <td>12 23 25 6 26 27 19</td>\n",
       "      <td>1</td>\n",
       "      <td>[17, 18, 12, 19, 20, 21, 22, 23, 24, [SEP], 12...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28 29 30 31 11</td>\n",
       "      <td>32 33 34 30 31</td>\n",
       "      <td>1</td>\n",
       "      <td>[28, 29, 30, 31, 11, [SEP], 32, 33, 34, 30, 31]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29 35 36 29</td>\n",
       "      <td>29 37 36 29</td>\n",
       "      <td>1</td>\n",
       "      <td>[29, 35, 36, 29, [SEP], 29, 37, 36, 29]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121100</th>\n",
       "      <td>282 283 283 282 158 228 58 180</td>\n",
       "      <td>282 283 283 282 158 228 1645 56 58 180</td>\n",
       "      <td>0</td>\n",
       "      <td>[282, 283, 283, 282, 158, 228, 58, 180, [SEP],...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121101</th>\n",
       "      <td>12 19 6788 23 24</td>\n",
       "      <td>12 19 307 3151 23</td>\n",
       "      <td>0</td>\n",
       "      <td>[12, 19, 6788, 23, 24, [SEP], 12, 19, 307, 315...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121102</th>\n",
       "      <td>72 29 146 146</td>\n",
       "      <td>176 29 227 134 146 146</td>\n",
       "      <td>1</td>\n",
       "      <td>[72, 29, 146, 146, [SEP], 176, 29, 227, 134, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121103</th>\n",
       "      <td>12 251 6277 6278 16</td>\n",
       "      <td>12 251 11350</td>\n",
       "      <td>1</td>\n",
       "      <td>[12, 251, 6277, 6278, 16, [SEP], 12, 251, 11350]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121104</th>\n",
       "      <td>282 283 283 282 140 632 329 330 1105</td>\n",
       "      <td>329 330 2418 282 283 283 282 1829</td>\n",
       "      <td>1</td>\n",
       "      <td>[282, 283, 283, 282, 140, 632, 329, 330, 1105,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>121105 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   sentence1  \\\n",
       "0                              1 2 3 4 5 6 7   \n",
       "1                                12 13 14 15   \n",
       "2                 17 18 12 19 20 21 22 23 24   \n",
       "3                             28 29 30 31 11   \n",
       "4                                29 35 36 29   \n",
       "...                                      ...   \n",
       "121100        282 283 283 282 158 228 58 180   \n",
       "121101                      12 19 6788 23 24   \n",
       "121102                         72 29 146 146   \n",
       "121103                   12 251 6277 6278 16   \n",
       "121104  282 283 283 282 140 632 329 330 1105   \n",
       "\n",
       "                                     sentence2  labels  \\\n",
       "0                                  8 9 10 4 11       0   \n",
       "1                                  12 15 11 16       0   \n",
       "2                          12 23 25 6 26 27 19       1   \n",
       "3                               32 33 34 30 31       1   \n",
       "4                                  29 37 36 29       1   \n",
       "...                                        ...     ...   \n",
       "121100  282 283 283 282 158 228 1645 56 58 180       0   \n",
       "121101                       12 19 307 3151 23       0   \n",
       "121102                  176 29 227 134 146 146       1   \n",
       "121103                            12 251 11350       1   \n",
       "121104       329 330 2418 282 283 283 282 1829       1   \n",
       "\n",
       "                                                 document  \n",
       "0           [1, 2, 3, 4, 5, 6, 7, [SEP], 8, 9, 10, 4, 11]  \n",
       "1                 [12, 13, 14, 15, [SEP], 12, 15, 11, 16]  \n",
       "2       [17, 18, 12, 19, 20, 21, 22, 23, 24, [SEP], 12...  \n",
       "3         [28, 29, 30, 31, 11, [SEP], 32, 33, 34, 30, 31]  \n",
       "4                 [29, 35, 36, 29, [SEP], 29, 37, 36, 29]  \n",
       "...                                                   ...  \n",
       "121100  [282, 283, 283, 282, 158, 228, 58, 180, [SEP],...  \n",
       "121101  [12, 19, 6788, 23, 24, [SEP], 12, 19, 307, 315...  \n",
       "121102  [72, 29, 146, 146, [SEP], 176, 29, 227, 134, 1...  \n",
       "121103   [12, 251, 6277, 6278, 16, [SEP], 12, 251, 11350]  \n",
       "121104  [282, 283, 283, 282, 140, 632, 329, 330, 1105,...  \n",
       "\n",
       "[121105 rows x 4 columns]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_loss_mask(len_arange, seq, pad_id):\n",
    "    rand_id = np.random.choice(len_arange, size=max(2, int(MASK_RATE * len(len_arange))), replace=False)\n",
    "    loss_mask = np.full_like(seq, pad_id, dtype=np.bool)\n",
    "    loss_mask[rand_id] = True\n",
    "    return loss_mask[None, :], rand_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_mask(seq, len_arange, pad_id, mask_id):\n",
    "    loss_mask, rand_id = _get_loss_mask(len_arange, seq, pad_id)\n",
    "    seq[rand_id] = mask_id\n",
    "    return loss_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_nothing(seq, len_arange, pad_id):\n",
    "    loss_mask, _ = _get_loss_mask(len_arange, seq, pad_id)\n",
    "    return loss_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_replace(seq, len_arange, pad_id, word_ids):\n",
    "    loss_mask, rand_id = _get_loss_mask(len_arange, seq, pad_id)\n",
    "    seq[rand_id] = np.random.choice(word_ids, size=len(rand_id))\n",
    "    return loss_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 101, 2099, 2990, 3309,  676, 2110, 1744, 5303,  102,  809,  782,\n",
       "       3300,  676, 3221,  102,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "input_ids = []\n",
    "max_length = 93\n",
    "for inputs in train_data[\"document\"]:\n",
    "    arange = np.arange(0, len(inputs) + 2)\n",
    "    padded = np.full(max_length, fill_value=0, dtype=np.int32)\n",
    "    padded[:(len(inputs) + 2)] = [v2i['[CLS]']] + [v2i.get(x, v2i['[UNK]']) for x in inputs] + [v2i['[SEP]']]\n",
    "    p = np.random.random\n",
    "    if p < 0.7:\n",
    "        loss_mask = do_mask(padded, arange, v2i[\"[PAD]\"], v2i[\"[MASK]\"])\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 101, 2099, 2990, 3309,  676,  103, 1744, 5303,  102,  809,  782,\n",
       "       3300,  676,  103,  102,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v2i['[PAD]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "MASK_RATE = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = padded.copy()\n",
    "len_arange = arange\n",
    "pad_id = v2i[\"[PAD]\"]\n",
    "mask_id = v2i[\"[MASK]\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False, False, False, False,  True, False, False, False, False,\n",
       "        False, False,  True, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False]])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "do_mask(seq, len_arange, pad_id, mask_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(15 * 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['6140', '1260', '9865', ..., '1319', '4224', '8969'], dtype='<U6')"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
